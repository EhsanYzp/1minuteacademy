{
  "id": "ai--the-future-of-ai--t-ai-and-mental-health",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Future of AI",
  "course_id": "ai--the-future-of-ai",
  "chapter_id": "ai--the-future-of-ai--ch05-society",
  "title": "AI and Mental Health",
  "emoji": "ğŸ”­",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on AI and Mental Health.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "At 3 AM, a person in crisis texts an AI. It responds with warmth and evidence-based coping strategies.",
      "visual": "ğŸŒ™"
    },
    "buildup": {
      "text": "AI chatbots like Woebot provide cognitive behavioral therapy techniques through text conversations.",
      "visual": "ğŸ’¬"
    },
    "discovery": {
      "text": "They offer 24/7 availability, zero judgment, and consistent therapeutic approaches at massive scale.",
      "visual": "ğŸŒ"
    },
    "twist": {
      "text": "But AI can't handle severe crises well. A chatbot misjudging suicidal ideation could have devastating consequences.",
      "visual": "âš ï¸"
    },
    "climax": {
      "text": "The future likely combines AI for routine support with human therapists for complex and critical care.",
      "visual": "ğŸ¤"
    },
    "punchline": {
      "text": "Available when no one else is. But never a replacement for real human care.",
      "visual": "â¤ï¸"
    }
  },
  "quiz": {
    "question": "What is a serious risk of AI mental health chatbots?",
    "options": [
      "Misjudging severe crises like suicidal ideation",
      "Being too expensive for anyone to use",
      "Providing too much human interaction"
    ],
    "correct": 0
  }
}
