{
  "id": "ai--the-future-of-ai--t-small-language-models-power-in-a-pocket",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Future of AI",
  "course_id": "ai--the-future-of-ai",
  "chapter_id": "ai--the-future-of-ai--ch01-next-gen",
  "title": "Small Language Models: Power in a Pocket",
  "emoji": "ğŸ”­",
  "color": "#7C3AED",
  "description": "A quick win: understand Small Language Models: Power in a Pocket.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "A 7-billion parameter model running on your phone outperforms GPT-3's 175 billion on many tasks.",
      "visual": "ğŸ“±"
    },
    "buildup": {
      "text": "Distillation and quantization compress large models into smaller ones that retain most capabilities.",
      "visual": "ğŸ—œï¸"
    },
    "discovery": {
      "text": "Models like Phi, Mistral, and Llama prove that training data quality trumps raw model size.",
      "visual": "ğŸ’"
    },
    "twist": {
      "text": "Small models enable privacy â€” your data never leaves your device if the AI runs locally.",
      "visual": "ğŸ”’"
    },
    "climax": {
      "text": "The future may be billions of small, specialized models, not one giant model ruling everything.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "The most powerful AI might be the one in your pocket.",
      "visual": "ğŸ¤"
    }
  },
  "quiz": {
    "question": "What advantage do small language models offer over large cloud models?",
    "options": [
      "They can run locally on devices, preserving privacy",
      "They are always more accurate",
      "They require more compute than large models"
    ],
    "correct": 0
  }
}
