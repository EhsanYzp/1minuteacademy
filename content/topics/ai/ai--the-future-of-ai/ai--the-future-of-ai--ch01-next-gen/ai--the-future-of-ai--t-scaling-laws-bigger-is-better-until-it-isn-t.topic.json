{
  "id": "ai--the-future-of-ai--t-scaling-laws-bigger-is-better-until-it-isn-t",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Future of AI",
  "course_id": "ai--the-future-of-ai",
  "chapter_id": "ai--the-future-of-ai--ch01-next-gen",
  "title": "Scaling Laws: Bigger Is Better â€” Until It Isn't",
  "emoji": "ğŸ”­",
  "color": "#7C3AED",
  "description": "A short lesson to help you apply Scaling Laws: Bigger Is Better â€” Until It Isn't.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Double the parameters, double the performance. This rule drove AI for years. It's starting to crack.",
      "visual": "ğŸ“ˆ"
    },
    "buildup": {
      "text": "Scaling laws predicted that larger models trained on more data would keep improving predictably.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "GPT-3 to GPT-4 confirmed these laws. But the cost of each new leap is growing exponentially.",
      "visual": "ğŸ’°"
    },
    "twist": {
      "text": "We may run out of high-quality training data before we run out of money to train larger models.",
      "visual": "ğŸ“š"
    },
    "climax": {
      "text": "The next era of AI may come from cleverness, not just brute scale â€” better algorithms, not bigger ones.",
      "visual": "ğŸ§ "
    },
    "punchline": {
      "text": "Size matters. But at some point, only smarts will do.",
      "visual": "ğŸ’¡"
    }
  },
  "quiz": {
    "question": "What challenge threatens the 'bigger is better' approach to AI?",
    "options": [
      "Exponential costs and potential exhaustion of quality training data",
      "Models are already too small to be useful",
      "Scaling laws were never valid"
    ],
    "correct": 0
  }
}
