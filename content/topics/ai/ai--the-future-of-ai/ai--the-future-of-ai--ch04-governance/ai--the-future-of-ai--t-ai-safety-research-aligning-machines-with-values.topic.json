{
  "id": "ai--the-future-of-ai--t-ai-safety-research-aligning-machines-with-values",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Future of AI",
  "course_id": "ai--the-future-of-ai",
  "chapter_id": "ai--the-future-of-ai--ch04-governance",
  "title": "AI Safety Research: Aligning Machines with Values",
  "emoji": "ğŸ”­",
  "color": "#7C3AED",
  "description": "A quick win: understand AI Safety Research: Aligning Machines with Values.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "How do you make a superintelligent AI want what humans want? This is the alignment problem.",
      "visual": "ğŸ¯"
    },
    "buildup": {
      "text": "RLHF trains models to prefer responses that human evaluators rate as helpful and harmless.",
      "visual": "ğŸ‘"
    },
    "discovery": {
      "text": "But whose values? Human evaluators disagree. Cultural norms vary. Alignment is a moving target.",
      "visual": "ğŸŒŠ"
    },
    "twist": {
      "text": "A perfectly aligned AI for one group might be harmful to another. Universal alignment may be impossible.",
      "visual": "ğŸŒ"
    },
    "climax": {
      "text": "AI safety research has grown from a niche concern to a core focus of every major AI lab.",
      "visual": "ğŸ¢"
    },
    "punchline": {
      "text": "Building powerful AI is hard. Making sure it stays helpful might be harder.",
      "visual": "ğŸ›¡ï¸"
    }
  },
  "quiz": {
    "question": "Why is AI alignment considered a difficult problem?",
    "options": [
      "Human values vary across cultures and individuals, making universal alignment elusive",
      "Alignment was already solved in 2020",
      "Only one definition of 'helpful' exists"
    ],
    "correct": 0
  }
}
