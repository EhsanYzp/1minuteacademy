{
  "id": "ai--the-future-of-ai--t-vision-language-models-seeing-and-speaking",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Future of AI",
  "course_id": "ai--the-future-of-ai",
  "chapter_id": "ai--the-future-of-ai--ch02-multimodal",
  "title": "Vision-Language Models: Seeing and Speaking",
  "emoji": "ğŸ”­",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Vision-Language Models: Seeing and Speaking.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Show AI a photo of a messy kitchen and ask 'what happened here?' It tells a story from the scene.",
      "visual": "ğŸ³"
    },
    "buildup": {
      "text": "Vision-language models like GPT-4V and Gemini process images and text in a unified architecture.",
      "visual": "ğŸ‘ï¸"
    },
    "discovery": {
      "text": "They understand spatial relationships, read handwriting, interpret charts, and describe scenes.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "They still make bizarre errors â€” misreading clocks, confusing left and right, hallucinating objects.",
      "visual": "ğŸŒ€"
    },
    "climax": {
      "text": "Combining vision and language is critical for AI assistants that interact with the real visual world.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "AI learned to read. Now it's learning to see and read at the same time.",
      "visual": "ğŸ“–"
    }
  },
  "quiz": {
    "question": "What can vision-language models do that text-only models cannot?",
    "options": [
      "Interpret images, charts, and scenes alongside text",
      "Process text faster",
      "Generate sounds from images"
    ],
    "correct": 0
  }
}
