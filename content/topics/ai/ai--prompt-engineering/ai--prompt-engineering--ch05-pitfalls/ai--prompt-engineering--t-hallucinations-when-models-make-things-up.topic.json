{
  "id": "ai--prompt-engineering--t-hallucinations-when-models-make-things-up",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch05-pitfalls",
  "title": "Hallucinations: When Models Make Things Up",
  "emoji": "âœï¸",
  "color": "#7C3AED",
  "description": "A quick, practical guide to Hallucinations: When Models Make Things Up.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "The model cited a paper that doesn't exist. It sounded so confident.",
      "visual": "ğŸ‘»"
    },
    "buildup": {
      "text": "Hallucinations happen because models predict plausible tokens, not verified facts.",
      "visual": "ğŸ°"
    },
    "discovery": {
      "text": "Reduce them: lower temperature, add 'say I don't know if unsure,' provide source docs.",
      "visual": "ğŸ“š"
    },
    "twist": {
      "text": "You can't fully eliminate hallucinations. Every answer needs a verification plan.",
      "visual": "ğŸ”"
    },
    "climax": {
      "text": "Best defense: give the model the facts in context and ask it to use only those.",
      "visual": "ğŸ›¡ï¸"
    },
    "punchline": {
      "text": "Confidence is not correctness.",
      "visual": "âš ï¸"
    }
  },
  "quiz": {
    "question": "Why do models hallucinate?",
    "options": [
      "They predict plausible tokens, not verified facts",
      "They access a live database",
      "They intentionally lie"
    ],
    "correct": 0
  }
}
