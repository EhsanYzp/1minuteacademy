{
  "id": "ai--prompt-engineering--t-debugging-a-bad-prompt",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch05-pitfalls",
  "title": "Debugging a Bad Prompt",
  "emoji": "âœï¸",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Debugging a Bad Prompt.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "The output is wrong but you changed five things at once. Which one broke it?",
      "visual": "ğŸ›"
    },
    "buildup": {
      "text": "Prompt debugging is like code debuggingâ€”isolate one variable at a time.",
      "visual": "ğŸ”¬"
    },
    "discovery": {
      "text": "Strategy: hold the input fixed, change one prompt element, compare outputs.",
      "visual": "ğŸ§ª"
    },
    "twist": {
      "text": "Sometimes the prompt is fineâ€”the model just needs a different temperature or more context.",
      "visual": "ğŸŒ¡ï¸"
    },
    "climax": {
      "text": "Log every prompt version and its output. A/B test systematically, not randomly.",
      "visual": "ğŸ““"
    },
    "punchline": {
      "text": "Change one thing. Test. Repeat.",
      "visual": "ğŸ”„"
    }
  },
  "quiz": {
    "question": "What's the best approach to debugging prompts?",
    "options": [
      "Change one variable at a time and compare",
      "Rewrite everything from scratch",
      "Increase temperature until it works"
    ],
    "correct": 0
  }
}
