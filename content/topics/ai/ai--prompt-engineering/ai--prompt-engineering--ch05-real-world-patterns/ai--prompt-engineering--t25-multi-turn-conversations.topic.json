{
  "id": "ai--prompt-engineering--t25-multi-turn-conversations",
  "version": 1,
  "subject": "AI",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch05-real-world-patterns",
  "title": "Multi-Turn Conversations",
  "emoji": "âœï¸",
  "color": "#EF4444",
  "description": "Design prompts that work across multiple back-and-forth exchanges.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ’¬",
      "text": "The chatbot answers the first question perfectly. By message 10, it's forgotten the original context and contradicts itself."
    },
    "buildup": {
      "visual": "ğŸ“œ",
      "text": "In multi-turn conversations, every previous message is re-sent as context. As the conversation grows, earlier messages get pushed toward the attention dead zone."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Periodically summarize the conversation and inject the summary as a 'state' message. 'Current context: user wants X, we've established Y, pending Z.' This keeps the model grounded."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Every turn adds tokens. A 30-message conversation can eat half the context window before the user asks their real question. Prune aggressively."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Design your system prompt to reinforce key instructions every turn: role, guardrails, output format. Don't rely on the model remembering what was said 20 messages ago."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Conversations are stateless under the hood. You manage the state, not the model."
    }
  },
  "quiz": {
    "question": "Why do chatbots lose context in long conversations?",
    "options": [
      "They have permanent memory that fills up",
      "Earlier messages get pushed into low-attention regions of the context window",
      "They only process the latest message",
      "Multi-turn conversations aren't supported"
    ],
    "correct": 1
  }
}
