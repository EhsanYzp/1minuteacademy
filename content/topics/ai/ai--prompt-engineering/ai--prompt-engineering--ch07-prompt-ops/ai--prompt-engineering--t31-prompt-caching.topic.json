{
  "id": "ai--prompt-engineering--t31-prompt-caching",
  "version": 1,
  "subject": "AI",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch07-prompt-ops",
  "title": "Prompt Caching",
  "emoji": "‚úèÔ∏è",
  "color": "#EF4444",
  "description": "Save money and latency by caching repeated prompt results.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "üí∞",
      "text": "Your FAQ bot answers 'What's your return policy?' 500 times a day with the same prompt and same answer. That's 500 API calls for one answer."
    },
    "buildup": {
      "visual": "üóÉÔ∏è",
      "text": "Prompt caching stores the response for a given (prompt + input) pair. If the same query comes in again, return the cached answer without hitting the API."
    },
    "discovery": {
      "visual": "üí°",
      "text": "For deterministic prompts (low temperature, stable input), caching can cut API costs by 60‚Äì80% and reduce latency from seconds to milliseconds."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "Caching is risky for personalized or dynamic prompts. If the user's context changes but the cache key doesn't, you serve stale or wrong answers."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "Cache aggressively for FAQ-style queries. Use short TTLs for dynamic contexts. Always include all variable parts in the cache key."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "The fastest API call is the one you never make."
    }
  },
  "quiz": {
    "question": "What's a risk of prompt caching?",
    "options": [
      "It makes the model less intelligent",
      "Serving stale answers when the user's context changes",
      "It increases API costs",
      "It only works with one model"
    ],
    "correct": 1
  }
}
