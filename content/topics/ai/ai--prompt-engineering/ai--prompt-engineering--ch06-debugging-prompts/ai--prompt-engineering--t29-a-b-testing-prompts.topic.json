{
  "id": "ai--prompt-engineering--t29-a-b-testing-prompts",
  "version": 1,
  "subject": "AI",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch06-debugging-prompts",
  "title": "A/B Testing Prompts",
  "emoji": "âœï¸",
  "color": "#EF4444",
  "description": "Compare prompt variants with real user traffic.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "âš–ï¸",
      "text": "Prompt A feels better. Prompt B scores higher on your eval set. Which do you ship? The answer is: test with real users."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "A/B testing sends a percentage of real traffic to each prompt variant and measures a business metric â€” user satisfaction, task completion, or conversion rate."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Internal evals miss user behavior. Your eval set said Prompt B was better, but users preferred Prompt A because it was more conversational. Only live testing revealed the gap."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Prompt A/B tests need sufficient volume to be statistically significant. If you get 10 requests a day, you'll wait months for a meaningful result. Use eval sets for low-traffic features."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "For high-traffic features, run A/B tests with clear success metrics. For low-traffic ones, rely on eval sets and qualitative review."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Eval sets tell you which prompt is correct. A/B tests tell you which one works."
    }
  },
  "quiz": {
    "question": "When is A/B testing prompts most useful?",
    "options": [
      "When you have very little user traffic",
      "When you need to compare prompt variants with real user behavior at scale",
      "When you're just starting to build the prompt",
      "When the prompt never changes"
    ],
    "correct": 1
  }
}
