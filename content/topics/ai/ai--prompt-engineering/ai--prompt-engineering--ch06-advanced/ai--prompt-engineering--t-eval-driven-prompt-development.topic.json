{
  "id": "ai--prompt-engineering--t-eval-driven-prompt-development",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch06-advanced",
  "title": "Eval-Driven Prompt Development",
  "emoji": "âœï¸",
  "color": "#7C3AED",
  "description": "A quick win: understand Eval-Driven Prompt Development.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "Your prompt 'feels' better. But is it actually better? You don't have numbers.",
      "visual": "ğŸ“‰"
    },
    "buildup": {
      "text": "Without evals, prompt engineering is guesswork. You need a test set and a metric.",
      "visual": "ğŸ§ª"
    },
    "discovery": {
      "text": "Build 20â€“50 test cases with expected outputs. Score each prompt version against them.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "A prompt that scores 95% on your eval may fail on real-world edge cases. Keep updating.",
      "visual": "ğŸ”„"
    },
    "climax": {
      "text": "Automate: run evals on every prompt change, like unit tests for code.",
      "visual": "ğŸ¤–"
    },
    "punchline": {
      "text": "If you can't measure it, you can't improve it.",
      "visual": "ğŸ“"
    }
  },
  "quiz": {
    "question": "What is eval-driven prompt development?",
    "options": [
      "Testing prompts against scored test cases",
      "Evaluating the model's training data",
      "Running prompts once and shipping"
    ],
    "correct": 0
  }
}
