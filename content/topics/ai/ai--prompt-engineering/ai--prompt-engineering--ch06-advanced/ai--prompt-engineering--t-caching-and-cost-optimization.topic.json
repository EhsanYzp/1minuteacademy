{
  "id": "ai--prompt-engineering--t-caching-and-cost-optimization",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch06-advanced",
  "title": "Caching and Cost Optimization",
  "emoji": "âœï¸",
  "color": "#7C3AED",
  "description": "A quick, practical guide to Caching and Cost Optimization.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "Your prompt bill tripled this month. Same prompt, same usersâ€”what happened?",
      "visual": "ğŸ’¸"
    },
    "buildup": {
      "text": "Repeated identical prompts cost the same each time. Caching stops that waste.",
      "visual": "ğŸ”"
    },
    "discovery": {
      "text": "Cache by hashing the prompt + parameters. Return stored responses for exact matches.",
      "visual": "ğŸ—„ï¸"
    },
    "twist": {
      "text": "Semantic caching (similar, not identical prompts) saves even more but risks stale answers.",
      "visual": "ğŸ§Š"
    },
    "climax": {
      "text": "Set TTLs, monitor hit rates, and invalidate when your system prompt changes.",
      "visual": "â²ï¸"
    },
    "punchline": {
      "text": "The cheapest API call is the one you don't make.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "How does prompt caching reduce costs?",
    "options": [
      "Returns stored responses for identical prompts",
      "Makes the model faster",
      "Reduces token size"
    ],
    "correct": 0
  }
}
