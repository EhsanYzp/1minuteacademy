{
  "id": "ai--prompt-engineering--t-negative-prompting-saying-what-not-to-do",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch02-techniques",
  "title": "Negative Prompting: Saying What NOT to Do",
  "emoji": "‚úèÔ∏è",
  "color": "#7C3AED",
  "description": "A quick, practical guide to Negative Prompting: Saying What NOT to Do.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "You said 'don't mention pricing' and the model's first word is a price.",
      "visual": "üö´"
    },
    "buildup": {
      "text": "Models sometimes fixate on negated concepts, doing exactly what you forbade.",
      "visual": "üß≤"
    },
    "discovery": {
      "text": "Instead of 'don't do X,' tell it what TO do: 'respond only about features.'",
      "visual": "‚úÖ"
    },
    "twist": {
      "text": "Negative prompts work better for style ('no bullet points') than for content.",
      "visual": "üé®"
    },
    "climax": {
      "text": "Combine: a positive instruction first, then a short 'never' list as a guardrail.",
      "visual": "üß±"
    },
    "punchline": {
      "text": "Tell it what to do. 'Don't' is a suggestion.",
      "visual": "üí°"
    }
  },
  "quiz": {
    "question": "Why can 'don't do X' backfire in prompts?",
    "options": [
      "Models can fixate on the negated concept",
      "It saves tokens",
      "It always works perfectly"
    ],
    "correct": 0
  }
}
