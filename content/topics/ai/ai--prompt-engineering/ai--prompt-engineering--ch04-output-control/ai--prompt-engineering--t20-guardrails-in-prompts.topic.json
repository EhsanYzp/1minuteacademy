{
  "id": "ai--prompt-engineering--t20-guardrails-in-prompts",
  "version": 1,
  "subject": "AI",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch04-output-control",
  "title": "Guardrails in Prompts",
  "emoji": "‚úèÔ∏è",
  "color": "#EF4444",
  "description": "Prevent the model from going off-script.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "üöß",
      "text": "Your customer-facing chatbot starts giving legal advice. It's confident, wrong, and now you have a liability problem."
    },
    "buildup": {
      "visual": "üõ°Ô∏è",
      "text": "Guardrails are explicit instructions that restrict the model's behavior: 'Never give medical/legal/financial advice. If asked, say: Please consult a professional.'"
    },
    "discovery": {
      "visual": "üí°",
      "text": "Combine negative constraints ('never do X') with positive fallbacks ('instead, do Y'). The model follows positive instructions more reliably than negative ones alone."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "Prompt-based guardrails can be bypassed with creative user input. For critical safety, add server-side validation that checks the model's output before showing it to the user."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "Layer your defenses: system prompt guardrails, output validation, content filters, and logging. No single layer is enough for production safety."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "Guardrails aren't optional in production. They're the difference between a product and a lawsuit."
    }
  },
  "quiz": {
    "question": "Why aren't prompt-based guardrails alone sufficient for production?",
    "options": [
      "They slow down the model too much",
      "They can be bypassed by creative user inputs",
      "They don't work with system prompts",
      "They increase API costs significantly"
    ],
    "correct": 1
  }
}
