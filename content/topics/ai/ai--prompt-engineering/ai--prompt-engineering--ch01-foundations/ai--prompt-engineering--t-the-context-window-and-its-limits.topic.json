{
  "id": "ai--prompt-engineering--t-the-context-window-and-its-limits",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Prompt Engineering",
  "course_id": "ai--prompt-engineering",
  "chapter_id": "ai--prompt-engineering--ch01-foundations",
  "title": "The Context Window and Its Limits",
  "emoji": "âœï¸",
  "color": "#7C3AED",
  "description": "A quick win: understand The Context Window and Its Limits.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "You paste 50 pages into the chat and the model 'forgets' the first 40.",
      "visual": "ğŸ“š"
    },
    "buildup": {
      "text": "The context window is the total token limit for input + output combined.",
      "visual": "ğŸªŸ"
    },
    "discovery": {
      "text": "Bigger windows exist (128K+), but attention quality drops with distance.",
      "visual": "ğŸ“‰"
    },
    "twist": {
      "text": "Models attend best to the beginning and endâ€”the 'lost in the middle' problem.",
      "visual": "ğŸœï¸"
    },
    "climax": {
      "text": "Put critical instructions at the start. Put key data near the end.",
      "visual": "ğŸ“Œ"
    },
    "punchline": {
      "text": "Bigger windows help, but placement wins.",
      "visual": "ğŸ†"
    }
  },
  "quiz": {
    "question": "Where in the context do models pay least attention?",
    "options": [
      "The middle",
      "The beginning",
      "The very end"
    ],
    "correct": 0
  }
}
