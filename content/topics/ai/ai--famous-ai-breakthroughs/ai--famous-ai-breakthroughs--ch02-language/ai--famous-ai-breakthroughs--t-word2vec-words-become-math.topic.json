{
  "id": "ai--famous-ai-breakthroughs--t-word2vec-words-become-math",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Famous AI Breakthroughs",
  "course_id": "ai--famous-ai-breakthroughs",
  "chapter_id": "ai--famous-ai-breakthroughs--ch02-language",
  "title": "Word2Vec: Words Become Math",
  "emoji": "ğŸ†",
  "color": "#7C3AED",
  "description": "A micro-lesson that makes Word2Vec: Words Become Math usable.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "King minus Man plus Woman equals Queen. Words became vectors, and analogies became algebra.",
      "visual": "ğŸ”¢"
    },
    "buildup": {
      "text": "In 2013, Mikolov at Google published Word2Vec â€” efficient word embeddings from vast text corpora.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "Words with similar meanings clustered together in vector space automatically.",
      "visual": "ğŸ—ºï¸"
    },
    "twist": {
      "text": "The embeddings also captured societal biases â€” 'doctor' was closer to 'man' than 'woman.'",
      "visual": "âš ï¸"
    },
    "climax": {
      "text": "Word2Vec became the foundation for nearly every NLP breakthrough that followed.",
      "visual": "ğŸ—ï¸"
    },
    "punchline": {
      "text": "Turn words into numbers and language becomes computable.",
      "visual": "ğŸ§®"
    }
  },
  "quiz": {
    "question": "What did Word2Vec demonstrate about word relationships?",
    "options": [
      "Words with similar meanings cluster together as vectors",
      "Words cannot be represented numerically",
      "Every word gets a unique random number"
    ],
    "correct": 0
  }
}
