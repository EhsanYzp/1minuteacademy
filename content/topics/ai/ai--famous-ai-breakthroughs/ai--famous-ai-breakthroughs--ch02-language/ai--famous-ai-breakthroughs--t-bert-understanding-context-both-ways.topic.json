{
  "id": "ai--famous-ai-breakthroughs--t-bert-understanding-context-both-ways",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Famous AI Breakthroughs",
  "course_id": "ai--famous-ai-breakthroughs",
  "chapter_id": "ai--famous-ai-breakthroughs--ch02-language",
  "title": "BERT: Understanding Context Both Ways",
  "emoji": "ğŸ†",
  "color": "#7C3AED",
  "description": "A 60-second lesson on BERT: Understanding Context Both Ways.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "In 'I went to the bank to fish,' does 'bank' mean finance or river? BERT reads both directions to decide.",
      "visual": "ğŸ”„"
    },
    "buildup": {
      "text": "Google's BERT (2018) was trained to predict masked words using context from both sides.",
      "visual": "ğŸ­"
    },
    "discovery": {
      "text": "Unlike previous models that read left-to-right, BERT understood words in full bidirectional context.",
      "visual": "ğŸ“–"
    },
    "twist": {
      "text": "BERT improved Google Search by understanding the intent behind queries, not just matching keywords.",
      "visual": "ğŸ”"
    },
    "climax": {
      "text": "Google called it the biggest change to search in five years. It affected 10% of all queries.",
      "visual": "ğŸ“Š"
    },
    "punchline": {
      "text": "Read forward and backward. Understanding lives in both directions.",
      "visual": "â†”ï¸"
    }
  },
  "quiz": {
    "question": "What made BERT different from previous language models?",
    "options": [
      "It understood words using context from both directions",
      "It could only read one word at a time",
      "It was the first model to process text"
    ],
    "correct": 0
  }
}
