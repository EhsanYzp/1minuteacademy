{
  "id": "ai--famous-ai-breakthroughs--t-the-transformer-paper-attention-is-all-you-need",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Famous AI Breakthroughs",
  "course_id": "ai--famous-ai-breakthroughs",
  "chapter_id": "ai--famous-ai-breakthroughs--ch02-language",
  "title": "The Transformer Paper: Attention Is All You Need",
  "emoji": "ğŸ†",
  "color": "#7C3AED",
  "description": "A 60-second lesson on The Transformer Paper: Attention Is All You Need.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "In 2017, eight Google researchers published a paper that would reshape all of artificial intelligence.",
      "visual": "ğŸ“„"
    },
    "buildup": {
      "text": "The transformer architecture replaced recurrence with self-attention â€” processing all words simultaneously.",
      "visual": "âš¡"
    },
    "discovery": {
      "text": "Self-attention lets every word attend to every other word, capturing relationships regardless of distance.",
      "visual": "ğŸ”—"
    },
    "twist": {
      "text": "The title 'Attention Is All You Need' was bold and provocative. It turned out to be exactly right.",
      "visual": "ğŸ¯"
    },
    "climax": {
      "text": "GPT, BERT, DALL-E, Whisper, AlphaFold â€” nearly every modern AI breakthrough uses transformers.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "One architecture. Every breakthrough. The paper that changed everything.",
      "visual": "ğŸ“œ"
    }
  },
  "quiz": {
    "question": "What was the key innovation in the transformer architecture?",
    "options": [
      "Self-attention replacing recurrence for parallel word processing",
      "Using deeper convolutional layers",
      "Processing words one at a time sequentially"
    ],
    "correct": 0
  }
}
