{
  "id": "ai--famous-ai-breakthroughs--t-resnet-going-deeper-than-ever",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Famous AI Breakthroughs",
  "course_id": "ai--famous-ai-breakthroughs",
  "chapter_id": "ai--famous-ai-breakthroughs--ch03-vision",
  "title": "ResNet: Going Deeper Than Ever",
  "emoji": "ğŸ†",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on ResNet: Going Deeper Than Ever.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "A 152-layer network should be impossible to train. Residual connections made it not just possible but dominant.",
      "visual": "ğŸ—ï¸"
    },
    "buildup": {
      "text": "Deeper networks suffered from vanishing gradients â€” learning signals faded to zero.",
      "visual": "ğŸ“‰"
    },
    "discovery": {
      "text": "ResNet added skip connections that let gradients flow through shortcut paths around layers.",
      "visual": "â­ï¸"
    },
    "twist": {
      "text": "The insight was simple: let each layer learn the difference from identity, not the full transformation.",
      "visual": "â•"
    },
    "climax": {
      "text": "ResNet won ImageNet 2015 and its skip connections now appear in virtually every deep network.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "The secret to going deeper: let information take shortcuts.",
      "visual": "ğŸ›¤ï¸"
    }
  },
  "quiz": {
    "question": "What problem did ResNet's skip connections solve?",
    "options": [
      "Vanishing gradients that prevented training very deep networks",
      "Slow inference speed",
      "Lack of training data"
    ],
    "correct": 0
  }
}
