{
  "id": "ai--history-of-ai--t-gpt-and-the-age-of-large-language-models",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The History of Artificial Intelligence",
  "course_id": "ai--history-of-ai",
  "chapter_id": "ai--history-of-ai--ch06-modern-era",
  "title": "GPT and the Age of Large Language Models",
  "emoji": "ğŸ“œ",
  "color": "#7C3AED",
  "description": "A fast breakdown of GPT and the Age of Large Language Models for builders.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "In 2020, OpenAI showed that scaling a language model to 175 billion parameters produces something eerie.",
      "visual": "ğŸ¤¯"
    },
    "buildup": {
      "text": "GPT-3 could write essays, code, poetry, and even legal briefs from a short prompt.",
      "visual": "âœï¸"
    },
    "discovery": {
      "text": "The recipe was simple: more data, more parameters, more compute. The results were not simple at all.",
      "visual": "ğŸ“ˆ"
    },
    "twist": {
      "text": "Nobody fully understands why scaling works so well. Emergent abilities appear unpredictably.",
      "visual": "âœ¨"
    },
    "climax": {
      "text": "ChatGPT reached 100 million users in two months â€” the fastest-growing app in history.",
      "visual": "ğŸ“±"
    },
    "punchline": {
      "text": "Scale it up and surprising things happen.",
      "visual": "ğŸš€"
    }
  },
  "quiz": {
    "question": "What made GPT-3 surprising to researchers?",
    "options": [
      "Scaling alone produced unexpected emergent abilities",
      "It was the first neural network ever built",
      "It could physically interact with the world"
    ],
    "correct": 0
  }
}
