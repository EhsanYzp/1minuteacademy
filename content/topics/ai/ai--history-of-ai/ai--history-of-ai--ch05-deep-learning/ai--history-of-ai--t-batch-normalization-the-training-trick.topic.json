{
  "id": "ai--history-of-ai--t-batch-normalization-the-training-trick",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The History of Artificial Intelligence",
  "course_id": "ai--history-of-ai",
  "chapter_id": "ai--history-of-ai--ch05-deep-learning",
  "title": "Batch Normalization: The Training Trick",
  "emoji": "ğŸ“œ",
  "color": "#7C3AED",
  "description": "A tiny lesson with a big payoff: Batch Normalization: The Training Trick.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "One simple trick made deep networks train 14 times faster. It became one of the most cited AI papers.",
      "visual": "âš¡"
    },
    "buildup": {
      "text": "Training deep networks was unstable â€” tiny changes in early layers created chaos in later ones.",
      "visual": "ğŸŒ€"
    },
    "discovery": {
      "text": "Batch normalization standardizes each layer's inputs, keeping the signal stable throughout training.",
      "visual": "ğŸ“"
    },
    "twist": {
      "text": "The original theory for why it works was later shown to be wrong. The technique worked for different reasons.",
      "visual": "ğŸ¤·"
    },
    "climax": {
      "text": "Batch norm enabled networks with hundreds of layers. Without it, modern AI wouldn't train at all.",
      "visual": "ğŸ—ï¸"
    },
    "punchline": {
      "text": "Sometimes the right trick matters more than the right theory.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "What was unusual about batch normalization's theoretical explanation?",
    "options": [
      "The original theory for why it works was later proven incorrect",
      "It was the first mathematical proof in AI",
      "No one ever tried to explain why it works"
    ],
    "correct": 0
  }
}
