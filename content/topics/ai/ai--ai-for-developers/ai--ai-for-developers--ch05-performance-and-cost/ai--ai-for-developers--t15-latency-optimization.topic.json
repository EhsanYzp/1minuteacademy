{
  "id": "ai--ai-for-developers--t15-latency-optimization",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI for Developers",
  "course_id": "ai--ai-for-developers",
  "chapter_id": "ai--ai-for-developers--ch05-performance-and-cost",
  "title": "Latency Optimization",
  "emoji": "ğŸ’»",
  "color": "#EF4444",
  "description": "Reduce AI response times to keep your app feeling snappy.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸŒ",
      "text": "An AI feature takes 12 seconds to respond. 60% of users abandon before seeing the result."
    },
    "buildup": {
      "visual": "â±ï¸",
      "text": "AI latency comes from: network round-trip, queue wait time, time-to-first-token, and total generation time."
    },
    "discovery": {
      "visual": "ğŸ”§",
      "text": "Optimise each layer: use edge deployments to cut network time, smaller models for faster generation, shorter prompts for less processing, and streaming for perceived speed."
    },
    "twist": {
      "visual": "ğŸ­",
      "text": "Sometimes the best optimisation is architectural: pre-compute likely responses, run AI tasks in the background, or show a useful UI while the AI thinks."
    },
    "climax": {
      "visual": "ğŸï¸",
      "text": "A well-optimised AI feature responds in under 2 seconds. Users don't even notice there's AI behind it."
    },
    "punchline": {
      "visual": "âš¡",
      "text": "Speed isn't a feature. It's the feature."
    }
  },
  "quiz": {
    "question": "Which is NOT a common source of AI response latency?",
    "options": [
      "Network round-trip time",
      "Token generation time",
      "Database schema design",
      "Queue wait time"
    ],
    "correct": 2
  }
}
