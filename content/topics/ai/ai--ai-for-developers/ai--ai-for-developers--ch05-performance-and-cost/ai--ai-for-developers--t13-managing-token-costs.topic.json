{
  "id": "ai--ai-for-developers--t13-managing-token-costs",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI for Developers",
  "course_id": "ai--ai-for-developers",
  "chapter_id": "ai--ai-for-developers--ch05-performance-and-cost",
  "title": "Managing Token Costs",
  "emoji": "ðŸ’»",
  "color": "#EF4444",
  "description": "Keep AI API costs under control as your app scales.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ðŸ’¸",
      "text": "A startup launches with GPT-4 for everything. First month's bill: $12,000. Their entire runway is 3 months."
    },
    "buildup": {
      "visual": "ðŸ§®",
      "text": "Every token costs money. Input tokens (your prompt + context) and output tokens (the response) are billed separately, with output often 3-4x more expensive."
    },
    "discovery": {
      "visual": "ðŸ“‰",
      "text": "Cost levers: use smaller models for simple tasks, cache frequent responses, shorten system prompts, limit max_tokens, and batch requests where possible."
    },
    "twist": {
      "visual": "ðŸ”€",
      "text": "Model routing â€” sending easy queries to cheap models and hard queries to expensive ones â€” can cut costs 60-80% with minimal quality loss."
    },
    "climax": {
      "visual": "ðŸ“Š",
      "text": "With monitoring dashboards, per-user budgets, and smart routing, you can scale to millions of requests without breaking the bank."
    },
    "punchline": {
      "visual": "ðŸ’°",
      "text": "The cheapest token is the one you never send."
    }
  },
  "quiz": {
    "question": "What is model routing?",
    "options": [
      "Sending all requests to the newest model",
      "Directing easy queries to cheap models and hard queries to expensive ones",
      "Routing requests through a VPN",
      "Splitting one prompt across multiple models"
    ],
    "correct": 1
  }
}
