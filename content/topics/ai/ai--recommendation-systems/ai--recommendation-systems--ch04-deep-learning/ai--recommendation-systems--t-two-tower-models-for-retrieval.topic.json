{
  "id": "ai--recommendation-systems--t-two-tower-models-for-retrieval",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Recommendation Systems",
  "course_id": "ai--recommendation-systems",
  "chapter_id": "ai--recommendation-systems--ch04-deep-learning",
  "title": "Two-Tower Models for Retrieval",
  "emoji": "ğŸ¯",
  "color": "#7C3AED",
  "description": "One-minute skill: Two-Tower Models for Retrieval.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "Score every item for every user? At scale that's billions of computations per request.",
      "visual": "ğŸ—ï¸"
    },
    "buildup": {
      "text": "Two-tower models encode users and items separately, then use fast nearest-neighbor lookup.",
      "visual": "ğŸ—¼"
    },
    "discovery": {
      "text": "User tower: encodes history and context. Item tower: encodes features. Dot product ranks them.",
      "visual": "ğŸ“"
    },
    "twist": {
      "text": "Pre-compute all item embeddings. At request time, only compute the user embedding. Sub-10ms.",
      "visual": "âš¡"
    },
    "climax": {
      "text": "YouTube, Spotify, and Pinterest all use two-tower retrieval at scale.",
      "visual": "ğŸ“ˆ"
    },
    "punchline": {
      "text": "Separate the towers. Unite the predictions.",
      "visual": "ğŸ¤"
    }
  },
  "quiz": {
    "question": "Why are two-tower models efficient at scale?",
    "options": [
      "Item embeddings are pre-computed; only user embedding is computed live",
      "They use a single model for everything",
      "They avoid embeddings entirely"
    ],
    "correct": 0
  }
}
