{
  "id": "ai--recommendation-systems--t-embedding-based-item-similarity",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Recommendation Systems",
  "course_id": "ai--recommendation-systems",
  "chapter_id": "ai--recommendation-systems--ch03-content-based",
  "title": "Embedding-Based Item Similarity",
  "emoji": "ğŸ¯",
  "color": "#7C3AED",
  "description": "A quick, practical guide to Embedding-Based Item Similarity.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "TF-IDF says 'car' and 'automobile' are different. Embeddings know they're the same.",
      "visual": "ğŸš—"
    },
    "buildup": {
      "text": "Embeddings map items into dense vector spaces where similar items are close together.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "Use pre-trained models (BERT, CLIP) or train custom embeddings on your interaction data.",
      "visual": "ğŸ§ "
    },
    "twist": {
      "text": "Multimodal embeddings combine text, images, and metadata into one vector per item.",
      "visual": "ğŸ”€"
    },
    "climax": {
      "text": "Nearest-neighbor search in embedding space retrieves similar items in milliseconds.",
      "visual": "âš¡"
    },
    "punchline": {
      "text": "Embed everything. Similarity becomes geometry.",
      "visual": "ğŸ“"
    }
  },
  "quiz": {
    "question": "Why are embeddings better than TF-IDF for similarity?",
    "options": [
      "They capture semantic meaning, not just word overlap",
      "They're faster to compute",
      "They use less storage"
    ],
    "correct": 0
  }
}
