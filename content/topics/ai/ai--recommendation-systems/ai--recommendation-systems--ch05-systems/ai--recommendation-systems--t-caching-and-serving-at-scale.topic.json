{
  "id": "ai--recommendation-systems--t-caching-and-serving-at-scale",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Recommendation Systems",
  "course_id": "ai--recommendation-systems",
  "chapter_id": "ai--recommendation-systems--ch05-systems",
  "title": "Caching and Serving at Scale",
  "emoji": "ğŸ¯",
  "color": "#7C3AED",
  "description": "A quick, practical guide to Caching and Serving at Scale.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "100 million users, each getting 50 recommendations. That's 5 billion scores per refresh cycle.",
      "visual": "ğŸ”¢"
    },
    "buildup": {
      "text": "Pre-compute recommendations for active users. Cache results. Serve from memory, not models.",
      "visual": "ğŸ’¾"
    },
    "discovery": {
      "text": "Cache strategies: user-level caches, item-level caches, and session-level real-time overrides.",
      "visual": "ğŸ—„ï¸"
    },
    "twist": {
      "text": "Stale recommendations from yesterday's cache can outperform slow real-time models.",
      "visual": "â°"
    },
    "climax": {
      "text": "Hybrid: serve cached baseline, then re-rank with real-time signals on top.",
      "visual": "ğŸ”„"
    },
    "punchline": {
      "text": "Fresh enough beats perfectly fresh.",
      "visual": "ğŸ¥—"
    }
  },
  "quiz": {
    "question": "Why are cached recommendations useful?",
    "options": [
      "They serve results at low latency without real-time model inference",
      "They're always more accurate",
      "They eliminate the need for models"
    ],
    "correct": 0
  }
}
