{
  "id": "ai--ai-myths-and-misconceptions--t-ai-hallucinations-confident-and-wrong",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Myths and Misconceptions",
  "course_id": "ai--ai-myths-and-misconceptions",
  "chapter_id": "ai--ai-myths-and-misconceptions--ch04-perfection",
  "title": "AI Hallucinations: Confident and Wrong",
  "emoji": "ğŸ”®",
  "color": "#7C3AED",
  "description": "A short lesson to help you apply AI Hallucinations: Confident and Wrong.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "A lawyer cites six court cases in a brief. The judge checks. None of them exist. AI invented them.",
      "visual": "âš–ï¸"
    },
    "buildup": {
      "text": "LLMs generate text that sounds authoritative regardless of whether the content is factually correct.",
      "visual": "ğŸ“¢"
    },
    "discovery": {
      "text": "Hallucinations happen because the model predicts plausible words, not verified facts.",
      "visual": "ğŸ²"
    },
    "twist": {
      "text": "The more confidently AI states something, the more people trust it â€” even when it's completely fabricated.",
      "visual": "ğŸ˜¨"
    },
    "climax": {
      "text": "Retrieval-augmented generation helps ground answers in real sources, but hallucinations persist.",
      "visual": "ğŸ“š"
    },
    "punchline": {
      "text": "It speaks with certainty. That's the most dangerous part.",
      "visual": "ğŸ­"
    }
  },
  "quiz": {
    "question": "Why do AI hallucinations occur in language models?",
    "options": [
      "Models predict plausible-sounding words rather than verified facts",
      "AI deliberately lies to confuse users",
      "Hallucinations only happen with outdated models"
    ],
    "correct": 0
  }
}
