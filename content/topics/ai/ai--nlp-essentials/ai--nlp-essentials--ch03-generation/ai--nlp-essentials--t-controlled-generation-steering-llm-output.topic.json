{
  "id": "ai--nlp-essentials--t-controlled-generation-steering-llm-output",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch03-generation",
  "title": "Controlled Generation: Steering LLM Output",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A quick, practical guide to Controlled Generation: Steering LLM Output.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "You want the model to write in formal English, under 100 words, with no jargon.",
      "visual": "ğŸ›ï¸"
    },
    "buildup": {
      "text": "Controlled generation constrains model output by style, length, tone, or format.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "System prompts, few-shot examples, and structured output schemas all steer generation.",
      "visual": "ğŸ§­"
    },
    "twist": {
      "text": "Hard constraints (JSON schema enforcement) are more reliable than soft prompts.",
      "visual": "ğŸ”’"
    },
    "climax": {
      "text": "Tools like Outlines and Instructor force valid JSON/schema output from any LLM.",
      "visual": "ğŸ”§"
    },
    "punchline": {
      "text": "Don't hope for the right format. Enforce it.",
      "visual": "âœ…"
    }
  },
  "quiz": {
    "question": "What is the most reliable way to control LLM output format?",
    "options": [
      "Hard schema enforcement like JSON mode",
      "Asking nicely in the prompt",
      "Increasing temperature"
    ],
    "correct": 0
  }
}
