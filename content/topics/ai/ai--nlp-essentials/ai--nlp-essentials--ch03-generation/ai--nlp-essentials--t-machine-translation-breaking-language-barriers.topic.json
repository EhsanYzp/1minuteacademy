{
  "id": "ai--nlp-essentials--t-machine-translation-breaking-language-barriers",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch03-generation",
  "title": "Machine Translation: Breaking Language Barriers",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A micro-lesson that makes Machine Translation: Breaking Language Barriers usable.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "Type in English, read in Japanese. Neural translation made this seamless.",
      "visual": "ğŸŒ"
    },
    "buildup": {
      "text": "Sequence-to-sequence models encode source language and decode into target language.",
      "visual": "ğŸ”„"
    },
    "discovery": {
      "text": "Attention mechanism lets the decoder focus on relevant source words during translation.",
      "visual": "ğŸ‘ï¸"
    },
    "twist": {
      "text": "Low-resource languages have little parallel dataâ€”translation quality drops dramatically.",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "Multilingual models (NLLB, mBART) transfer knowledge across 200+ languages.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "One model, hundreds of language pairs.",
      "visual": "ğŸ”€"
    }
  },
  "quiz": {
    "question": "Why do low-resource languages have worse translation?",
    "options": [
      "They have less parallel training data",
      "They have simpler grammar",
      "Their words are shorter"
    ],
    "correct": 0
  }
}
