{
  "id": "ai--nlp-essentials--t-sentence-embeddings-for-semantic-search",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch01-text-basics",
  "title": "Sentence Embeddings for Semantic Search",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A fast breakdown of Sentence Embeddings for Semantic Search for builders.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "Search for 'affordable flights' and find 'cheap plane tickets.' Keywords don't match, meaning does.",
      "visual": "âœˆï¸"
    },
    "buildup": {
      "text": "Sentence embeddings compress an entire sentence into one dense vector.",
      "visual": "ğŸ“¦"
    },
    "discovery": {
      "text": "Models like Sentence-BERT and E5 are trained on paraphrase and NLI data for similarity.",
      "visual": "ğŸ”—"
    },
    "twist": {
      "text": "Cosine similarity between vectors measures semantic closenessâ€”no keyword matching needed.",
      "visual": "ğŸ“"
    },
    "climax": {
      "text": "Sentence embeddings power semantic search, RAG retrieval, and duplicate detection.",
      "visual": "ğŸ”"
    },
    "punchline": {
      "text": "Match meaning, not words.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "How do sentence embeddings enable semantic search?",
    "options": [
      "By comparing vector similarity instead of keyword matches",
      "By counting shared words",
      "By using regex patterns"
    ],
    "correct": 0
  }
}
