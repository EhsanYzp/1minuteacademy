{
  "id": "ai--nlp-essentials--t-contextual-embeddings-one-word-many-meanings",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch01-text-basics",
  "title": "Contextual Embeddings: One Word, Many Meanings",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A micro-lesson that makes Contextual Embeddings: One Word, Many Meanings usable.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "'Bank' means different things in 'river bank' and 'bank account.' Static embeddings miss this.",
      "visual": "ğŸ¦"
    },
    "buildup": {
      "text": "Contextual embeddings give each word a different vector depending on its surrounding sentence.",
      "visual": "ğŸ”„"
    },
    "discovery": {
      "text": "BERT and GPT produce contextual embeddingsâ€”every token vector is context-dependent.",
      "visual": "ğŸ§©"
    },
    "twist": {
      "text": "This is why transformers excel at disambiguationâ€”they see the whole sentence at once.",
      "visual": "ğŸ‘ï¸"
    },
    "climax": {
      "text": "Contextual embeddings replaced Word2Vec as the default for almost every NLP task.",
      "visual": "ğŸ”€"
    },
    "punchline": {
      "text": "Same word, different context, different vector.",
      "visual": "ğŸ­"
    }
  },
  "quiz": {
    "question": "What makes contextual embeddings different from Word2Vec?",
    "options": [
      "The same word gets different vectors based on context",
      "They use fewer dimensions",
      "They ignore word order"
    ],
    "correct": 0
  }
}
