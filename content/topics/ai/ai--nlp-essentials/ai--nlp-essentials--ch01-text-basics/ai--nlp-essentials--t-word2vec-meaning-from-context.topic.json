{
  "id": "ai--nlp-essentials--t-word2vec-meaning-from-context",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch01-text-basics",
  "title": "Word2Vec: Meaning from Context",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A quick, practical guide to Word2Vec: Meaning from Context.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "'You shall know a word by the company it keeps.' Word2Vec made that literal.",
      "visual": "ğŸ‘¥"
    },
    "buildup": {
      "text": "Word2Vec trains a neural network to predict a word from its neighbors (or vice versa).",
      "visual": "ğŸ§ "
    },
    "discovery": {
      "text": "The hidden layer weights become word embeddingsâ€”vectors that capture meaning.",
      "visual": "ğŸ“"
    },
    "twist": {
      "text": "Famous result: king âˆ’ man + woman = queen. Arithmetic on meaning.",
      "visual": "ğŸ‘‘"
    },
    "climax": {
      "text": "Word2Vec was the bridge from sparse BoW to dense neural representations.",
      "visual": "ğŸŒ‰"
    },
    "punchline": {
      "text": "Context defines meaning. Embeddings encode it.",
      "visual": "ğŸ”®"
    }
  },
  "quiz": {
    "question": "What does Word2Vec learn from?",
    "options": [
      "Predicting words from their surrounding context",
      "Dictionary definitions",
      "Image captions"
    ],
    "correct": 0
  }
}
