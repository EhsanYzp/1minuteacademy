{
  "id": "ai--nlp-essentials--t-multilingual-models-one-model-many-languages",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch06-multilingual",
  "title": "Multilingual Models: One Model, Many Languages",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A short lesson to help you apply Multilingual Models: One Model, Many Languages.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "Train on English. Test on German. It worksâ€”zero-shot cross-lingual transfer.",
      "visual": "ğŸŒ"
    },
    "buildup": {
      "text": "Multilingual models like mBERT and XLM-R are trained on 100+ languages simultaneously.",
      "visual": "ğŸ“š"
    },
    "discovery": {
      "text": "Shared subword vocabularies and training objectives create a universal language space.",
      "visual": "ğŸ”—"
    },
    "twist": {
      "text": "Low-resource languages get less representation and perform worseâ€”the 'curse of multilinguality.'",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "For critical languages, fine-tune on language-specific data after multilingual pre-training.",
      "visual": "ğŸ¯"
    },
    "punchline": {
      "text": "One model understands the world's languages. Unevenly.",
      "visual": "ğŸŒ"
    }
  },
  "quiz": {
    "question": "What is cross-lingual transfer?",
    "options": [
      "Training on one language and testing on another",
      "Translating before training",
      "Using separate models per language"
    ],
    "correct": 0
  }
}
