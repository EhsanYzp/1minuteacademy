{
  "id": "ai--nlp-essentials--t-document-parsing-and-chunking-for-rag",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "NLP Essentials",
  "course_id": "ai--nlp-essentials",
  "chapter_id": "ai--nlp-essentials--ch04-information",
  "title": "Document Parsing and Chunking for RAG",
  "emoji": "ğŸ’¬",
  "color": "#D97706",
  "description": "A 60-second lesson on Document Parsing and Chunking for RAG.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "You have a 200-page PDF. The LLM's context window is 8K tokens. Chunking bridges the gap.",
      "visual": "ğŸ“„"
    },
    "buildup": {
      "text": "Chunking splits documents into pieces small enough for embedding and retrieval.",
      "visual": "âœ‚ï¸"
    },
    "discovery": {
      "text": "Strategies: fixed-size chunks, sentence-based, paragraph-based, or semantic splitting.",
      "visual": "ğŸ§©"
    },
    "twist": {
      "text": "Bad chunks split ideas in half. Good chunks preserve coherent meaning units.",
      "visual": "ğŸ’¡"
    },
    "climax": {
      "text": "Overlap between chunks (e.g., 50 tokens) prevents losing context at boundaries.",
      "visual": "ğŸ”„"
    },
    "punchline": {
      "text": "Chunk smart. Retrieve right. Answer well.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "Why do chunks need overlap?",
    "options": [
      "To prevent losing context at chunk boundaries",
      "To increase file size",
      "To slow down retrieval"
    ],
    "correct": 0
  }
}
