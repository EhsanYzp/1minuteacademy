{
  "id": "ai--rag-basics--t01-what-is-rag",
  "version": 1,
  "subject": "AI",
  "subcategory": "RAG Basics",
  "course_id": "ai--rag-basics",
  "chapter_id": "ai--rag-basics--ch01-why-rag",
  "title": "What Is RAG?",
  "emoji": "ğŸ”",
  "color": "#EF4444",
  "description": "The pattern that grounds LLMs in your actual data.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“š",
      "text": "Your chatbot confidently tells a customer your return policy is 30 days. It's actually 14. The model was guessing â€” it doesn't know your policies."
    },
    "buildup": {
      "visual": "ğŸ”",
      "text": "Retrieval-Augmented Generation (RAG) fixes this by fetching relevant documents before the model answers. The model reads your real data, then responds based on what it found."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Step 1: User asks a question. Step 2: Search your knowledge base for relevant passages. Step 3: Inject those passages into the prompt. Step 4: The model answers using the retrieved context."
    },
    "twist": {
      "visual": "âš¡",
      "text": "RAG doesn't change the model's weights â€” it changes the model's input. Same model, but now it has the right information at generation time."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "RAG is the most practical way to make LLMs answer questions about your data without fine-tuning. It's fast to build and easy to update."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Don't teach the model your data. Show it your data at query time."
    }
  },
  "quiz": {
    "question": "What does RAG do differently from a standalone LLM?",
    "options": [
      "It retrains the model on new data",
      "It retrieves relevant documents and includes them in the prompt",
      "It reduces the model's parameter count",
      "It replaces the LLM with a search engine"
    ],
    "correct": 1
  }
}
