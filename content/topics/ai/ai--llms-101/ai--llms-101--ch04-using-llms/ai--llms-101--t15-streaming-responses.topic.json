{
  "id": "ai--llms-101--t15-streaming-responses",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch04-using-llms",
  "title": "Streaming Responses",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "Get tokens as they're generated instead of waiting for the full answer.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸŒŠ",
      "text": "You hit the API and wait 15 seconds for a long response. The user stares at a blank screen. Streaming shows words appearing in real time â€” same total time, vastly better UX."
    },
    "buildup": {
      "visual": "ğŸ“º",
      "text": "Streaming returns tokens one by one (or in small chunks) as the model generates them. You display each token immediately. The user sees the answer forming letter by letter."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Time-to-first-token (TTFT) is usually under 500ms with streaming, even for responses that take 10+ seconds total. Perceived speed improves dramatically."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Streaming complicates your code: you need to handle partial JSON, incomplete sentences, and connection drops mid-stream. It's worth it for user-facing features; skip it for backend processing."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "If a human is watching, stream. If a machine is processing, wait for the full response. Match the delivery mode to the consumer."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Same answer, same time. But streaming turns waiting into watching â€” and watching feels fast."
    }
  },
  "quiz": {
    "question": "What is the main benefit of streaming LLM responses?",
    "options": [
      "It reduces the total generation time",
      "It shows tokens as they're generated, improving perceived speed",
      "It uses fewer tokens",
      "It eliminates hallucinations"
    ],
    "correct": 1
  }
}
