{
  "id": "ai--llms-101--t30-temperature-and-sampling",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch04-using-llms",
  "title": "Temperature & Sampling",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "Control creativity vs consistency in LLM outputs with sampling parameters.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸŒ¡ï¸",
      "text": "Ask an LLM the same question twice. With temperature 0, you get the same answer. With temperature 1, you might get something wildly creative â€” or nonsensical."
    },
    "buildup": {
      "visual": "ğŸ²",
      "text": "Temperature controls how 'random' the model's token selection is. At 0, it always picks the most probable token (greedy). At 1, it samples proportionally from the distribution. Above 1, rare tokens become more likely."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Top-p (nucleus sampling) is a complementary control: it only considers tokens whose cumulative probability reaches p. Top-p of 0.9 means 'consider the most likely tokens that together cover 90% of the probability.'"
    },
    "twist": {
      "visual": "âš¡",
      "text": "For factual Q&A, use low temperature (0-0.3). For creative writing, use higher (0.7-1.0). There's no universal best setting â€” it depends entirely on your use case."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Temperature and top-p are the two most impactful parameters you control at inference time. They shape everything from reliability to creativity."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Temperature is the creativity dial. Turn it down for facts, up for fiction, and learn where your sweet spot lives."
    }
  },
  "quiz": {
    "question": "What happens when you set temperature to 0?",
    "options": [
      "The model refuses to respond",
      "The model always picks the most probable token (deterministic output)",
      "The model generates random nonsense",
      "The model outputs tokens in reverse order"
    ],
    "correct": 1
  }
}
