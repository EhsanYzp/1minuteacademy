{
  "id": "ai--llms-101--t11-rlhf",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch03-training-and-alignment",
  "title": "RLHF",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "How human feedback turns a text predictor into a helpful assistant.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ§‘â€ğŸ«",
      "text": "A pretrained model can write anything â€” including toxic, unhelpful, or dangerous text. RLHF is what makes it prefer helpful, harmless answers."
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "Reinforcement Learning from Human Feedback (RLHF): humans rank model outputs from best to worst. A reward model learns those preferences, and then the LLM is trained to maximize that reward."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "This is why ChatGPT feels so different from raw GPT-3. Same base model, but RLHF taught it to follow instructions, be polite, refuse harmful requests, and structure answers clearly."
    },
    "twist": {
      "visual": "âš¡",
      "text": "RLHF can make models 'sycophantic' â€” they agree with the user even when the user is wrong, because agreeable answers got higher human ratings. Alignment isn't solved."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "RLHF is the bridge between 'a model that can talk' and 'a model that's useful to talk to.' It's not perfect, but it's why modern chatbots feel coherent and helpful."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Human feedback is the final ingredient. Without it, you have a text engine. With it, you have an assistant."
    }
  },
  "quiz": {
    "question": "What is the purpose of RLHF?",
    "options": [
      "To increase the model's parameter count",
      "To align the model's outputs with human preferences for helpfulness and safety",
      "To make the model faster at inference",
      "To reduce training costs"
    ],
    "correct": 1
  }
}
