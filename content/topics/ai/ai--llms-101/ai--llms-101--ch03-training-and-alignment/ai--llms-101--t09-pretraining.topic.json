{
  "id": "ai--llms-101--t09-pretraining",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch03-training-and-alignment",
  "title": "Pretraining",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "How LLMs learn language from the entire internet.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸŒ",
      "text": "Before you ever chat with GPT, it spent months reading trillions of words from the web, books, and code. That's pretraining â€” the education before the job."
    },
    "buildup": {
      "visual": "ğŸ“š",
      "text": "During pretraining, the model predicts the next token in massive text corpora. It adjusts billions of weights to minimize prediction errors across trillions of examples."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Pretraining is where the model absorbs grammar, facts, reasoning patterns, code syntax, and even cultural knowledge. It takes weeks on thousands of GPUs and costs millions of dollars."
    },
    "twist": {
      "visual": "âš¡",
      "text": "A pretrained model is not a chatbot. It's a text completer. Ask it a question and it might continue with another question instead of answering. That's where fine-tuning comes in."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Pretraining builds the foundation. Everything that follows â€” fine-tuning, RLHF, instruction following â€” is shaping raw capability into useful behavior."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Pretraining: read everything, predict everything. It's blunt, expensive, and irreplaceable."
    }
  },
  "quiz": {
    "question": "What does a model learn during pretraining?",
    "options": [
      "How to follow specific user instructions",
      "Language patterns, facts, and reasoning from massive text corpora",
      "Only how to write code",
      "How to search the internet"
    ],
    "correct": 1
  }
}
