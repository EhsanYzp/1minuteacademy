{
  "id": "ai--llms-101--t33-small-language-models",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch05-model-landscape",
  "title": "Small Language Models",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "Why smaller models are often the smarter choice for production applications.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ",
      "text": "A 7B-parameter model runs on a laptop. A 70B model needs a GPU cluster. For many tasks, the small one performs 95% as well at 1% of the cost."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "Small language models (SLMs) like Phi, Gemma, and Mistral 7B are trained with curated data and distillation techniques that punch far above their parameter count."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "For focused tasks â€” classification, extraction, summarisation of specific domains â€” a fine-tuned 7B model can match or beat a general-purpose 70B model."
    },
    "twist": {
      "visual": "âš¡",
      "text": "SLMs enable on-device AI, offline operation, and microsecond latency. They unlock use cases where sending data to the cloud isn't an option â€” healthcare, military, and edge IoT."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "The trend is clear: bigger isn't always better. The best production teams use the smallest model that meets their quality bar, then scale up only when needed."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The best model isn't the biggest. It's the smallest one that gets the job done."
    }
  },
  "quiz": {
    "question": "What is a key advantage of small language models (SLMs)?",
    "options": [
      "They are always more accurate than large models",
      "They can run on-device with lower cost and latency",
      "They don't require any training",
      "They have larger context windows"
    ],
    "correct": 1
  }
}
