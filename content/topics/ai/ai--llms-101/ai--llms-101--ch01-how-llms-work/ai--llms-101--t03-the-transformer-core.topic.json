{
  "id": "ai--llms-101--t03-the-transformer-core",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch01-how-llms-work",
  "title": "The Transformer Core",
  "emoji": "üß†",
  "color": "#EF4444",
  "description": "Attention and parallelism ‚Äî the architecture that made LLMs possible.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "‚öôÔ∏è",
      "text": "Before transformers, language models processed words one at a time ‚Äî slow and forgetful over long texts. A 2017 paper changed everything."
    },
    "buildup": {
      "visual": "üîß",
      "text": "Transformers process all tokens in parallel and use 'attention' to let each token look at every other token. The word 'it' can attend to 'dog' five sentences earlier in a single step."
    },
    "discovery": {
      "visual": "üí°",
      "text": "Parallelism is why training is fast: GPUs love doing many operations at once. Older sequential models couldn't exploit GPU power effectively."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "Attention scales quadratically with sequence length ‚Äî doubling the input quadruples the compute. That's why context windows have hard limits."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "Every modern LLM ‚Äî GPT, Claude, Gemini, Llama ‚Äî is a transformer. The architecture won. Understanding it unlocks understanding of every model built on it."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "The transformer's superpower: let every word look at every other word, all at once."
    }
  },
  "quiz": {
    "question": "Why did the transformer architecture enable modern LLMs?",
    "options": [
      "It processes words sequentially for accuracy",
      "It allows parallel processing and attention across all tokens",
      "It eliminated the need for training data",
      "It uses convolutions instead of attention"
    ],
    "correct": 1
  }
}
