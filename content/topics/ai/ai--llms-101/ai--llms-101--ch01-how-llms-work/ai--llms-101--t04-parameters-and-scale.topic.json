{
  "id": "ai--llms-101--t04-parameters-and-scale",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch01-how-llms-work",
  "title": "Parameters & Scale",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "What a billion parameters means and why scale matters.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“Š",
      "text": "GPT-3 has 175 billion parameters. GPT-4 reportedly has over a trillion. What even is a parameter, and why do more of them help?"
    },
    "buildup": {
      "visual": "ğŸ”¢",
      "text": "A parameter is a single tunable number (a weight) in the neural network. During training, the model adjusts these numbers to better predict the next token."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "More parameters = more capacity to memorize patterns, nuances, and relationships. A 7B model writes decent text. A 70B model handles subtle reasoning that the smaller one can't."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Scale has diminishing returns. Going from 7B to 70B is a bigger jump than 70B to 700B. And bigger models are slower and more expensive to run."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "The right model size depends on your task. Classification? 7B is fine. Complex multi-step reasoning? You might need 70B+. Match scale to need, not to hype."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "More parameters, more capability, more cost. Pick the size that's big enough â€” not the biggest."
    }
  },
  "quiz": {
    "question": "What is a parameter in an LLM?",
    "options": [
      "A setting you configure in the API call",
      "A tunable numerical weight adjusted during training",
      "A rule written by engineers",
      "A unit of text the model processes"
    ],
    "correct": 1
  }
}
