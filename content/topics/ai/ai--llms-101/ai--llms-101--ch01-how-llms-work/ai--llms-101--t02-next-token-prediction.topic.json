{
  "id": "ai--llms-101--t02-next-token-prediction",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch01-how-llms-work",
  "title": "Next-Token Prediction",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "The single mechanism that powers all LLM capabilities.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ”®",
      "text": "Writing code, translating languages, answering questions â€” these feel like completely different skills. But inside the model, they're all the same operation."
    },
    "buildup": {
      "visual": "ğŸ°",
      "text": "Given all the tokens so far, the model assigns a probability to every possible next token. 'The cat sat on theâ€¦' â†’ 'mat' 32%, 'floor' 18%, 'roof' 7%â€¦"
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "The model picks one token (based on temperature settings), adds it to the sequence, and repeats. One token at a time, a paragraph emerges."
    },
    "twist": {
      "visual": "âš¡",
      "text": "This means the model has no plan for the whole sentence. It commits to each word before knowing how the paragraph ends. Long-range coherence is a statistical miracle, not a guarantee."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Every capability â€” summarization, translation, chat â€” is an emergent behavior of next-token prediction at massive scale. No special code for each task."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "One token at a time. That's literally all it does â€” and it's enough to fake general intelligence."
    }
  },
  "quiz": {
    "question": "How does an LLM generate a full paragraph?",
    "options": [
      "It writes the whole paragraph at once",
      "It predicts one token at a time and appends each to the sequence",
      "It retrieves pre-written paragraphs from memory",
      "It uses a separate model for each sentence"
    ],
    "correct": 1
  }
}
