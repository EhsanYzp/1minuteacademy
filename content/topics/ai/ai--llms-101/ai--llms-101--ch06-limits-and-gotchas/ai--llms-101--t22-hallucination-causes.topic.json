{
  "id": "ai--llms-101--t22-hallucination-causes",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch06-limits-and-gotchas",
  "title": "Why LLMs Hallucinate",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "The mechanics behind confident but false outputs.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ«§",
      "text": "The model invents a research paper that doesn't exist â€” title, authors, journal, year. Every detail is made up, yet it reads perfectly. Why does this happen?"
    },
    "buildup": {
      "visual": "ğŸ”",
      "text": "LLMs are trained to predict plausible next tokens, not factual ones. If the pattern 'Smith et al., 2021, published in Nature' is plausible, the model writes it â€” even if no such paper exists."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Hallucinations increase when the model has low-confidence predictions but must still output something. It can't say 'I don't know' naturally â€” it was trained to always continue."
    },
    "twist": {
      "visual": "âš¡",
      "text": "You can't fully eliminate hallucinations without grounding the model in verified sources. Prompting 'only state facts' helps, but the model can't distinguish its real knowledge from its fabrications."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Reduce hallucinations with: low temperature, retrieval-augmented generation, explicit 'say I don't know if unsure' instructions, and post-generation fact-checking."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The model doesn't know what it knows. That's the core problem, and it's not solved yet."
    }
  },
  "quiz": {
    "question": "What fundamentally causes LLM hallucinations?",
    "options": [
      "Intentional deception by the model",
      "The model is optimized for plausibility, not factual accuracy",
      "Insufficient GPU memory during inference",
      "Bugs in the tokenizer"
    ],
    "correct": 1
  }
}
