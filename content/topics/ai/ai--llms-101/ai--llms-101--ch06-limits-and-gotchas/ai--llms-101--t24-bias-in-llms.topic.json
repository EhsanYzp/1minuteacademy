{
  "id": "ai--llms-101--t24-bias-in-llms",
  "version": 1,
  "subject": "AI",
  "subcategory": "LLMs 101",
  "course_id": "ai--llms-101",
  "chapter_id": "ai--llms-101--ch06-limits-and-gotchas",
  "title": "Bias in LLMs",
  "emoji": "ğŸ§ ",
  "color": "#EF4444",
  "description": "How training data biases show up in model outputs.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "âš–ï¸",
      "text": "Ask an LLM to write a story about a nurse and a doctor. Notice who gets which gender? That's not the model's choice â€” it's the training data's pattern."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "LLMs absorb every bias in their training data: gender stereotypes, cultural assumptions, geographic skew (overrepresenting English and Western perspectives)."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Bias shows up in subtle ways: completion suggestions, assumed defaults, tone differences when discussing different groups. It's often invisible unless you test for it specifically."
    },
    "twist": {
      "visual": "âš¡",
      "text": "RLHF reduces some biases but can introduce new ones. If human raters have their own biases, the model learns to align with those too."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Test your LLM outputs across demographic categories. Use bias benchmarks. Don't assume the model is neutral just because it seems polite."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The model doesn't have opinions. But it does have patterns â€” and those patterns aren't always fair."
    }
  },
  "quiz": {
    "question": "Where does bias in LLMs primarily come from?",
    "options": [
      "Deliberate programming by developers",
      "The biases present in training data and human feedback",
      "The model's personal beliefs",
      "Random noise during inference"
    ],
    "correct": 1
  }
}
