{
  "id": "ai--generative-ai--t-text-to-image-how-it-works",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Generative AI",
  "course_id": "ai--generative-ai",
  "chapter_id": "ai--generative-ai--ch03-image",
  "title": "Text-to-Image: How It Works",
  "emoji": "ğŸ¨",
  "color": "#DB2777",
  "description": "A tiny lesson with a big payoff: Text-to-Image: How It Works.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Type 'sunset over a cyberpunk city, neon lights, rain.' An image materializes.",
      "visual": "ğŸŒ†"
    },
    "buildup": {
      "text": "A text encoder (CLIP) converts the prompt into an embedding that guides diffusion.",
      "visual": "ğŸ§­"
    },
    "discovery": {
      "text": "Cross-attention layers let the denoising network 'see' the text at every step.",
      "visual": "ğŸ‘ï¸"
    },
    "twist": {
      "text": "Classifier-free guidance (CFG) strengthens text adherenceâ€”higher CFG = more prompt-faithful.",
      "visual": "ğŸšï¸"
    },
    "climax": {
      "text": "SDXL, DALL-E 3, and Midjourney v6 produce near-photorealistic results from text alone.",
      "visual": "ğŸ“¸"
    },
    "punchline": {
      "text": "Words in. Worlds out.",
      "visual": "ğŸŒ"
    }
  },
  "quiz": {
    "question": "What does classifier-free guidance control?",
    "options": [
      "How closely the image follows the text prompt",
      "The number of diffusion steps",
      "Image resolution"
    ],
    "correct": 0
  }
}
