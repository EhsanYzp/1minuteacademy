{
  "id": "ai--generative-ai--t-sampling-strategies-temperature-and-top-p",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Generative AI",
  "course_id": "ai--generative-ai",
  "chapter_id": "ai--generative-ai--ch02-text",
  "title": "Sampling Strategies: Temperature and Top-p",
  "emoji": "ğŸ¨",
  "color": "#DB2777",
  "description": "A quick win: understand Sampling Strategies: Temperature and Top-p.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Temperature 0: always the same answer. Temperature 2: creative nonsense.",
      "visual": "ğŸŒ¡ï¸"
    },
    "buildup": {
      "text": "Temperature scales the probability distribution before sampling. Lower = more deterministic.",
      "visual": "ğŸ“Š"
    },
    "discovery": {
      "text": "Top-p (nucleus sampling) cuts the tail: only sample from tokens covering p% of probability.",
      "visual": "âœ‚ï¸"
    },
    "twist": {
      "text": "Top-k limits to the k most likely tokens. Combine top-p and top-k for best control.",
      "visual": "ğŸšï¸"
    },
    "climax": {
      "text": "For factual tasks: temperature 0. For creative writing: temperature 0.7â€“1.0 + top-p 0.9.",
      "visual": "ğŸ¯"
    },
    "punchline": {
      "text": "Tune the randomness. Shape the creativity.",
      "visual": "ğŸ¨"
    }
  },
  "quiz": {
    "question": "What does a lower temperature produce?",
    "options": [
      "More deterministic, predictable output",
      "More creative, diverse output",
      "Longer responses"
    ],
    "correct": 0
  }
}
