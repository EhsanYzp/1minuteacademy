{
  "id": "ai--generative-ai--t-autoencoders-compress-and-reconstruct",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Generative AI",
  "course_id": "ai--generative-ai",
  "chapter_id": "ai--generative-ai--ch01-foundations",
  "title": "Autoencoders: Compress and Reconstruct",
  "emoji": "ğŸ¨",
  "color": "#DB2777",
  "description": "A 1-minute de-risking session on Autoencoders: Compress and Reconstruct.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "Squeeze an image through a tiny bottleneck, then rebuild it. What survives is the essence.",
      "visual": "ğŸ—œï¸"
    },
    "buildup": {
      "text": "Autoencoders have an encoder (compress) and decoder (reconstruct) with a narrow middle layer.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "The bottleneck forces the model to learn a compact, meaningful representation.",
      "visual": "ğŸ’"
    },
    "twist": {
      "text": "Standard autoencoders don't generate well. VAEs add randomness to the bottleneck.",
      "visual": "ğŸ²"
    },
    "climax": {
      "text": "VAEs (variational autoencoders) enable smooth interpolation between data points.",
      "visual": "ğŸŒŠ"
    },
    "punchline": {
      "text": "Compress to understand. Sample to create.",
      "visual": "ğŸ”®"
    }
  },
  "quiz": {
    "question": "What do VAEs add to standard autoencoders?",
    "options": [
      "Randomness in the bottleneck for generation",
      "More layers",
      "Faster training"
    ],
    "correct": 0
  }
}
