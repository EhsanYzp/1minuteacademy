{
  "id": "ai--generative-ai--t-the-gan-framework-revisited",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Generative AI",
  "course_id": "ai--generative-ai",
  "chapter_id": "ai--generative-ai--ch01-foundations",
  "title": "The GAN Framework Revisited",
  "emoji": "ğŸ¨",
  "color": "#DB2777",
  "description": "A tiny lesson with a big payoff: The GAN Framework Revisited.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "A counterfeiter vs a detective. The counterfeiter keeps improving until the detective gives up.",
      "visual": "ğŸ­"
    },
    "buildup": {
      "text": "GANs use adversarial training: generator creates, discriminator judges, both improve.",
      "visual": "âš”ï¸"
    },
    "discovery": {
      "text": "Nash equilibrium: the generator produces images the discriminator can't distinguish from real.",
      "visual": "ğŸ¤"
    },
    "twist": {
      "text": "Training instability plagued early GANs. Wasserstein loss and spectral normalization helped.",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "GANs excel at sharp, photorealistic output but are largely replaced by diffusion models now.",
      "visual": "ğŸ”„"
    },
    "punchline": {
      "text": "The rivalry that birthed an era of generation.",
      "visual": "ğŸ›ï¸"
    }
  },
  "quiz": {
    "question": "What stabilized GAN training?",
    "options": [
      "Wasserstein loss and spectral normalization",
      "Larger datasets only",
      "Removing the discriminator"
    ],
    "correct": 0
  }
}
