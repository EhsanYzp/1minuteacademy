{
  "id": "ai--ai-foundations--t13-labeling-data",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch03-data-matters",
  "title": "Labeling Data",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "The unglamorous work that makes or breaks your model.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ·ï¸",
      "text": "You need 50,000 labeled images by next month. You hire a team of annotators. Three weeks in, you realize they all disagree on edge cases."
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "Labeling is the process of tagging data with the correct answer so supervised models can learn. It sounds simple until you hit ambiguity."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Clear labeling guidelines with examples cut disagreement dramatically. 'Is this photo a hot dog?' needs a guide for photos of hot dog costumes, drawings, and half-eaten ones."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Cheap crowdsourced labels save money upfront but cost you in model quality. Specialist labels (doctors, lawyers) are expensive but can be the only option for critical tasks."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Measure inter-annotator agreement. If your labelers disagree 30% of the time, your model's ceiling is already capped."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The model is only as smart as the labels it learned from."
    }
  },
  "quiz": {
    "question": "What's the biggest risk with crowdsourced data labels?",
    "options": [
      "Labels arrive too quickly",
      "Annotators may disagree or mislabel ambiguous cases",
      "Crowdsourced labels are always wrong",
      "It's impossible to measure label quality"
    ],
    "correct": 1
  }
}
