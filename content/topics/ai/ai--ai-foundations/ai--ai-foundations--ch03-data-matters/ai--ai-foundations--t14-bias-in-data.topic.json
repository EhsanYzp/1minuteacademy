{
  "id": "ai--ai-foundations--t14-bias-in-data",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch03-data-matters",
  "title": "Bias in Data",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "How biased training data creates biased models.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "âš–ï¸",
      "text": "A hiring model rejects qualified candidates because the training data was 90% male resumes. The model learned that male = hirable."
    },
    "buildup": {
      "visual": "ğŸ”",
      "text": "Bias in AI starts with the data. If your dataset over-represents one group or under-represents another, the model inherits that skew."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Amazon's real recruiting tool did exactly this. It penalized resumes that mentioned women's colleges. The bias was in the historical hiring data, not the algorithm."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Removing the biased feature (gender) doesn't always fix it. The model can still learn proxies â€” hobbies, schools, zip codes that correlate with the removed feature."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Audit your data before training. Check representation across groups. Test model outputs for disparate impact. Bias is a data problem first and an algorithm problem second."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "An unbiased algorithm trained on biased data is still biased. Fix the data."
    }
  },
  "quiz": {
    "question": "Why doesn't removing a biased feature (like gender) always fix model bias?",
    "options": [
      "The model ignores features anyway",
      "Other features can act as proxies for the removed one",
      "Removing features always improves accuracy",
      "Bias only exists in labels, not features"
    ],
    "correct": 1
  }
}
