{
  "id": "ai--ai-foundations--t12-why-data-quality-beats-quantity",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch03-data-matters",
  "title": "Quality Beats Quantity",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "Why clean data matters more than massive data.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ—‘ï¸",
      "text": "A team scraped 10 million images to train a medical model. The model learned to detect the hospital's watermark instead of the disease."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "More data helps, but only if the data actually represents the problem. Noisy labels, duplicates, and artifacts teach the model the wrong things."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Curating 50,000 well-labeled examples often outperforms dumping 5 million messy ones. The signal-to-noise ratio matters more than volume."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Cleaning data isn't glamorous, so teams skip it. Then they spend months debugging a model that was doomed by dirty inputs."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Before you add more data, audit what you already have. Fix label errors, remove duplicates, and check for leakage."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Garbage in, garbage out isn't a clichÃ© â€” it's the most expensive lesson in ML."
    }
  },
  "quiz": {
    "question": "Why can more data sometimes hurt model performance?",
    "options": [
      "Models can only handle small datasets",
      "Noisy or mislabeled data teaches wrong patterns",
      "Large datasets always cause overfitting",
      "Training on more data is always better"
    ],
    "correct": 1
  }
}
