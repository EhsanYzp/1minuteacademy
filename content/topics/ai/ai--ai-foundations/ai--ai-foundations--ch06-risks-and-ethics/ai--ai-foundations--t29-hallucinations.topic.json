{
  "id": "ai--ai-foundations--t29-hallucinations",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch06-risks-and-ethics",
  "title": "Hallucinations",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "When AI confidently states things that aren't true.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ«§",
      "text": "You ask a chatbot for a legal citation. It gives you a case name, court, year â€” all completely made up. A lawyer filed it in court and got sanctioned."
    },
    "buildup": {
      "visual": "ğŸ”",
      "text": "Hallucination is when a model generates text that is fluent and confident but factually wrong. It's not lying â€” it has no concept of truth. It generates plausible next tokens."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Hallucinations happen because the model is optimized for fluency, not accuracy. If the most probable next word leads to a false statement, the model writes it without hesitation."
    },
    "twist": {
      "visual": "âš¡",
      "text": "You can't reliably detect hallucinations from the model's output alone. The confidence sounds the same whether the fact is real or invented."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Mitigate hallucinations with retrieval-augmented generation (RAG), fact-checking layers, or restricting the model to domains with verified sources."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The model will never tell you it's guessing. That's your job to figure out."
    }
  },
  "quiz": {
    "question": "Why do language models hallucinate?",
    "options": [
      "They intentionally deceive users",
      "They are optimized for fluency, not factual accuracy",
      "They have too little training data",
      "They can only process short inputs"
    ],
    "correct": 1
  }
}
