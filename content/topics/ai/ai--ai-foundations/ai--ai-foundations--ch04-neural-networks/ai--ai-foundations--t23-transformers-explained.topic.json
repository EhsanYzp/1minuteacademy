{
  "id": "ai--ai-foundations--t23-transformers-explained",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch04-neural-networks",
  "title": "Transformers Explained",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "The architecture behind ChatGPT and modern AI.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ”®",
      "text": "GPT, BERT, Gemini, Claude â€” all built on the same architecture. What makes transformers so dominant?"
    },
    "buildup": {
      "visual": "ğŸ§©",
      "text": "Transformers process all tokens in a sequence simultaneously (not one by one like older models) and use 'attention' to figure out which tokens matter to each other."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "In the sentence 'The cat sat on the mat because it was tired,' attention links 'it' to 'cat' â€” not 'mat.' The model learns these relationships from millions of examples."
    },
    "twist": {
      "visual": "âš¡",
      "text": "The 2017 paper was called 'Attention Is All You Need' and it was right. Removing recurrence and convolutions made training massively parallelizable on GPUs."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Transformers now dominate NLP, vision (ViT), audio, protein folding, and more. The architecture generalized far beyond text."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Attention is all you need â€” and it turns out you need it everywhere."
    }
  },
  "quiz": {
    "question": "What is the key innovation of the transformer architecture?",
    "options": [
      "Processing tokens sequentially for accuracy",
      "Using attention to relate all tokens simultaneously",
      "Eliminating the need for training data",
      "Using convolutional filters on text"
    ],
    "correct": 1
  }
}
