{
  "id": "ai--ai-foundations--t20-activation-functions",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Foundations",
  "course_id": "ai--ai-foundations",
  "chapter_id": "ai--ai-foundations--ch04-neural-networks",
  "title": "Activation Functions",
  "emoji": "ğŸ“˜",
  "color": "#EF4444",
  "description": "The nonlinear twist that gives networks their power.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“ˆ",
      "text": "Stack ten layers of multiply-and-add. Mathematically, it collapses into one layer. Your deep network is secretly shallow. How do you fix that?"
    },
    "buildup": {
      "visual": "ğŸ”§",
      "text": "Activation functions inject nonlinearity after each layer. ReLU (rectified linear unit) is the most common: if the value is negative, output zero; if positive, keep it."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "This tiny kink â€” zeroing out negatives â€” is enough to let stacked layers model curves, edges, complex boundaries that a straight line never could."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Before ReLU, people used sigmoid and tanh, but they had a vanishing gradient problem in deep networks. ReLU's simplicity accidentally solved a decade-old bottleneck."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Without activation functions, depth is an illusion. With them, each layer genuinely adds representational power."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "ReLU: the simplest function that makes deep learning deep."
    }
  },
  "quiz": {
    "question": "Why are activation functions essential in neural networks?",
    "options": [
      "They speed up training by reducing data size",
      "They add nonlinearity so layers don't collapse into one",
      "They prevent the network from overfitting",
      "They convert text into numbers"
    ],
    "correct": 1
  }
}
