{
  "id": "ai--llm-fundamentals--t-scaling-laws-bigger-vs-better",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch02-training",
  "title": "Scaling Laws: Bigger vs Better",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A 1-minute de-risking session on Scaling Laws: Bigger vs Better.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Double the parameters, double the performance? Not quiteâ€”it's a power law.",
      "visual": "ğŸ“ˆ"
    },
    "buildup": {
      "text": "Scaling laws predict loss improvement as a function of data, compute, and parameters.",
      "visual": "ğŸ”¬"
    },
    "discovery": {
      "text": "Chinchilla showed that many models were undertrained: more data matters as much as more params.",
      "visual": "ğŸ­"
    },
    "twist": {
      "text": "Scaling eventually hits diminishing returns. 10Ã— compute â‰  10Ã— smarter.",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "The frontier is shifting to efficiency: smaller models trained longer on better data.",
      "visual": "ğŸ’¡"
    },
    "punchline": {
      "text": "Scale smart, not just big.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "What did Chinchilla scaling research show?",
    "options": [
      "Many models were undertrainedâ€”more data matters",
      "Bigger models always win",
      "Training data is irrelevant"
    ],
    "correct": 0
  }
}
