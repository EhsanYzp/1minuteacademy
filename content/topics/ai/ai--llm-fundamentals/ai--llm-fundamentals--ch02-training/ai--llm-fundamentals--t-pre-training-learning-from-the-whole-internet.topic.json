{
  "id": "ai--llm-fundamentals--t-pre-training-learning-from-the-whole-internet",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch02-training",
  "title": "Pre-Training: Learning from the Whole Internet",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "One-minute skill: Pre-Training: Learning from the Whole Internet.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "How does a model learn to code, write poetry, and answer science questions?",
      "visual": "ğŸŒ"
    },
    "buildup": {
      "text": "Pre-training feeds the model trillions of tokens from books, websites, and code repos.",
      "visual": "ğŸ“š"
    },
    "discovery": {
      "text": "The objective is simple: predict the next token. Repeat billions of times.",
      "visual": "ğŸ”"
    },
    "twist": {
      "text": "Quality matters more than quantityâ€”web garbage in, web garbage out.",
      "visual": "ğŸ—‘ï¸"
    },
    "climax": {
      "text": "Pre-training takes weeks on thousands of GPUs and costs millions of dollars.",
      "visual": "ğŸ’µ"
    },
    "punchline": {
      "text": "Read everything. Remember patterns. Forget nothing.",
      "visual": "ğŸ§ "
    }
  },
  "quiz": {
    "question": "What is the objective during LLM pre-training?",
    "options": [
      "Predict the next token",
      "Classify images",
      "Answer user questions directly"
    ],
    "correct": 0
  }
}
