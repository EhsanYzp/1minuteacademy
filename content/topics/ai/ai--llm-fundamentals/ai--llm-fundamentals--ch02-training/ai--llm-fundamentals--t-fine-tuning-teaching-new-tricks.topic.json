{
  "id": "ai--llm-fundamentals--t-fine-tuning-teaching-new-tricks",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch02-training",
  "title": "Fine-Tuning: Teaching New Tricks",
  "emoji": "ðŸ§ ",
  "color": "#0891B2",
  "description": "One-minute skill: Fine-Tuning: Teaching New Tricks.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "The base model knows English but can't follow instructions. Fine-tuning bridges that gap.",
      "visual": "ðŸŽ“"
    },
    "buildup": {
      "text": "Fine-tuning continues training on a smaller, curated dataset for a specific task.",
      "visual": "ðŸŽ¯"
    },
    "discovery": {
      "text": "Instruction tuning teaches the model to follow commands instead of just completing text.",
      "visual": "ðŸ“‹"
    },
    "twist": {
      "text": "Fine-tune too much and the model 'forgets' general knowledgeâ€”catastrophic forgetting.",
      "visual": "ðŸ§Š"
    },
    "climax": {
      "text": "Use LoRA or QLoRA to fine-tune cheaply by updating only a fraction of the weights.",
      "visual": "ðŸ”§"
    },
    "punchline": {
      "text": "A little targeted data goes a long way.",
      "visual": "ðŸ’Ž"
    }
  },
  "quiz": {
    "question": "What risk does excessive fine-tuning carry?",
    "options": [
      "Catastrophic forgetting of general knowledge",
      "Making the model faster",
      "Reducing the parameter count"
    ],
    "correct": 0
  }
}
