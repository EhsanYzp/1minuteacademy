{
  "id": "ai--llm-fundamentals--t-positional-encoding-order-matters",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch03-architectures",
  "title": "Positional Encoding: Order Matters",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A fast breakdown of Positional Encoding: Order Matters for builders.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "'Dog bites man' and 'man bites dog' have the same tokens. Order changes everything.",
      "visual": "ğŸ”„"
    },
    "buildup": {
      "text": "Attention is permutation-invariantâ€”it doesn't know token order without help.",
      "visual": "ğŸ¤·"
    },
    "discovery": {
      "text": "Positional encodings inject order info: absolute (sinusoidal) or relative (RoPE, ALiBi).",
      "visual": "ğŸ“"
    },
    "twist": {
      "text": "Relative encodings generalize to longer sequences unseen during training.",
      "visual": "ğŸ“"
    },
    "climax": {
      "text": "RoPE (rotary position embedding) is now the standard in most open-source LLMs.",
      "visual": "ğŸ”§"
    },
    "punchline": {
      "text": "Without position, language is just a bag of words.",
      "visual": "ğŸ’"
    }
  },
  "quiz": {
    "question": "Why do transformers need positional encoding?",
    "options": [
      "Attention is order-agnostic without it",
      "To reduce model size",
      "To speed up training"
    ],
    "correct": 0
  }
}
