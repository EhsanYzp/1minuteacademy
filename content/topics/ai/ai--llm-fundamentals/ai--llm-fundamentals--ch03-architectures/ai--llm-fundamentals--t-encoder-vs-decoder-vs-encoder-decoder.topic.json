{
  "id": "ai--llm-fundamentals--t-encoder-vs-decoder-vs-encoder-decoder",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch03-architectures",
  "title": "Encoder vs Decoder vs Encoder-Decoder",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A quick, practical guide to Encoder vs Decoder vs Encoder-Decoder.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "BERT, GPT, and T5 are all transformersâ€”but they work very differently.",
      "visual": "ğŸ”€"
    },
    "buildup": {
      "text": "Encoder-only (BERT) reads the whole input at onceâ€”great for classification.",
      "visual": "ğŸ“–"
    },
    "discovery": {
      "text": "Decoder-only (GPT) generates left-to-rightâ€”great for text generation.",
      "visual": "âœï¸"
    },
    "twist": {
      "text": "Encoder-decoder (T5) does bothâ€”reads input fully, then generates output.",
      "visual": "ğŸ”„"
    },
    "climax": {
      "text": "Most modern LLMs are decoder-only because generation is the hardest, most general task.",
      "visual": "ğŸ†"
    },
    "punchline": {
      "text": "Architecture shapes what the model does best.",
      "visual": "ğŸ§¬"
    }
  },
  "quiz": {
    "question": "Which architecture type is GPT?",
    "options": [
      "Decoder-only",
      "Encoder-only",
      "Encoder-decoder"
    ],
    "correct": 0
  }
}
