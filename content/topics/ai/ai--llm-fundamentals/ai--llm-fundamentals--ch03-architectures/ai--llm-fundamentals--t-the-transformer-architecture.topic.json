{
  "id": "ai--llm-fundamentals--t-the-transformer-architecture",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch03-architectures",
  "title": "The Transformer Architecture",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A 60-second lesson on The Transformer Architecture.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "Before transformers, language models were slow and forgot long sentences.",
      "visual": "ğŸŒ"
    },
    "buildup": {
      "text": "RNNs processed tokens one by one. Transformers process all tokens in parallel.",
      "visual": "âš¡"
    },
    "discovery": {
      "text": "Key ingredients: self-attention, positional encoding, layer normalization, feed-forward layers.",
      "visual": "ğŸ§±"
    },
    "twist": {
      "text": "Parallelism made transformers GPU-friendlyâ€”and that's why they scaled so fast.",
      "visual": "ğŸš€"
    },
    "climax": {
      "text": "Nearly every modern AI systemâ€”text, image, audioâ€”uses some form of transformer.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "One architecture changed everything.",
      "visual": "ğŸ›ï¸"
    }
  },
  "quiz": {
    "question": "What advantage do transformers have over RNNs?",
    "options": [
      "Parallel processing of all tokens",
      "Smaller model sizes",
      "No need for GPUs"
    ],
    "correct": 0
  }
}
