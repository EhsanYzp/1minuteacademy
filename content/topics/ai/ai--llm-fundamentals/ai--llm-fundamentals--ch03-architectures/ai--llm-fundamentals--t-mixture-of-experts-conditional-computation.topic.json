{
  "id": "ai--llm-fundamentals--t-mixture-of-experts-conditional-computation",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch03-architectures",
  "title": "Mixture of Experts: Conditional Computation",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A micro-lesson that makes Mixture of Experts: Conditional Computation usable.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "What if only 10% of the model activated per token? You'd save 90% of compute.",
      "visual": "ğŸ§©"
    },
    "buildup": {
      "text": "MoE models have many 'expert' sub-networks. A router picks which ones fire.",
      "visual": "ğŸš¦"
    },
    "discovery": {
      "text": "Each token activates only 1â€“2 experts, making inference cheaper than a dense model.",
      "visual": "ğŸ’¡"
    },
    "twist": {
      "text": "MoE models have huge total parameter counts but small active parameter counts.",
      "visual": "ğŸ­"
    },
    "climax": {
      "text": "Mixtral, Switch Transformer, and GPT-4 (rumored) all use MoE variants.",
      "visual": "ğŸ—ï¸"
    },
    "punchline": {
      "text": "Bigger model, same compute budget.",
      "visual": "âš¡"
    }
  },
  "quiz": {
    "question": "What does MoE achieve?",
    "options": [
      "Activates only a few experts per token to save compute",
      "Uses one giant expert for everything",
      "Eliminates attention layers"
    ],
    "correct": 0
  }
}
