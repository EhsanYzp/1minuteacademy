{
  "id": "ai--llm-fundamentals--t-human-evaluation-the-gold-standard",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch05-evaluation",
  "title": "Human Evaluation: The Gold Standard",
  "emoji": "ðŸ§ ",
  "color": "#0891B2",
  "description": "A micro-lesson that makes Human Evaluation: The Gold Standard usable.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "Your model scores 90% on MMLU but users say the answers are awful. Who's right?",
      "visual": "ðŸ¤”"
    },
    "buildup": {
      "text": "Automated metrics miss nuance: tone, helpfulness, safety, and coherence.",
      "visual": "ðŸ“Š"
    },
    "discovery": {
      "text": "Human eval uses raters to score outputs on specific criteria: accuracy, clarity, safety.",
      "visual": "ðŸ‘¥"
    },
    "twist": {
      "text": "Human eval is expensive and slow. Use it for high-stakes decisions, not daily iteration.",
      "visual": "ðŸ’°"
    },
    "climax": {
      "text": "Best practice: automated evals daily, human evals weekly, on a rotating sample.",
      "visual": "ðŸ”„"
    },
    "punchline": {
      "text": "Users are the ultimate benchmark.",
      "visual": "ðŸ‘¤"
    }
  },
  "quiz": {
    "question": "When should you use human evaluation?",
    "options": [
      "For high-stakes quality decisions automated metrics miss",
      "For every single API call",
      "Neverâ€”benchmarks are enough"
    ],
    "correct": 0
  }
}
