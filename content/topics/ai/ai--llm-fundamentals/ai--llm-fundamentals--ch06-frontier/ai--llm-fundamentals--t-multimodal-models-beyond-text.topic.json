{
  "id": "ai--llm-fundamentals--t-multimodal-models-beyond-text",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch06-frontier",
  "title": "Multimodal Models: Beyond Text",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A quick, practical guide to Multimodal Models: Beyond Text.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "text": "You upload a photo and the model describes it. Text-only models can't do that.",
      "visual": "ğŸ–¼ï¸"
    },
    "buildup": {
      "text": "Multimodal models process text, images, audio, and video in a shared embedding space.",
      "visual": "ğŸ”€"
    },
    "discovery": {
      "text": "Vision encoders (like CLIP) convert images to embeddings the LLM can attend to.",
      "visual": "ğŸ‘ï¸"
    },
    "twist": {
      "text": "Multimodal isn't just inputâ€”models now generate images, speech, and code from one prompt.",
      "visual": "ğŸ¨"
    },
    "climax": {
      "text": "GPT-4V, Gemini, and Claude all accept images. The text-only era is ending.",
      "visual": "ğŸŒ…"
    },
    "punchline": {
      "text": "One model, every modality.",
      "visual": "ğŸŒ"
    }
  },
  "quiz": {
    "question": "How do multimodal models handle images?",
    "options": [
      "Convert images to embeddings the LLM can process",
      "Ignore images entirely",
      "Run a separate program for images"
    ],
    "correct": 0
  }
}
