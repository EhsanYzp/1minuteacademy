{
  "id": "ai--llm-fundamentals--t-predicting-the-next-token",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch01-how-llms-work",
  "title": "Predicting the Next Token",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A quick, practical guide to Predicting the Next Token.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "text": "Type 'the cat sat on the' and the model guesses 'mat.' That's the entire trick.",
      "visual": "ğŸ”®"
    },
    "buildup": {
      "text": "LLMs are next-token predictors trained on billions of sentences from the web.",
      "visual": "ğŸ“š"
    },
    "discovery": {
      "text": "Given all previous tokens, the model outputs a probability for every possible next token.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "This simple objective produces translation, code, and reasoning as side effects.",
      "visual": "ğŸ¤¯"
    },
    "climax": {
      "text": "The 'intelligence' is compressed patternsâ€”not understanding, not memory, not beliefs.",
      "visual": "ğŸ—œï¸"
    },
    "punchline": {
      "text": "One trick, endless emergent behaviors.",
      "visual": "âœ¨"
    }
  },
  "quiz": {
    "question": "What is the core task of a large language model?",
    "options": [
      "Predicting the most likely next token",
      "Searching the internet",
      "Storing a knowledge database"
    ],
    "correct": 0
  }
}
