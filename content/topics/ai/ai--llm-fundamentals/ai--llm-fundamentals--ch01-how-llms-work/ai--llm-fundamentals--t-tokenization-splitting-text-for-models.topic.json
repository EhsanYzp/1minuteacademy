{
  "id": "ai--llm-fundamentals--t-tokenization-splitting-text-for-models",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch01-how-llms-work",
  "title": "Tokenization: Splitting Text for Models",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A 60-second lesson on Tokenization: Splitting Text for Models.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "The word 'unhappiness' becomes three tokens: 'un', 'happiness', and sometimes more.",
      "visual": "âœ‚ï¸"
    },
    "buildup": {
      "text": "Tokenizers break text into sub-word pieces the model can process.",
      "visual": "ğŸ§©"
    },
    "discovery": {
      "text": "BPE (byte pair encoding) merges frequent character pairs into single tokens.",
      "visual": "ğŸ”—"
    },
    "twist": {
      "text": "Tokenization is language-dependent: English is cheap; CJK and code can cost 2â€“4Ã— more.",
      "visual": "ğŸ’°"
    },
    "climax": {
      "text": "Always use the model's own tokenizer to count tokens accurately before sending.",
      "visual": "ğŸ§®"
    },
    "punchline": {
      "text": "The model doesn't see words. It sees tokens.",
      "visual": "ğŸ‘€"
    }
  },
  "quiz": {
    "question": "What is BPE?",
    "options": [
      "Byte pair encodingâ€”merges frequent character pairs",
      "A type of neural network",
      "A fine-tuning method"
    ],
    "correct": 0
  }
}
