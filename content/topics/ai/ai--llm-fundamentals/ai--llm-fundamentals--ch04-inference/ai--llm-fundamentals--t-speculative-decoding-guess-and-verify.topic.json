{
  "id": "ai--llm-fundamentals--t-speculative-decoding-guess-and-verify",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch04-inference",
  "title": "Speculative Decoding: Guess and Verify",
  "emoji": "ğŸ§ ",
  "color": "#0891B2",
  "description": "A micro-lesson that makes Speculative Decoding: Guess and Verify usable.",
  "difficulty": "Premium",
  "published": true,
  "story": {
    "hook": {
      "text": "What if a tiny model drafts 5 tokens and the big model verifies them in one pass?",
      "visual": "ğŸƒ"
    },
    "buildup": {
      "text": "Speculative decoding uses a small 'draft' model to propose multiple tokens at once.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "The large model checks all 5 tokens in a single forward passâ€”accepting or correcting.",
      "visual": "âœ…"
    },
    "twist": {
      "text": "If most guesses are right, you get 3â€“5Ã— speedup with identical output quality.",
      "visual": "âš¡"
    },
    "climax": {
      "text": "The trick: verification is parallel (cheap), generation is sequential (expensive).",
      "visual": "ğŸ”€"
    },
    "punchline": {
      "text": "Guess fast. Verify cheap. Ship faster.",
      "visual": "ğŸš€"
    }
  },
  "quiz": {
    "question": "How does speculative decoding speed up inference?",
    "options": [
      "A small model drafts tokens, a large model verifies in parallel",
      "It skips tokens",
      "It reduces model size"
    ],
    "correct": 0
  }
}
