{
  "id": "ai--llm-fundamentals--t-quantization-shrinking-models-to-fit",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "LLM Fundamentals",
  "course_id": "ai--llm-fundamentals",
  "chapter_id": "ai--llm-fundamentals--ch04-inference",
  "title": "Quantization: Shrinking Models to Fit",
  "emoji": "üß†",
  "color": "#0891B2",
  "description": "One-minute skill: Quantization: Shrinking Models to Fit.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "A 70B parameter model needs 140 GB of memory. Your GPU has 24 GB. Now what?",
      "visual": "üò∞"
    },
    "buildup": {
      "text": "Quantization converts 16-bit weights to 8-bit or 4-bit, slashing memory needs.",
      "visual": "üìâ"
    },
    "discovery": {
      "text": "GPTQ and AWQ quantize to 4-bit with minimal quality loss for most tasks.",
      "visual": "üîß"
    },
    "twist": {
      "text": "Aggressive quantization (2-bit) works for easy tasks but degrades complex reasoning.",
      "visual": "‚ö†Ô∏è"
    },
    "climax": {
      "text": "GGUF format + llama.cpp lets you run quantized models on a laptop CPU.",
      "visual": "üíª"
    },
    "punchline": {
      "text": "Half the bits, most of the brains.",
      "visual": "üß†"
    }
  },
  "quiz": {
    "question": "What does quantization reduce?",
    "options": [
      "Memory needed by lowering weight precision",
      "The number of parameters",
      "Training data size"
    ],
    "correct": 0
  }
}
