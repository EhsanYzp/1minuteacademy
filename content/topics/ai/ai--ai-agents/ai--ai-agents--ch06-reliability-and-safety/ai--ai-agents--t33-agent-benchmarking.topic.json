{
  "id": "ai--ai-agents--t33-agent-benchmarking",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Agents",
  "course_id": "ai--ai-agents",
  "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
  "title": "Agent Benchmarking",
  "emoji": "ğŸ¤–",
  "color": "#EF4444",
  "description": "Measuring agent performance across a standardised set of tasks.",
  "difficulty": "Premium",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ‹ï¸",
      "text": "You upgraded the model from GPT-4 to GPT-4o-mini. It's cheaper â€” but is the agent still good? Without benchmarks, you're guessing."
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "Agent benchmarks are curated sets of tasks with expected outcomes. Run the agent against all tasks, score the results, and compare across model versions, prompt changes, or architecture tweaks."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Build benchmarks from real user queries: 50-100 representative tasks with expected final answers. Score on task completion rate, step efficiency, cost per task, and error rate."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Agent performance is non-deterministic â€” run each benchmark task 3-5 times and report the median. A single run can be misleadingly good or bad."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Benchmarks are your safety net for change. Swap a model? Run the benchmark. Change a prompt? Run the benchmark. Add a tool? Run the benchmark."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "If you can't measure it, you can't improve it. Benchmark your agents."
    }
  },
  "quiz": {
    "question": "Why should agent benchmarks be run multiple times per task?",
    "options": [
      "To increase API costs",
      "Because agent behavior is non-deterministic, and single runs can be misleading",
      "To train the model on the benchmark",
      "Because the first run always fails"
    ],
    "correct": 1
  }
}
