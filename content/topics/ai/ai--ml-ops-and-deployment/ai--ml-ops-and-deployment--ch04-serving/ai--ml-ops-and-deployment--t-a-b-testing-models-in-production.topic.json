{
  "id": "ai--ml-ops-and-deployment--t-a-b-testing-models-in-production",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "ML Ops & Deployment",
  "course_id": "ai--ml-ops-and-deployment",
  "chapter_id": "ai--ml-ops-and-deployment--ch04-serving",
  "title": "A/B Testing Models in Production",
  "emoji": "ğŸš€",
  "color": "#4F46E5",
  "description": "One-minute skill: A/B Testing Models in Production.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "The new model scores better on benchmarks. Will it actually improve the business metric?",
      "visual": "ğŸ“Š"
    },
    "buildup": {
      "text": "A/B testing splits traffic between the old and new model to compare real-world performance.",
      "visual": "ğŸ”€"
    },
    "discovery": {
      "text": "Route 10% of traffic to the new model. Measure conversion, latency, and error rates.",
      "visual": "ğŸ“ˆ"
    },
    "twist": {
      "text": "Statistical significance takes time. Don't declare a winner after 100 requests.",
      "visual": "â³"
    },
    "climax": {
      "text": "Use feature flags (LaunchDarkly, Unleash) to control traffic splits without redeploying.",
      "visual": "ğŸš¦"
    },
    "punchline": {
      "text": "Offline metrics propose. Online metrics decide.",
      "visual": "âš–ï¸"
    }
  },
  "quiz": {
    "question": "Why is A/B testing needed beyond offline evaluation?",
    "options": [
      "Benchmark scores don't always predict real-world performance",
      "It's faster than benchmarks",
      "It requires no data"
    ],
    "correct": 0
  }
}
