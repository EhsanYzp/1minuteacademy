{
  "id": "ai--ml-ops-and-deployment--t-hyperparameter-tuning-at-scale",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "ML Ops & Deployment",
  "course_id": "ai--ml-ops-and-deployment",
  "chapter_id": "ai--ml-ops-and-deployment--ch03-training",
  "title": "Hyperparameter Tuning at Scale",
  "emoji": "ğŸš€",
  "color": "#4F46E5",
  "description": "One-minute skill: Hyperparameter Tuning at Scale.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "Learning rate 0.001 or 0.0001? Batch size 32 or 256? The right combo can double accuracy.",
      "visual": "ğŸ›ï¸"
    },
    "buildup": {
      "text": "Hyperparameters control training behavior but aren't learnedâ€”they must be searched.",
      "visual": "ğŸ”"
    },
    "discovery": {
      "text": "Methods: grid search, random search, Bayesian optimization, and population-based training.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "Random search beats grid search most of the timeâ€”important hyperparameters vary per problem.",
      "visual": "ğŸ²"
    },
    "climax": {
      "text": "Tools like Optuna, Ray Tune, and W&B Sweeps automate distributed hyperparameter search.",
      "visual": "ğŸ”§"
    },
    "punchline": {
      "text": "Don't guess hyperparameters. Search them systematically.",
      "visual": "ğŸ¯"
    }
  },
  "quiz": {
    "question": "Why does random search often beat grid search?",
    "options": [
      "It covers the important dimensions more efficiently",
      "It's slower",
      "It uses more compute"
    ],
    "correct": 0
  }
}
