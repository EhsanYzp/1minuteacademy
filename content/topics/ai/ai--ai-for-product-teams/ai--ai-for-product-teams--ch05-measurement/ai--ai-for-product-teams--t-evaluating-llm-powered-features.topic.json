{
  "id": "ai--ai-for-product-teams--t-evaluating-llm-powered-features",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI for Product Teams",
  "course_id": "ai--ai-for-product-teams",
  "chapter_id": "ai--ai-for-product-teams--ch05-measurement",
  "title": "Evaluating LLM-Powered Features",
  "emoji": "ğŸ“¦",
  "color": "#0D9488",
  "description": "A quick win: understand Evaluating LLM-Powered Features.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "How do you A/B test a chatbot? There's no single 'correct' answer to compare.",
      "visual": "ğŸ¤–"
    },
    "buildup": {
      "text": "LLM evaluation is harder than classification because outputs are open-ended.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "Methods: human ratings, LLM-as-judge, task completion rate, and user satisfaction scores.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "A single bad interaction can destroy trust. Measure worst-case performance, not just average.",
      "visual": "âš ï¸"
    },
    "climax": {
      "text": "Build an eval suite: 100+ test cases covering happy paths, edge cases, and adversarial inputs.",
      "visual": "ğŸ§ª"
    },
    "punchline": {
      "text": "If you can't measure it, you can't improve it.",
      "visual": "ğŸ“"
    }
  },
  "quiz": {
    "question": "Why is evaluating LLM features harder than classification?",
    "options": [
      "Outputs are open-ended with no single correct answer",
      "LLMs always give the same answer",
      "There are no metrics for LLMs"
    ],
    "correct": 0
  }
}
