{
  "id": "ai--ai-for-product-teams--t-latency-perception-making-ai-feel-fast",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI for Product Teams",
  "course_id": "ai--ai-for-product-teams",
  "chapter_id": "ai--ai-for-product-teams--ch02-design",
  "title": "Latency Perception: Making AI Feel Fast",
  "emoji": "ğŸ“¦",
  "color": "#0D9488",
  "description": "One-minute skill: Latency Perception: Making AI Feel Fast.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "The model takes 3 seconds. Streaming the response token by token makes it feel instant.",
      "visual": "âš¡"
    },
    "buildup": {
      "text": "Perceived latency matters more than actual latency. Users judge the first visible result.",
      "visual": "ğŸ‘ï¸"
    },
    "discovery": {
      "text": "Techniques: token streaming, skeleton loaders, progressive rendering, and optimistic UI.",
      "visual": "ğŸ¨"
    },
    "twist": {
      "text": "For long tasks, show progress steps: 'Searchingâ€¦ Analyzingâ€¦ Generating answer.'",
      "visual": "ğŸ“‹"
    },
    "climax": {
      "text": "If the user sees activity, they wait 3Ã— longer before feeling frustrated.",
      "visual": "â³"
    },
    "punchline": {
      "text": "Show progress. Users wait for motion, not completion.",
      "visual": "ğŸƒ"
    }
  },
  "quiz": {
    "question": "Why does token streaming improve perceived performance?",
    "options": [
      "Users see the first tokens immediately instead of waiting",
      "It reduces actual computation time",
      "It uses less memory"
    ],
    "correct": 0
  }
}
