{
  "id": "ai--computer-vision-basics--t-batch-normalization-in-vision-models",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Computer Vision Basics",
  "course_id": "ai--computer-vision-basics",
  "chapter_id": "ai--computer-vision-basics--ch02-cnns",
  "title": "Batch Normalization in Vision Models",
  "emoji": "ğŸ‘ï¸",
  "color": "#059669",
  "description": "A fast breakdown of Batch Normalization in Vision Models for builders.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Without batch norm, deep CNNs train painfully slowly. With it, they converge in half the time.",
      "visual": "âš¡"
    },
    "buildup": {
      "text": "Batch normalization normalizes layer inputs to zero mean and unit variance during training.",
      "visual": "ğŸ“Š"
    },
    "discovery": {
      "text": "This reduces internal covariate shiftâ€”layers don't need to constantly adapt to changing inputs.",
      "visual": "ğŸ”„"
    },
    "twist": {
      "text": "BatchNorm behaves differently at train vs inference time, which causes subtle bugs in deployment.",
      "visual": "ğŸ›"
    },
    "climax": {
      "text": "LayerNorm and GroupNorm are alternatives that avoid batch-size dependency issues.",
      "visual": "ğŸ”§"
    },
    "punchline": {
      "text": "Normalize the chaos. Train faster.",
      "visual": "ğŸƒ"
    }
  },
  "quiz": {
    "question": "What does batch normalization help with?",
    "options": [
      "Faster training convergence by normalizing layer inputs",
      "Making images brighter",
      "Increasing model size"
    ],
    "correct": 0
  }
}
