{
  "id": "ai--computer-vision-basics--t-vision-transformers-cnns-meet-attention",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Computer Vision Basics",
  "course_id": "ai--computer-vision-basics",
  "chapter_id": "ai--computer-vision-basics--ch02-cnns",
  "title": "Vision Transformers: CNNs Meet Attention",
  "emoji": "ğŸ‘ï¸",
  "color": "#059669",
  "description": "A micro-lesson that makes Vision Transformers: CNNs Meet Attention usable.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "What if you treated an image like a sentenceâ€”splitting it into patches and using attention?",
      "visual": "ğŸ§©"
    },
    "buildup": {
      "text": "Vision Transformers (ViT) divide images into 16Ã—16 patches and process them with self-attention.",
      "visual": "ğŸ”²"
    },
    "discovery": {
      "text": "ViTs match or beat CNNs on image classification when trained on enough data.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "ViTs need more data than CNNs to learn spatial structure that convolutions get for free.",
      "visual": "ğŸ“š"
    },
    "climax": {
      "text": "Hybrid models combine CNN stems with transformer bodies for the best of both worlds.",
      "visual": "ğŸ¤"
    },
    "punchline": {
      "text": "Patches are the new pixels. Attention is the new convolution.",
      "visual": "ğŸ”€"
    }
  },
  "quiz": {
    "question": "How do Vision Transformers process images?",
    "options": [
      "Split into patches and process with self-attention",
      "One pixel at a time",
      "Using only convolutions"
    ],
    "correct": 0
  }
}
