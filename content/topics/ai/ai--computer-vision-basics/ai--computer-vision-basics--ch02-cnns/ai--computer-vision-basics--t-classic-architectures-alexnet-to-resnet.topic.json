{
  "id": "ai--computer-vision-basics--t-classic-architectures-alexnet-to-resnet",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Computer Vision Basics",
  "course_id": "ai--computer-vision-basics",
  "chapter_id": "ai--computer-vision-basics--ch02-cnns",
  "title": "Classic Architectures: AlexNet to ResNet",
  "emoji": "ğŸ‘ï¸",
  "color": "#059669",
  "description": "A short lesson to help you apply Classic Architectures: AlexNet to ResNet.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "In 2012, AlexNet crushed ImageNet by 10%. Deep learning was no longer theoretical.",
      "visual": "ğŸ†"
    },
    "buildup": {
      "text": "VGG went deeper (16â€“19 layers). GoogLeNet went wider with inception modules.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "ResNet added skip connections: input + output of a block. Now you could train 152 layers.",
      "visual": "ğŸ”—"
    },
    "twist": {
      "text": "Without skip connections, very deep networks degradedâ€”more layers made them worse.",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "ResNet's skip connections are now in almost every modern architecture.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "The shortcut that made depth possible.",
      "visual": "âš¡"
    }
  },
  "quiz": {
    "question": "What problem do ResNet skip connections solve?",
    "options": [
      "Degradation in very deep networks",
      "Slow training speed",
      "Overfitting on small datasets"
    ],
    "correct": 0
  }
}
