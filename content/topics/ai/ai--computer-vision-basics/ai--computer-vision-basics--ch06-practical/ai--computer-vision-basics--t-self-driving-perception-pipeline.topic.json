{
  "id": "ai--computer-vision-basics--t-self-driving-perception-pipeline",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Computer Vision Basics",
  "course_id": "ai--computer-vision-basics",
  "chapter_id": "ai--computer-vision-basics--ch06-practical",
  "title": "Self-Driving Perception Pipeline",
  "emoji": "ğŸ‘ï¸",
  "color": "#059669",
  "description": "A short lesson to help you apply Self-Driving Perception Pipeline.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "A car sees a stop sign, a pedestrian, and a lane lineâ€”all in 50 milliseconds.",
      "visual": "ğŸš—"
    },
    "buildup": {
      "text": "Self-driving uses cameras, lidar, and radar fused into a single perception stack.",
      "visual": "ğŸ“¡"
    },
    "discovery": {
      "text": "Tasks run in parallel: detection, segmentation, depth, lane tracking, and traffic sign reading.",
      "visual": "ğŸ”€"
    },
    "twist": {
      "text": "Edge cases kill: unusual lighting, obscured signs, and construction zones break assumptions.",
      "visual": "ğŸš§"
    },
    "climax": {
      "text": "Redundancy is safety: if one sensor fails, others must compensate.",
      "visual": "ğŸ›¡ï¸"
    },
    "punchline": {
      "text": "See everything. Miss nothing. Decide instantly.",
      "visual": "âš¡"
    }
  },
  "quiz": {
    "question": "Why do self-driving cars use multiple sensor types?",
    "options": [
      "For redundancy if one sensor fails",
      "To reduce cost",
      "Because cameras don't work at night"
    ],
    "correct": 0
  }
}
