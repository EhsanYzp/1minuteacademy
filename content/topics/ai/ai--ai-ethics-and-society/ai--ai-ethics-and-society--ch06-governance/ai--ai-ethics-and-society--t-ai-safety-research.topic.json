{
  "id": "ai--ai-ethics-and-society--t-ai-safety-research",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Ethics and Society",
  "course_id": "ai--ai-ethics-and-society",
  "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
  "title": "AI Safety Research",
  "emoji": "âš–ï¸",
  "color": "#7C3AED",
  "description": "A quick, practical guide to AI Safety Research.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "A research lab's mission: make sure AI doesn't accidentally destroy humanity. No pressure.",
      "visual": "ğŸ”¬"
    },
    "buildup": {
      "text": "AI safety studies alignment â€” making sure AI systems do what humans actually want.",
      "visual": "ğŸ¯"
    },
    "discovery": {
      "text": "Key problems include reward hacking, goal misalignment, and power-seeking behavior.",
      "visual": "âš ï¸"
    },
    "twist": {
      "text": "We don't know how to verify that a superintelligent system is truly aligned. Not yet.",
      "visual": "â“"
    },
    "climax": {
      "text": "Organizations like MIRI, Anthropic, and DeepMind dedicate teams to these existential questions.",
      "visual": "ğŸ¢"
    },
    "punchline": {
      "text": "The most important engineering problem: make sure AI wants what we want.",
      "visual": "ğŸ§­"
    }
  },
  "quiz": {
    "question": "What is the core problem AI safety research addresses?",
    "options": [
      "Ensuring AI systems are aligned with human intentions",
      "Making AI faster and cheaper",
      "Teaching AI to pass exams"
    ],
    "correct": 0
  }
}
