{
  "id": "ai--agent-builder-lab--t43-offline-vs-online-evaluation",
  "version": 1,
  "subject": "AI",
  "subcategory": "Agent Builder Lab",
  "course_id": "ai--agent-builder-lab",
  "chapter_id": "ai--agent-builder-lab--ch06-evaluation",
  "title": "Offline vs Online Evaluation",
  "emoji": "ğŸ¤–",
  "color": "#EF4444",
  "description": "A quick win: understand Offline vs Online Evaluation.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ§ª",
      "text": "Your offline score improvedâ€¦ and support tickets doubled. Thatâ€™s why online eval exists."
    },
    "buildup": {
      "visual": "ğŸ“",
      "text": "Offline evaluation is fast and repeatable. Online evaluation is reality: real users, real context, real constraints."
    },
    "discovery": {
      "visual": "ğŸ§±",
      "text": "Use offline tests to iterate quickly, then validate with a small online rollout (A/B or canary) to confirm real-world impact."
    },
    "twist": {
      "visual": "âš ï¸",
      "text": "Offline data is often too clean. It misses latency, tool outages, confusing user input, and long-tail behavior."
    },
    "climax": {
      "visual": "ğŸ“Œ",
      "text": "Treat offline as a gate, not a guarantee. Ship small, watch metrics, and roll back fast if users suffer."
    },
    "punchline": {
      "visual": "ğŸ”",
      "text": "Practice first. Then play."
    }
  },
  "quiz": {
    "question": "What is online evaluation best for?",
    "options": [
      "Fast iteration only",
      "Real-world validation",
      "Avoiding test sets",
      "Replacing monitoring"
    ],
    "correct": 1
  }
}
