{
  "id": "ai--agent-builder-lab--t46-evaluating-tool-use",
  "version": 1,
  "subject": "AI",
  "subcategory": "Agent Builder Lab",
  "course_id": "ai--agent-builder-lab",
  "chapter_id": "ai--agent-builder-lab--ch06-evaluation",
  "title": "Evaluating Tool Use",
  "emoji": "ğŸ¤–",
  "color": "#EF4444",
  "description": "A fast breakdown of Evaluating Tool Use for builders.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "âš¡ï¸",
      "text": "Calling a tool isnâ€™t success. Success is using the right tool, with the right inputs, safely."
    },
    "buildup": {
      "visual": "ğŸ§±",
      "text": "Tool evaluation is about outcomes: correctness, safety, and recovery when the tool fails."
    },
    "discovery": {
      "visual": "ğŸ› ï¸",
      "text": "Score three things: tool choice, parameters, and result handling (did it validate, retry safely, and stop when uncertain?)."
    },
    "twist": {
      "visual": "ğŸ§¯",
      "text": "A system can spam tools and still fail users. Counting calls rewards noise, not quality."
    },
    "climax": {
      "visual": "ğŸ§ª",
      "text": "Add failure cases to your tests: timeouts, partial results, permission errors, and ambiguous inputs."
    },
    "punchline": {
      "visual": "ğŸ“£",
      "text": "Outcomes beat call counts."
    }
  },
  "quiz": {
    "question": "What should tool eval measure?",
    "options": [
      "Only call count",
      "Choice + params + handling",
      "Only latency",
      "Only token use"
    ],
    "correct": 1
  }
}
