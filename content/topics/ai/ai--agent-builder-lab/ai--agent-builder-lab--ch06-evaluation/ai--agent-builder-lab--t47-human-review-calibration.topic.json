{
  "id": "ai--agent-builder-lab--t47-human-review-calibration",
  "version": 1,
  "subject": "AI",
  "subcategory": "Agent Builder Lab",
  "course_id": "ai--agent-builder-lab",
  "chapter_id": "ai--agent-builder-lab--ch06-evaluation",
  "title": "Human Review Calibration",
  "emoji": "ğŸ¤–",
  "color": "#EF4444",
  "description": "A micro-lesson that makes Human Review Calibration usable.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ¤–",
      "text": "If Alice gives 5 stars and Bob gives 2 for the same answer, your â€œquality scoreâ€ is meaningless."
    },
    "buildup": {
      "visual": "ğŸ“Œ",
      "text": "Calibration makes human review consistent so a score actually means something."
    },
    "discovery": {
      "visual": "ğŸ“",
      "text": "Run short calibration sessions: reviewers score the same examples, compare, and agree on what earns each score."
    },
    "twist": {
      "visual": "ğŸ§¨",
      "text": "Standards drift over time. New reviewers join. Without calibration, your metrics quietly rot."
    },
    "climax": {
      "visual": "ğŸ§­",
      "text": "Use â€œanchorâ€ examples (this is a 5, this is a 1) and update them when your product changes."
    },
    "punchline": {
      "visual": "ğŸ§¾",
      "text": "Make reviewers consistent."
    }
  },
  "quiz": {
    "question": "Why calibrate reviewers?",
    "options": [
      "To slow down shipping",
      "To align scoring standards",
      "To remove rubrics",
      "To hide disagreements"
    ],
    "correct": 1
  }
}
