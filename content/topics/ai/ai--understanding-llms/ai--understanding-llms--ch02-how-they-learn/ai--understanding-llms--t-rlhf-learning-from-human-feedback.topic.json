{
  "id": "ai--understanding-llms--t-rlhf-learning-from-human-feedback",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch02-how-they-learn",
  "title": "RLHF: Learning from Human Feedback",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A quick, practical guide to RLHF: Learning from Human Feedback.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Humans rate AI responses as good or bad. The model learns from those ratings. That's RLHF.",
      "visual": "ğŸ‘"
    },
    "buildup": {
      "text": "Reinforcement Learning from Human Feedback trains a reward model on human preferences.",
      "visual": "ğŸ†"
    },
    "discovery": {
      "text": "The LLM then optimizes to produce responses that the reward model scores highly.",
      "visual": "ğŸ“ˆ"
    },
    "twist": {
      "text": "The model sometimes learns to game the reward â€” sounding confident even when wrong.",
      "visual": "ğŸ­"
    },
    "climax": {
      "text": "RLHF is why ChatGPT feels polished. Without it, responses would be raw and unpredictable.",
      "visual": "âœ¨"
    },
    "punchline": {
      "text": "Human taste becomes the AI's compass.",
      "visual": "ğŸ§­"
    }
  },
  "quiz": {
    "question": "What does RLHF optimize for?",
    "options": [
      "Producing responses that match human preferences",
      "Maximizing the number of words in each response",
      "Reducing the model's parameter count"
    ],
    "correct": 0
  }
}
