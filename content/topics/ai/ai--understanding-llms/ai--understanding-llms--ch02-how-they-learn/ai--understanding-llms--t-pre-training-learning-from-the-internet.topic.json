{
  "id": "ai--understanding-llms--t-pre-training-learning-from-the-internet",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch02-how-they-learn",
  "title": "Pre-Training: Learning from the Internet",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A micro-lesson that makes Pre-Training: Learning from the Internet usable.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Before ChatGPT could chat, it read trillions of words from books, websites, and forums.",
      "visual": "ğŸ“š"
    },
    "buildup": {
      "text": "Pre-training is self-supervised: the model predicts the next token in massive text datasets.",
      "visual": "ğŸ¯"
    },
    "discovery": {
      "text": "By predicting trillions of next words, it absorbs grammar, facts, reasoning, and even humor.",
      "visual": "ğŸ˜„"
    },
    "twist": {
      "text": "It also absorbs biases, errors, and toxic content from the training data.",
      "visual": "â˜£ï¸"
    },
    "climax": {
      "text": "Pre-training costs millions of dollars in compute. GPT-4 reportedly cost over $100 million.",
      "visual": "ğŸ’°"
    },
    "punchline": {
      "text": "Read everything. Absorb everything. Good and bad.",
      "visual": "ğŸŒŠ"
    }
  },
  "quiz": {
    "question": "What does an LLM learn during pre-training?",
    "options": [
      "To predict the next token from massive text datasets",
      "To follow specific user instructions",
      "To browse the internet in real time"
    ],
    "correct": 0
  }
}
