{
  "id": "ai--understanding-llms--t-transfer-learning-one-model-many-skills",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch02-how-they-learn",
  "title": "Transfer Learning: One Model, Many Skills",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A 60-second lesson on Transfer Learning: One Model, Many Skills.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "A model trained to write English can suddenly translate French. Nobody taught it French specifically.",
      "visual": "ğŸ‡«ğŸ‡·"
    },
    "buildup": {
      "text": "Transfer learning means skills learned in one domain carry over to related tasks.",
      "visual": "ğŸ”„"
    },
    "discovery": {
      "text": "A pre-trained LLM can be fine-tuned for medicine, law, or code with relatively little extra data.",
      "visual": "âš•ï¸"
    },
    "twist": {
      "text": "This is why foundation models are so valuable â€” one model serves as the base for thousands of apps.",
      "visual": "ğŸ›ï¸"
    },
    "climax": {
      "text": "GPT-4 wasn't trained to write sonnets or debug Python specifically. It learned both from general text.",
      "visual": "ğŸ­"
    },
    "punchline": {
      "text": "Learn language deeply, and everything else comes along for the ride.",
      "visual": "ğŸ¢"
    }
  },
  "quiz": {
    "question": "What is transfer learning?",
    "options": [
      "Skills from one task carry over to help with related tasks",
      "Copying one model's weights into another",
      "Teaching a model two languages at once"
    ],
    "correct": 0
  }
}
