{
  "id": "ai--understanding-llms--t-hallucinations-when-ai-makes-things-up",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch05-limitations",
  "title": "Hallucinations: When AI Makes Things Up",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A short lesson to help you apply Hallucinations: When AI Makes Things Up.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Ask an LLM for a source and it invents a real-sounding paper that doesn't exist.",
      "visual": "ğŸ“„"
    },
    "buildup": {
      "text": "Hallucinations happen when the model generates plausible-sounding but factually wrong content.",
      "visual": "ğŸŒ«ï¸"
    },
    "discovery": {
      "text": "The model predicts likely next tokens. If the truth is unlikely, it generates something plausible instead.",
      "visual": "ğŸ²"
    },
    "twist": {
      "text": "Hallucinations are confident. The AI doesn't signal uncertainty â€” it states fiction as fact.",
      "visual": "ğŸ˜Œ"
    },
    "climax": {
      "text": "Retrieval-augmented generation (RAG) helps by grounding responses in real documents.",
      "visual": "ğŸ“"
    },
    "punchline": {
      "text": "It doesn't know what it doesn't know.",
      "visual": "ğŸ•³ï¸"
    }
  },
  "quiz": {
    "question": "Why do LLMs hallucinate?",
    "options": [
      "They predict plausible tokens even when truth is unlikely",
      "They deliberately lie to users",
      "Their memory gets corrupted over time"
    ],
    "correct": 0
  }
}
