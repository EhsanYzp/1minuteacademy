{
  "id": "ai--understanding-llms--t-bias-in-language-models",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch05-limitations",
  "title": "Bias in Language Models",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "One-minute skill: Bias in Language Models.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Ask an LLM to describe a CEO and it defaults to male. The bias came from training data.",
      "visual": "ğŸ‘”"
    },
    "buildup": {
      "text": "LLMs absorb the biases present in the text they're trained on â€” societal, cultural, and historical.",
      "visual": "ğŸ“š"
    },
    "discovery": {
      "text": "Gender, racial, and cultural biases show up in word associations and default assumptions.",
      "visual": "âš–ï¸"
    },
    "twist": {
      "text": "Debiasing is hard. Remove one bias and you might amplify another or lose useful information.",
      "visual": "ğŸ”„"
    },
    "climax": {
      "text": "Researchers use RLHF and filtered data to reduce bias, but no model is truly neutral.",
      "visual": "ğŸ¯"
    },
    "punchline": {
      "text": "AI mirrors society's biases at scale.",
      "visual": "ğŸª"
    }
  },
  "quiz": {
    "question": "Where do LLM biases come from?",
    "options": [
      "From biases present in the training data",
      "From the hardware they run on",
      "From user interactions after deployment"
    ],
    "correct": 0
  }
}
