{
  "id": "ai--understanding-llms--t-the-transformer-architecture",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch01-what-are-llms",
  "title": "The Transformer Architecture",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "Learn The Transformer Architecture in one minute.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Every major LLM â€” GPT, Claude, Gemini, Llama â€” is built on the same 2017 invention.",
      "visual": "ğŸ—ï¸"
    },
    "buildup": {
      "text": "The transformer uses self-attention to let every word look at every other word simultaneously.",
      "visual": "ğŸ‘€"
    },
    "discovery": {
      "text": "Unlike older models that read left-to-right, transformers process entire sequences at once.",
      "visual": "âš¡"
    },
    "twist": {
      "text": "This parallelism is why transformers train so efficiently on GPUs.",
      "visual": "ğŸ–¥ï¸"
    },
    "climax": {
      "text": "The architecture is surprisingly simple â€” attention layers stacked with feed-forward networks.",
      "visual": "ğŸ§±"
    },
    "punchline": {
      "text": "Simple parts, stacked deep, produce something extraordinary.",
      "visual": "âœ¨"
    }
  },
  "quiz": {
    "question": "What is the key advantage of the transformer architecture?",
    "options": [
      "It processes all words in a sequence simultaneously",
      "It reads text one character at a time",
      "It doesn't require training data"
    ],
    "correct": 0
  }
}
