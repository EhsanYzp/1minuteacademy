{
  "id": "ai--understanding-llms--t-why-attention-matters",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch03-attention",
  "title": "Why Attention Matters",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A short lesson to help you apply Why Attention Matters.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "In the sentence 'The bank by the river was muddy,' how does AI know 'bank' means riverbank?",
      "visual": "ğŸï¸"
    },
    "buildup": {
      "text": "Old models processed words in order and forgot earlier context. Attention changed everything.",
      "visual": "ğŸ§“"
    },
    "discovery": {
      "text": "Attention lets each word look at every other word to determine which ones are most relevant.",
      "visual": "ğŸ”"
    },
    "twist": {
      "text": "'Bank' attends strongly to 'river' and 'muddy,' which disambiguates its meaning instantly.",
      "visual": "ğŸ¯"
    },
    "climax": {
      "text": "This mechanism is why LLMs handle long, complex sentences that old models couldn't.",
      "visual": "ğŸ“œ"
    },
    "punchline": {
      "text": "Context is everything. Attention captures it.",
      "visual": "ğŸ‘ï¸"
    }
  },
  "quiz": {
    "question": "How does the attention mechanism resolve word ambiguity?",
    "options": [
      "By letting each word examine every other word for context",
      "By looking up definitions in a dictionary",
      "By always choosing the most common meaning"
    ],
    "correct": 0
  }
}
