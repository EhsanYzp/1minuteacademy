{
  "id": "ai--understanding-llms--t-multi-head-attention-seeing-multiple-patterns",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch03-attention",
  "title": "Multi-Head Attention: Seeing Multiple Patterns",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Multi-Head Attention: Seeing Multiple Patterns.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "One attention head tracks grammar. Another tracks meaning. A third tracks sentiment. All run in parallel.",
      "visual": "ğŸ™"
    },
    "buildup": {
      "text": "Multi-head attention runs several attention operations simultaneously on the same input.",
      "visual": "ğŸ”€"
    },
    "discovery": {
      "text": "Each head learns to focus on different relationship types â€” syntax, coreference, topic.",
      "visual": "ğŸ”¬"
    },
    "twist": {
      "text": "Researchers can visualize what each head focuses on. Some heads specialize in surprising ways.",
      "visual": "ğŸ“¸"
    },
    "climax": {
      "text": "The outputs of all heads are combined, giving the model a rich, multi-perspective view.",
      "visual": "ğŸŒˆ"
    },
    "punchline": {
      "text": "Many eyes see more than one.",
      "visual": "ğŸ‘€"
    }
  },
  "quiz": {
    "question": "Why does multi-head attention use multiple heads?",
    "options": [
      "Each head captures different types of relationships",
      "More heads make the model faster",
      "Only one head is active at a time"
    ],
    "correct": 0
  }
}
