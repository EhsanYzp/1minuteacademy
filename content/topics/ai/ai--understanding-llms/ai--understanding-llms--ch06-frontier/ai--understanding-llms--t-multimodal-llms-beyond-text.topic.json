{
  "id": "ai--understanding-llms--t-multimodal-llms-beyond-text",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "Understanding Large Language Models",
  "course_id": "ai--understanding-llms",
  "chapter_id": "ai--understanding-llms--ch06-frontier",
  "title": "Multimodal LLMs: Beyond Text",
  "emoji": "ğŸ§ ",
  "color": "#7C3AED",
  "description": "A quick win: understand Multimodal LLMs: Beyond Text.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Send a photo of your fridge and ask 'What can I cook?' The AI sees the ingredients and suggests recipes.",
      "visual": "ğŸ³"
    },
    "buildup": {
      "text": "Multimodal LLMs process text, images, audio, and even video in a single unified model.",
      "visual": "ğŸ”„"
    },
    "discovery": {
      "text": "GPT-4V and Gemini use vision encoders that translate images into the same token space as text.",
      "visual": "ğŸ‘ï¸"
    },
    "twist": {
      "text": "Vision capabilities are still inconsistent â€” models may misread simple clocks or count objects wrong.",
      "visual": "â°"
    },
    "climax": {
      "text": "Multimodal AI opens applications impossible with text alone â€” from medical imaging to accessibility.",
      "visual": "ğŸ¥"
    },
    "punchline": {
      "text": "First it read. Then it saw. Soon it will sense the whole world.",
      "visual": "ğŸŒ"
    }
  },
  "quiz": {
    "question": "What makes multimodal LLMs different from text-only models?",
    "options": [
      "They process images, audio, and text in a unified architecture",
      "They only work with images, not text",
      "They require separate models for each input type"
    ],
    "correct": 0
  }
}
