{
  "id": "ai--evaluation-and-testing--t18-the-eval-mindset",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch01-why-eval-matters",
  "title": "The Eval Mindset",
  "emoji": "ğŸ§ª",
  "color": "#EF4444",
  "description": "Thinking about evaluation from the very first line of your AI project.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ§ ",
      "text": "A team builds an AI feature for 3 months, then asks 'How do we test this?' They should have asked that on day one."
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "The eval mindset means defining 'what does good look like?' before writing any code. If you can't describe success, you can't measure it."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Start every AI feature with three questions: (1) What output quality means for this use case, (2) How you'll measure it, (3) What score threshold means 'good enough.'"
    },
    "twist": {
      "visual": "âš¡",
      "text": "Eval isn't a phase â€” it's a continuous practice. The best teams run evals on every prompt change, every model swap, every data update."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Teams with an eval mindset ship faster because they have data to back decisions. No debates about 'which prompt is better' â€” the eval score decides."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Define good before building anything. Then measure relentlessly."
    }
  },
  "quiz": {
    "question": "When should you start thinking about evaluation for an AI feature?",
    "options": [
      "After deployment",
      "During the last sprint before launch",
      "From the very beginning of the project",
      "Only when users complain"
    ],
    "correct": 2
  }
}
