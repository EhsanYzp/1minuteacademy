{
  "id": "ai--evaluation-and-testing--t21-latency-as-metric",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch02-metrics",
  "title": "Latency as a Metric",
  "emoji": "ğŸ§ª",
  "color": "#EF4444",
  "description": "Why response time is a critical quality metric for AI systems.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "â±ï¸",
      "text": "Your AI feature is 95% accurate. But it takes 12 seconds to respond. Users abandon after 3. Accuracy is irrelevant if no one waits for the answer."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "Latency metrics to track: P50 (median), P95 (worst 5%), P99 (worst 1%), and time-to-first-token (for streaming). Each tells a different story."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Set latency budgets per feature: 'Chat must respond within 2s P95. Search must return within 500ms P95.' Treat latency regressions as bugs, just like accuracy regressions."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Latency and quality often trade off directly. A larger model is more accurate but slower. Shorter prompts are faster but less accurate. Your eval must measure both."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "The best eval dashboards show quality AND latency side by side. An improvement that tanks latency isn't an improvement."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "A perfect answer delivered too late is a failed answer. Measure speed alongside quality."
    }
  },
  "quiz": {
    "question": "Which latency metric captures the experience of the worst 5% of users?",
    "options": [
      "P50",
      "P95",
      "Average latency",
      "Time-to-first-token"
    ],
    "correct": 1
  }
}
