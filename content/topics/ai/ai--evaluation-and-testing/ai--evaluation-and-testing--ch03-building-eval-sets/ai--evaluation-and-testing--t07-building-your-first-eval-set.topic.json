{
  "id": "ai--evaluation-and-testing--t07-building-your-first-eval-set",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch03-building-eval-sets",
  "title": "Building Your First Eval Set",
  "emoji": "ğŸ§ª",
  "color": "#EF4444",
  "description": "Creating a dataset of questions and expected answers.",
  "difficulty": "Beginner",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“",
      "text": "You need to evaluate your chatbot but you have zero test data. Where do you even start?"
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "Start small: write 30 questions that represent what real users ask. For each question, write the expected answer (or key facts the answer should contain). That's your eval set."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Cover the distribution: don't just write easy questions. Include hard edge cases, ambiguous queries, and questions your system can't answer (to test refusal behavior)."
    },
    "twist": {
      "visual": "âš¡",
      "text": "30 well-chosen examples beat 1,000 random ones. Focus on diversity of question types, not raw count. You can always expand later."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Format: JSON or CSV with columns for query, expected_answer, category, and difficulty. Keep it version-controlled alongside your code."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Thirty questions. Three hours. That's all it takes to stop guessing and start measuring."
    }
  },
  "quiz": {
    "question": "What makes a good eval set?",
    "options": [
      "As many questions as possible, regardless of quality",
      "A diverse set covering easy cases, edge cases, and out-of-scope questions",
      "Only questions the system answers correctly",
      "Random samples from the internet"
    ],
    "correct": 1
  }
}
