{
  "id": "ai--evaluation-and-testing--t23-test-case-coverage",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch03-building-eval-sets",
  "title": "Test Case Coverage",
  "emoji": "ðŸ§ª",
  "color": "#EF4444",
  "description": "Ensuring your eval set covers the full range of real-world inputs.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ðŸ—ºï¸",
      "text": "Your eval set has 50 questions â€” all in English, all well-formatted, all about the same topic. Eval score: 96%. Real-world performance: 60%."
    },
    "buildup": {
      "visual": "ðŸ“Š",
      "text": "Coverage means your eval set represents the true distribution of inputs: different topics, languages, difficulty levels, edge cases, adversarial inputs, and formatting variations."
    },
    "discovery": {
      "visual": "ðŸ’¡",
      "text": "Create a coverage matrix: categories Ã— difficulty Ã— format. Ensure every cell has at least 3-5 test cases. Gaps in coverage are blind spots in your evaluation."
    },
    "twist": {
      "visual": "âš¡",
      "text": "The hardest cases to cover are the ones you haven't seen yet. Use production logs to discover new input patterns and add them to your eval set quarterly."
    },
    "climax": {
      "visual": "ðŸ",
      "text": "A small, well-covered eval set (100 diverse cases) beats a large, homogeneous one (1000 similar cases). Diversity > quantity."
    },
    "punchline": {
      "visual": "ðŸŽ¬",
      "text": "Your eval is only as good as its weakest blind spot. Cover the edges."
    }
  },
  "quiz": {
    "question": "What makes an eval set effective?",
    "options": [
      "Having as many test cases as possible",
      "Only including easy, well-formatted questions",
      "Covering diverse topics, difficulties, formats, and edge cases",
      "Using only synthetic data"
    ],
    "correct": 2
  }
}
