{
  "id": "ai--evaluation-and-testing--t25-llm-judge-rubrics",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch04-automated-evaluation",
  "title": "LLM Judge Rubrics",
  "emoji": "ğŸ§ª",
  "color": "#EF4444",
  "description": "Designing scoring rubrics that make LLM-as-judge evaluations reliable.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“‹",
      "text": "'Rate this response on a scale of 1-5.' The LLM judge gives everything a 4. Not useful. The rubric was too vague."
    },
    "buildup": {
      "visual": "ğŸ“",
      "text": "A rubric defines exactly what each score means. '5 = fully correct, cites sources, well-structured. 3 = partially correct, missing key details. 1 = factually wrong or harmful.'"
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Good rubrics are specific to your task, include examples for each score level, and define exactly what constitutes a pass vs fail. The LLM judge needs the same clarity a human judge would."
    },
    "twist": {
      "visual": "âš¡",
      "text": "LLM judges have biases: they prefer longer responses, are more lenient than humans, and can be influenced by response ordering. Calibrate your rubric against human scores."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "A well-designed rubric makes LLM-as-judge scores 85-90% correlated with human judgments. A vague rubric gives random results."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The judge is only as good as the rubric. Invest in the scoring criteria."
    }
  },
  "quiz": {
    "question": "What makes an LLM-as-judge rubric effective?",
    "options": [
      "Using a simple 1-10 scale without descriptions",
      "Specific score definitions with examples for each level",
      "Letting the LLM define its own criteria",
      "Using binary pass/fail only"
    ],
    "correct": 1
  }
}
