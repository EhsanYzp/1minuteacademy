{
  "id": "ai--evaluation-and-testing--t28-crowdsourced-eval",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch05-human-evaluation",
  "title": "Crowdsourced Eval",
  "emoji": "üß™",
  "color": "#EF4444",
  "description": "Using crowd workers for scalable human evaluation.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "üë•",
      "text": "You need 10,000 human judgments for your eval set. Your team of 3 would take months. A crowd platform delivers them in 48 hours."
    },
    "buildup": {
      "visual": "üåê",
      "text": "Platforms like Scale AI, Surge AI, and Amazon Mechanical Turk provide on-demand human annotators who can rate, compare, or label AI outputs at scale."
    },
    "discovery": {
      "visual": "üí°",
      "text": "Key practices: provide clear guidelines with examples, include gold-standard questions (known answers) to detect low-quality workers, and collect 3+ judgments per item to smooth out noise."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "Crowd quality varies enormously. Some workers rush through tasks. Use qualification tests, attention checks, and agreement-based filtering to maintain quality."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "Crowdsourced eval gives you scale and diversity that internal teams can't match. With proper quality controls, it's the backbone of most serious AI evaluation programs."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "Crowd eval scales. But without quality controls, it scales garbage."
    }
  },
  "quiz": {
    "question": "How do you maintain quality in crowdsourced evaluation?",
    "options": [
      "Accept all annotations without review",
      "Use gold-standard questions, attention checks, and multiple judgments per item",
      "Only use one annotator per item",
      "Skip qualification tests to save time"
    ],
    "correct": 1
  }
}
