{
  "id": "ai--evaluation-and-testing--t27-inter-annotator-agreement",
  "version": 1,
  "subject": "AI",
  "subcategory": "Evaluation & Testing",
  "course_id": "ai--evaluation-and-testing",
  "chapter_id": "ai--evaluation-and-testing--ch05-human-evaluation",
  "title": "Inter-Annotator Agreement",
  "emoji": "ğŸ§ª",
  "color": "#EF4444",
  "description": "Measuring consistency between different human evaluators.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ¤",
      "text": "Three annotators rate the same AI response. One gives 5/5, another 2/5, and the third 4/5. Which score do you trust? None of them â€” your annotation guidelines are broken."
    },
    "buildup": {
      "visual": "ğŸ“",
      "text": "Inter-annotator agreement (IAA) measures how consistently multiple judges rate the same items. Cohen's kappa and Krippendorff's alpha are common metrics; >0.7 indicates good agreement."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Low agreement means the task is ambiguous, the rubric is unclear, or the annotators need more training. Fix the guidelines, run calibration sessions, and re-measure."
    },
    "twist": {
      "visual": "âš¡",
      "text": "If humans can't agree on what's 'good,' your LLM judge won't either. IAA is the ceiling for automated evaluation quality â€” you can't automate what humans can't consistently do."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Always measure IAA before scaling up annotation. If agreement is low, invest in better guidelines and training before annotating thousands of examples."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "If your judges disagree, the problem isn't the judges â€” it's the rubric."
    }
  },
  "quiz": {
    "question": "What does low inter-annotator agreement indicate?",
    "options": [
      "The annotators are lazy",
      "The task or rubric is ambiguous and needs clarification",
      "The AI outputs are perfect",
      "You need more annotators"
    ],
    "correct": 1
  }
}
