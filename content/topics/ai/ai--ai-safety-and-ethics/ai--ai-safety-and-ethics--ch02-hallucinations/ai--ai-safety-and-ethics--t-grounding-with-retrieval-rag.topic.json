{
  "id": "ai--ai-safety-and-ethics--t-grounding-with-retrieval-rag",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Safety & Ethics",
  "course_id": "ai--ai-safety-and-ethics",
  "chapter_id": "ai--ai-safety-and-ethics--ch02-hallucinations",
  "title": "Grounding with Retrieval (RAG)",
  "emoji": "ğŸ›¡ï¸",
  "color": "#DC2626",
  "description": "Learn Grounding with Retrieval (RAG) in one minute.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Instead of trusting the model's memory, hand it the source document.",
      "visual": "ğŸ“š"
    },
    "buildup": {
      "text": "RAG retrieves relevant documents and injects them into the prompt before generation.",
      "visual": "ğŸ”"
    },
    "discovery": {
      "text": "The model can now cite real passages instead of inventing plausible-sounding ones.",
      "visual": "ğŸ“Œ"
    },
    "twist": {
      "text": "RAG reduces but doesn't eliminate hallucinationsâ€”the model can still misinterpret context.",
      "visual": "âš ï¸"
    },
    "climax": {
      "text": "Add citation verification: check that the model's claim actually appears in the source.",
      "visual": "âœ…"
    },
    "punchline": {
      "text": "Don't ask the model to remember. Give it the receipts.",
      "visual": "ğŸ§¾"
    }
  },
  "quiz": {
    "question": "What does RAG do to reduce hallucinations?",
    "options": [
      "Injects retrieved documents into the prompt",
      "Deletes false outputs",
      "Retrains the model in real time"
    ],
    "correct": 0
  }
}
