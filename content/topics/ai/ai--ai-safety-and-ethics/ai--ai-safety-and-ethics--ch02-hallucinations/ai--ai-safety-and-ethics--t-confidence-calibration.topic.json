{
  "id": "ai--ai-safety-and-ethics--t-confidence-calibration",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Safety & Ethics",
  "course_id": "ai--ai-safety-and-ethics",
  "chapter_id": "ai--ai-safety-and-ethics--ch02-hallucinations",
  "title": "Confidence Calibration",
  "emoji": "ğŸ›¡ï¸",
  "color": "#DC2626",
  "description": "A quick, practical guide to Confidence Calibration.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "The model says 'definitely Paris' when the answer is Berlin. Its confidence lied.",
      "visual": "ğŸ—¼"
    },
    "buildup": {
      "text": "Well-calibrated models express low confidence when they're uncertain.",
      "visual": "ğŸ“Š"
    },
    "discovery": {
      "text": "Calibration techniques: temperature scaling, verbalized confidence, and log-prob thresholds.",
      "visual": "ğŸŒ¡ï¸"
    },
    "twist": {
      "text": "RLHF makes models sound confident even when wrongâ€”it trains away hedging language.",
      "visual": "ğŸ­"
    },
    "climax": {
      "text": "For safety-critical apps, block outputs below a confidence threshold and escalate to humans.",
      "visual": "ğŸš¦"
    },
    "punchline": {
      "text": "Confidence without calibration is just eloquent guessing.",
      "visual": "ğŸ²"
    }
  },
  "quiz": {
    "question": "How does RLHF affect model calibration?",
    "options": [
      "It trains away hedging, making the model sound confident even when wrong",
      "It improves calibration",
      "It has no effect on confidence"
    ],
    "correct": 0
  }
}
