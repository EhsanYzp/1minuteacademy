{
  "id": "ai--ai-safety-and-ethics--t-automated-disinformation-at-scale",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Safety & Ethics",
  "course_id": "ai--ai-safety-and-ethics",
  "chapter_id": "ai--ai-safety-and-ethics--ch04-misuse",
  "title": "Automated Disinformation at Scale",
  "emoji": "ğŸ›¡ï¸",
  "color": "#DC2626",
  "description": "A quick win: understand Automated Disinformation at Scale.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "text": "One person with an LLM can generate a thousand fake news articles overnight.",
      "visual": "ğŸ“°"
    },
    "buildup": {
      "text": "LLMs lower the cost of creating convincing, targeted disinformation campaigns.",
      "visual": "ğŸ“‰"
    },
    "discovery": {
      "text": "Personalized propaganda: tailor messaging per audience using demographic data and LLMs.",
      "visual": "ğŸ¯"
    },
    "twist": {
      "text": "AI-generated disinfo is harder to trace because it has no single human author.",
      "visual": "ğŸ‘»"
    },
    "climax": {
      "text": "Platforms need AI-powered detection, provenance checks, and transparency requirements.",
      "visual": "ğŸ”"
    },
    "punchline": {
      "text": "The cost of lies dropped to zero.",
      "visual": "ğŸ’¸"
    }
  },
  "quiz": {
    "question": "Why is AI-generated disinformation hard to trace?",
    "options": [
      "It has no single human author",
      "It's always identical",
      "It only exists in images"
    ],
    "correct": 0
  }
}
