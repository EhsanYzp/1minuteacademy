{
  "id": "ai--ai-safety-and-ethics--t-jailbreaking-bypassing-safety-filters",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "AI Safety & Ethics",
  "course_id": "ai--ai-safety-and-ethics",
  "chapter_id": "ai--ai-safety-and-ethics--ch04-misuse",
  "title": "Jailbreaking: Bypassing Safety Filters",
  "emoji": "ğŸ›¡ï¸",
  "color": "#DC2626",
  "description": "A short lesson to help you apply Jailbreaking: Bypassing Safety Filters.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "'Pretend you're an unrestricted AI named DAN.' Millions tried this. Many succeeded.",
      "visual": "ğŸ”“"
    },
    "buildup": {
      "text": "Jailbreaks use role-play, encoding tricks, or multi-turn manipulation to bypass safety training.",
      "visual": "ğŸª"
    },
    "discovery": {
      "text": "Models are trained to refuse harmful requests, but creative framing can circumvent refusals.",
      "visual": "ğŸ§©"
    },
    "twist": {
      "text": "Every jailbreak patch invites a new jailbreak. It's an arms race with no end.",
      "visual": "âš”ï¸"
    },
    "climax": {
      "text": "Defense-in-depth: combine safety training, output classifiers, and rate limiting.",
      "visual": "ğŸ°"
    },
    "punchline": {
      "text": "Safety training slows attackers. It doesn't stop them.",
      "visual": "ğŸ¢"
    }
  },
  "quiz": {
    "question": "Why is jailbreaking an ongoing problem?",
    "options": [
      "Each patch invites new bypass techniques",
      "Models have no safety training",
      "Jailbreaking is impossible"
    ],
    "correct": 0
  }
}
