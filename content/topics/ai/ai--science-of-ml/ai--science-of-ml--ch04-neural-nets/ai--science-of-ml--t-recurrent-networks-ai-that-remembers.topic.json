{
  "id": "ai--science-of-ml--t-recurrent-networks-ai-that-remembers",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch04-neural-nets",
  "title": "Recurrent Networks: AI That Remembers",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A quick win: understand Recurrent Networks: AI That Remembers.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "To understand 'it' in 'The cat sat. It purred.' you need to remember 'cat.' RNNs do that.",
      "visual": "ğŸ”„"
    },
    "buildup": {
      "text": "Recurrent Neural Networks pass information from one step to the next, creating a form of memory.",
      "visual": "ğŸ§ "
    },
    "discovery": {
      "text": "LSTMs added gates that control what to remember and what to forget at each step.",
      "visual": "ğŸšª"
    },
    "twist": {
      "text": "RNNs process sequentially â€” slow. Transformers replaced them by processing in parallel.",
      "visual": "âš¡"
    },
    "climax": {
      "text": "RNNs pioneered sequence modeling. Transformers inherited the crown and ran with it.",
      "visual": "ğŸ‘‘"
    },
    "punchline": {
      "text": "Remember the past. Transformers just do it faster.",
      "visual": "ğŸƒ"
    }
  },
  "quiz": {
    "question": "What limitation of RNNs did transformers solve?",
    "options": [
      "RNNs process sequentially and are slow; transformers parallelize",
      "RNNs use too little memory",
      "Transformers don't need training data"
    ],
    "correct": 0
  }
}
