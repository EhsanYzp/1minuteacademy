{
  "id": "ai--science-of-ml--t-recurrent-neural-networks-memory-for-sequences",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch04-neural-nets",
  "title": "Recurrent Neural Networks: Memory for Sequences",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A short lesson to help you apply Recurrent Neural Networks: Memory for Sequences.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Regular networks forget everything between inputs. Recurrent networks remember â€” they have loops.",
      "visual": "ğŸ”"
    },
    "buildup": {
      "text": "RNNs pass information from one step to the next, maintaining a hidden state like short-term memory.",
      "visual": "ğŸ§ "
    },
    "discovery": {
      "text": "This makes them natural for sequences â€” text, speech, music, stock prices over time.",
      "visual": "ğŸ“ˆ"
    },
    "twist": {
      "text": "Vanilla RNNs suffer from vanishing gradients â€” they forget long-ago context. LSTMs fixed this.",
      "visual": "ğŸ’¡"
    },
    "climax": {
      "text": "Transformers eventually replaced RNNs for most tasks, but the concept of memory in networks endures.",
      "visual": "ğŸ”„"
    },
    "punchline": {
      "text": "To understand a sentence, remember its start. Networks learned that.",
      "visual": "ğŸ“–"
    }
  },
  "quiz": {
    "question": "What problem do recurrent neural networks solve?",
    "options": [
      "Processing sequential data by maintaining memory between steps",
      "Classifying individual images without context",
      "Generating random numbers efficiently"
    ],
    "correct": 0
  }
}
