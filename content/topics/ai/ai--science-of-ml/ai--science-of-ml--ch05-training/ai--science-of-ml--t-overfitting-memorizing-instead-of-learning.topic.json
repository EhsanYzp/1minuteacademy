{
  "id": "ai--science-of-ml--t-overfitting-memorizing-instead-of-learning",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch05-training",
  "title": "Overfitting: Memorizing Instead of Learning",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Overfitting: Memorizing Instead of Learning.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "A student memorizes every test answer but can't solve a new problem. That's overfitting.",
      "visual": "ğŸ“–"
    },
    "buildup": {
      "text": "An overfit model performs perfectly on training data but fails on new, unseen data.",
      "visual": "ğŸ“ˆ"
    },
    "discovery": {
      "text": "It memorized noise and quirks in the training set instead of learning general patterns.",
      "visual": "ğŸ”Š"
    },
    "twist": {
      "text": "More data, dropout, and regularization all help prevent overfitting â€” but too much kills learning.",
      "visual": "âš–ï¸"
    },
    "climax": {
      "text": "The training/validation split is how you catch overfitting: test on data the model hasn't seen.",
      "visual": "ğŸ§ª"
    },
    "punchline": {
      "text": "The goal isn't to remember. It's to generalize.",
      "visual": "ğŸŒ"
    }
  },
  "quiz": {
    "question": "What causes overfitting?",
    "options": [
      "The model memorizes training data instead of learning patterns",
      "The model is too simple",
      "The training data is too clean"
    ],
    "correct": 0
  }
}
