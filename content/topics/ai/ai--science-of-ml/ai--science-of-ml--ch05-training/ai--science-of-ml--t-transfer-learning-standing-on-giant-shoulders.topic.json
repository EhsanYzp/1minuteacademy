{
  "id": "ai--science-of-ml--t-transfer-learning-standing-on-giant-shoulders",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch05-training",
  "title": "Transfer Learning: Standing on Giant Shoulders",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Transfer Learning: Standing on Giant Shoulders.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Train a model on a million images, then adapt it for medical scans with just 200 examples. That's transfer learning.",
      "visual": "ğŸ”„"
    },
    "buildup": {
      "text": "Training from scratch requires massive datasets. Transfer learning reuses knowledge from pre-trained models.",
      "visual": "ğŸ—ï¸"
    },
    "discovery": {
      "text": "Early layers learn general features like edges and textures. Later layers specialize for the new task.",
      "visual": "ğŸ¯"
    },
    "twist": {
      "text": "Sometimes transferred knowledge hurts â€” if the source domain is too different, negative transfer occurs.",
      "visual": "âŒ"
    },
    "climax": {
      "text": "Transfer learning made AI practical for fields with limited data â€” medicine, agriculture, rare languages.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "Don't start from zero. Start from where someone else finished.",
      "visual": "ğŸš€"
    }
  },
  "quiz": {
    "question": "When can transfer learning be harmful?",
    "options": [
      "When the source domain is too different from the target task",
      "When the model is too small",
      "Transfer learning is always beneficial"
    ],
    "correct": 0
  }
}
