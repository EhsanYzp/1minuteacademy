{
  "id": "ai--science-of-ml--t-batch-size-epochs-and-learning-rate",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch05-training",
  "title": "Batch Size, Epochs, and Learning Rate",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Batch Size, Epochs, and Learning Rate.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Three numbers control how a model trains: how much data per step, how many passes, how fast to learn.",
      "visual": "ğŸ›ï¸"
    },
    "buildup": {
      "text": "Batch size = examples per gradient update. Epoch = one full pass through data. Learning rate = step size.",
      "visual": "ğŸ“Š"
    },
    "discovery": {
      "text": "Small batches add noise that can help escape bad solutions. Large batches are more stable.",
      "visual": "ğŸŒŠ"
    },
    "twist": {
      "text": "The learning rate is the most critical setting. Too high: divergence. Too low: no progress.",
      "visual": "ğŸ“‰"
    },
    "climax": {
      "text": "Modern training uses learning rate schedules that start high and decay over time.",
      "visual": "ğŸ“…"
    },
    "punchline": {
      "text": "Training is a balancing act of speed, stability, and patience.",
      "visual": "ğŸª"
    }
  },
  "quiz": {
    "question": "What happens if the learning rate is too high?",
    "options": [
      "The model overshoots good solutions and diverges",
      "Training becomes slower but more accurate",
      "The model immediately converges"
    ],
    "correct": 0
  }
}
