{
  "id": "ai--science-of-ml--t-gradient-descent-learning-by-sliding-downhill",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch05-training",
  "title": "Gradient Descent: Learning by Sliding Downhill",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A 1-minute de-risking session on Gradient Descent: Learning by Sliding Downhill.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "Imagine you're blindfolded on a hilly landscape. You feel which way slopes down and step that way.",
      "visual": "â›°ï¸"
    },
    "buildup": {
      "text": "Gradient descent adjusts model weights in the direction that reduces the error most.",
      "visual": "ğŸ“‰"
    },
    "discovery": {
      "text": "The gradient tells you the slope. The learning rate tells you how big a step to take.",
      "visual": "ğŸ‘£"
    },
    "twist": {
      "text": "Too large a step and you overshoot. Too small and training takes forever.",
      "visual": "ğŸ¯"
    },
    "climax": {
      "text": "Every neural network ever trained uses some variant of gradient descent. It's the universal optimizer.",
      "visual": "ğŸŒ"
    },
    "punchline": {
      "text": "Slide downhill toward the answer. One step at a time.",
      "visual": "ğŸ‚"
    }
  },
  "quiz": {
    "question": "What does gradient descent optimize?",
    "options": [
      "It adjusts weights to minimize the model's error",
      "It increases the number of layers",
      "It selects the best training data"
    ],
    "correct": 0
  }
}
