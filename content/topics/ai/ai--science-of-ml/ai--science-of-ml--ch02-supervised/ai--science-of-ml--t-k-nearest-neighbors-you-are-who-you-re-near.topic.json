{
  "id": "ai--science-of-ml--t-k-nearest-neighbors-you-are-who-you-re-near",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch02-supervised",
  "title": "k-Nearest Neighbors: You Are Who You're Near",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A micro-lesson that makes k-Nearest Neighbors: You Are Who You're Near usable.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "To classify a new data point, just look at its closest neighbors and go with the majority.",
      "visual": "ğŸ‘¥"
    },
    "buildup": {
      "text": "k-NN stores all training data and classifies new points by proximity.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "If three of your five nearest neighbors are cats, you're probably a cat too.",
      "visual": "ğŸ±"
    },
    "twist": {
      "text": "It struggles in high dimensions â€” the 'curse of dimensionality' makes distances meaningless.",
      "visual": "ğŸŒ€"
    },
    "climax": {
      "text": "Despite its simplicity, k-NN works surprisingly well for recommendation systems.",
      "visual": "ğŸ¯"
    },
    "punchline": {
      "text": "The simplest algorithm: ask your neighbors.",
      "visual": "ğŸ˜ï¸"
    }
  },
  "quiz": {
    "question": "How does k-Nearest Neighbors classify a new data point?",
    "options": [
      "By majority vote of the closest training examples",
      "By fitting a complex equation",
      "By random selection"
    ],
    "correct": 0
  }
}
