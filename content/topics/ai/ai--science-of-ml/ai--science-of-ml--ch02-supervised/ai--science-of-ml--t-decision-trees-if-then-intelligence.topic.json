{
  "id": "ai--science-of-ml--t-decision-trees-if-then-intelligence",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch02-supervised",
  "title": "Decision Trees: If-Then Intelligence",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "One-minute skill: Decision Trees: If-Then Intelligence.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Is income > $50K? Yes â†’ Is age > 30? Yes â†’ Approve loan. A tree of questions makes the decision.",
      "visual": "ğŸŒ³"
    },
    "buildup": {
      "text": "Decision trees split data by asking the most informative question at each step.",
      "visual": "â“"
    },
    "discovery": {
      "text": "They're intuitive â€” you can literally draw the decision process and explain it to anyone.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "Single trees overfit easily. Random forests fix this by averaging hundreds of trees together.",
      "visual": "ğŸŒ²"
    },
    "climax": {
      "text": "Gradient-boosted trees (XGBoost, LightGBM) dominate tabular data competitions to this day.",
      "visual": "ğŸ†"
    },
    "punchline": {
      "text": "Sometimes the best AI is a really good game of 20 questions.",
      "visual": "ğŸ¤”"
    }
  },
  "quiz": {
    "question": "Why are random forests better than single decision trees?",
    "options": [
      "Averaging many trees reduces overfitting",
      "They use fewer features",
      "They're faster to train"
    ],
    "correct": 0
  }
}
