{
  "id": "ai--science-of-ml--t-dimensionality-reduction-simplifying-complexity",
  "version": 1,
  "subject": "AI & Agents",
  "subcategory": "The Science of Machine Learning",
  "course_id": "ai--science-of-ml",
  "chapter_id": "ai--science-of-ml--ch03-unsupervised",
  "title": "Dimensionality Reduction: Simplifying Complexity",
  "emoji": "ğŸ”¬",
  "color": "#7C3AED",
  "description": "A tiny lesson with a big payoff: Dimensionality Reduction: Simplifying Complexity.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Your dataset has 10,000 features. Most are redundant. How do you find the ones that matter?",
      "visual": "ğŸ—œï¸"
    },
    "buildup": {
      "text": "Dimensionality reduction compresses data to fewer features while preserving key patterns.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "PCA finds the axes of maximum variance â€” the directions along which data varies most.",
      "visual": "ğŸ“Š"
    },
    "twist": {
      "text": "You can visualize 1000-dimensional data in 2D. It won't be perfect, but patterns pop out.",
      "visual": "ğŸ—ºï¸"
    },
    "climax": {
      "text": "t-SNE and UMAP create stunning visualizations of high-dimensional data clusters.",
      "visual": "ğŸ¨"
    },
    "punchline": {
      "text": "Complexity hides patterns. Simplification reveals them.",
      "visual": "ğŸ”"
    }
  },
  "quiz": {
    "question": "What is the purpose of dimensionality reduction?",
    "options": [
      "To compress data while preserving important patterns",
      "To add more features to the data",
      "To increase model training time"
    ],
    "correct": 0
  }
}
