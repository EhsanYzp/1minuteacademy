{
  "id": "ai--ai-safety-basics--t15-content-moderation",
  "version": 1,
  "subject": "AI",
  "subcategory": "AI Safety Basics",
  "course_id": "ai--ai-safety-basics",
  "chapter_id": "ai--ai-safety-basics--ch06-building-safe-systems",
  "title": "Content Moderation",
  "emoji": "ğŸ›¡ï¸",
  "color": "#EF4444",
  "description": "Filtering harmful AI outputs at scale.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸš¦",
      "text": "Your AI generates 100,000 responses per day. A human reviews 50. The other 99,950 go out unchecked. Content moderation at AI scale requires automation."
    },
    "buildup": {
      "visual": "ğŸ›¡ï¸",
      "text": "Automated moderation layers: (1) Input classifiers (block harmful requests), (2) Output classifiers (flag harmful responses), (3) Rule-based filters (block specific patterns), (4) Human review queue for edge cases."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Use AI to moderate AI: a smaller, specialized safety classifier can scan outputs in real-time. Flag anything above a risk threshold for human review. Route clear violations to automatic blocking."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Moderation at scale has false positives: legitimate questions about safety, medical topics, or historical events get blocked. Users get frustrated. Tune sensitivity by domain."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Build a moderation pipeline: automatic blocking for clear violations, human review for borderline cases, and feedback loops to improve the classifier over time."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "You can't review every AI output by hand. Build automated moderation and handle the exceptions."
    }
  },
  "quiz": {
    "question": "What's the main challenge of AI content moderation at scale?",
    "options": [
      "It's too cheap to implement",
      "Balancing false positives (blocking legitimate content) with catching actual harmful outputs",
      "Human reviewers can check every response",
      "Content moderation isn't needed for AI systems"
    ],
    "correct": 1
  }
}
