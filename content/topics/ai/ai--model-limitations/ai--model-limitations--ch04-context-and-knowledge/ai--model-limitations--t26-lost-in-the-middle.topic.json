{
  "id": "ai--model-limitations--t26-lost-in-the-middle",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
  "title": "Lost in the Middle",
  "emoji": "ğŸš§",
  "color": "#EF4444",
  "description": "Models pay more attention to the beginning and end of long contexts.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“„",
      "text": "You give the model a 20-page document and ask a question. The answer is on page 11. The model misses it. The same answer on page 1 or page 20? Found instantly."
    },
    "buildup": {
      "visual": "ğŸ“Š",
      "text": "The 'Lost in the Middle' phenomenon: models attend more strongly to information at the beginning and end of the context window. Information in the middle receives less attention."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Research shows accuracy on question-answering tasks drops significantly when the relevant information is in the middle 40% of the context. Position matters more than it should."
    },
    "twist": {
      "visual": "âš¡",
      "text": "This undermines the promise of large context windows. 128K tokens sounds impressive, but if the model ignores information in the middle, those extra tokens are less useful than they seem."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Mitigate: put the most important information first (or last), use chunking + retrieval instead of dumping entire documents, and test with information at different positions."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "A 128K context window is great. The model's attention span within it is not."
    }
  },
  "quiz": {
    "question": "Where in a long context does a model pay the least attention?",
    "options": [
      "The beginning",
      "The end",
      "The middle",
      "Attention is equal throughout"
    ],
    "correct": 2
  }
}
