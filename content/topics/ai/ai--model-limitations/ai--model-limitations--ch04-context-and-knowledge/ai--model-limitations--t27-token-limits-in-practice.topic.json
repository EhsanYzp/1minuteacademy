{
  "id": "ai--model-limitations--t27-token-limits-in-practice",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
  "title": "Token Limits in Practice",
  "emoji": "ğŸš§",
  "color": "#EF4444",
  "description": "What actually happens when you exceed or approach context limits.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“",
      "text": "Your prompt uses 120K out of 128K tokens. The model has 8K tokens left for its response. It starts generating, hits the limit mid-sentence, and stops. Your answer is truncated."
    },
    "buildup": {
      "visual": "ğŸ“¦",
      "text": "The context window is shared between input and output. A 128K model with a 100K prompt can only generate 28K tokens of response. Stuff too much in, and responses get cut short."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Token counting is tricky: 1 token â‰ˆ Â¾ of a word in English, but varies by language and content. Code uses more tokens per line than prose. Non-Latin scripts use more tokens per word."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Approaching the limit doesn't just truncate â€” quality degrades before you hit the wall. Models get less coherent and more repetitive as they generate long outputs, even within the limit."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Rule of thumb: use at most 80% of the context window for input. Monitor token usage in production. Implement pagination or summarization for long conversations."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "Context windows have hard walls. Budget your tokens or crash into them."
    }
  },
  "quiz": {
    "question": "What happens as the model approaches its context window limit?",
    "options": [
      "Performance stays constant until the exact limit",
      "Quality degrades and responses may be truncated before or at the limit",
      "The model automatically summarizes to save space",
      "Context limits don't affect output quality"
    ],
    "correct": 1
  }
}
