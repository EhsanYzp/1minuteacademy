{
  "id": "ai--model-limitations--t06-citation-hallucinations",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch02-hallucinations",
  "title": "Citation Hallucinations",
  "emoji": "ğŸš§",
  "color": "#EF4444",
  "description": "When the model invents sources that don't exist.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸ“–",
      "text": "A lawyer submits a brief citing six precedent cases. The judge finds that none of the cases exist â€” the model generated realistic-sounding case names, docket numbers, and summaries for imaginary cases."
    },
    "buildup": {
      "visual": "ğŸ“‹",
      "text": "Citation hallucination: the model generates sources (papers, court cases, URLs, book titles) that follow the pattern of real citations but reference things that don't exist."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "The model doesn't search databases â€” it generates text that looks like a citation. 'Smith v. Jones, 2019 WL 1234567' follows the pattern perfectly, but the case is invented."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Citation hallucinations are especially insidious because people trust citations â€” the whole point of a citation is to verify a claim. When the citation itself is fake, the verification chain breaks."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "Never trust AI-generated citations without verification. Use RAG with actual databases, or cross-reference every citation with the real source before using it."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "A hallucinated citation isn't evidence â€” it's evidence of how well the model mimics citation formats."
    }
  },
  "quiz": {
    "question": "Why are citation hallucinations particularly problematic?",
    "options": [
      "They're easy to spot",
      "People trust citations as verification, so fake citations undermine the entire trust chain",
      "Citations aren't important",
      "LLMs don't generate citations"
    ],
    "correct": 1
  }
}
