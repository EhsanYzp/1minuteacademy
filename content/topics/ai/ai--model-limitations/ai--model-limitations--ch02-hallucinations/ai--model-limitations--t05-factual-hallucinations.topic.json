{
  "id": "ai--model-limitations--t05-factual-hallucinations",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch02-hallucinations",
  "title": "Factual Hallucinations",
  "emoji": "üöß",
  "color": "#EF4444",
  "description": "When the model states false facts as truth.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "‚ùå",
      "text": "'Einstein won the Nobel Prize in 1905 for his theory of relativity.' Sounds right, but it's wrong twice: he won it in 1921, and it was for the photoelectric effect."
    },
    "buildup": {
      "visual": "üìã",
      "text": "Factual hallucinations include: wrong dates, invented statistics, attributed quotes to wrong people, confused identities (mixing up similar-sounding names), and completely fabricated events."
    },
    "discovery": {
      "visual": "üí°",
      "text": "Factual hallucinations are most dangerous in professional contexts: legal advice with fake case citations, medical information with wrong dosages, financial data with invented numbers."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "Partial truths are worse than complete fabrications: 'Einstein won the Nobel Prize' (true) 'in 1905' (wrong) 'for relativity' (wrong). The grain of truth makes the errors harder to catch."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "For any factual claim: verify independently. Use RAG to ground facts in real data. Add disclaimers for high-stakes content. Never ship unverified AI-generated facts."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "The most convincing hallucinations contain real facts mixed with false ones. That's what makes them dangerous."
    }
  },
  "quiz": {
    "question": "Why are partial-truth hallucinations more dangerous than complete fabrications?",
    "options": [
      "They're not ‚Äî complete fabrications are always worse",
      "The grain of truth makes the false parts harder to detect",
      "Partial truths are always caught by users",
      "LLMs never produce partial truths"
    ],
    "correct": 1
  }
}
