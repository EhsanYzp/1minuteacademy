{
  "id": "ai--model-limitations--t21-detecting-hallucinations",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch02-hallucinations",
  "title": "Detecting Hallucinations",
  "emoji": "üöß",
  "color": "#EF4444",
  "description": "Practical techniques for catching AI fabrications before users see them.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "üîç",
      "text": "Your AI customer service bot tells a customer their refund will arrive in 3 days. Your policy says 10 days. How do you catch this before the message is sent?"
    },
    "buildup": {
      "visual": "üìã",
      "text": "Hallucination detection approaches: (1) Cross-reference against source documents, (2) Ask a second model to verify, (3) Check factual claims against a knowledge base, (4) Use NLI (Natural Language Inference) models."
    },
    "discovery": {
      "visual": "üí°",
      "text": "The most practical technique for RAG apps: compare the AI's response against the retrieved documents. If the response contains claims not supported by the sources, flag it as a potential hallucination."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "No detection method is perfect. You're using one imperfect system to check another. The goal is reducing hallucination frequency, not eliminating it entirely."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "Layer defenses: source grounding + response validation + user feedback signals. Each layer catches different types of hallucinations. Together, they catch most."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "You can't prevent all hallucinations. But you can catch most of them before users see them."
    }
  },
  "quiz": {
    "question": "What is the most practical hallucination detection technique for RAG applications?",
    "options": [
      "Checking token probabilities",
      "Comparing the AI's response against the retrieved source documents",
      "Asking the user if the answer seems right",
      "Using a larger model"
    ],
    "correct": 1
  }
}
