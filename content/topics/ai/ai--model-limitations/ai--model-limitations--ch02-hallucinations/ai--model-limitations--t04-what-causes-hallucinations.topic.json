{
  "id": "ai--model-limitations--t04-what-causes-hallucinations",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch02-hallucinations",
  "title": "What Causes Hallucinations",
  "emoji": "ğŸš§",
  "color": "#EF4444",
  "description": "Why models make things up and the mechanisms behind it.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ğŸŒ€",
      "text": "You ask for a bibliography. The model generates titles that look real, authors that exist, and journals that publish similar work â€” but none of the papers are real."
    },
    "buildup": {
      "visual": "ğŸ”",
      "text": "Hallucinations happen because the model generates the most probable continuation, not the most truthful. If plausible-sounding text requires inventing a fact, the model invents it."
    },
    "discovery": {
      "visual": "ğŸ’¡",
      "text": "Root causes: (1) Training data gaps â€” the model extrapolates into unknown territory. (2) Distributional patterns â€” 'Dr. Smith published in Nature' is a common pattern, so the model generates instances of it. (3) No truth verification â€” the model can't check its own output."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Hallucinations aren't bugs â€” they're features of how the model works. The same mechanism that lets the model write creative fiction also lets it fabricate academic papers."
    },
    "climax": {
      "visual": "ğŸ",
      "text": "You can reduce hallucinations (RAG, constrained generation, fact-checking) but you can't eliminate them entirely. Design systems that assume hallucinations will happen."
    },
    "punchline": {
      "visual": "ğŸ¬",
      "text": "The model doesn't lie â€” it doesn't know what truth is. It generates probable text. Sometimes probable isn't true."
    }
  },
  "quiz": {
    "question": "What is the fundamental cause of LLM hallucinations?",
    "options": [
      "Intentional deception by the model",
      "The model generates the most probable text continuation, which may not be factually true",
      "Hardware failures",
      "Insufficient training data"
    ],
    "correct": 1
  }
}
