{
  "id": "ai--model-limitations--t08-logical-reasoning",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
  "title": "Logical Reasoning",
  "emoji": "üöß",
  "color": "#EF4444",
  "description": "Where model reasoning breaks down.",
  "difficulty": "Advanced",
  "published": true,
  "story": {
    "hook": {
      "visual": "üîó",
      "text": "'All cats are animals. Some animals are pets. Therefore, all cats are pets.' The model agrees. The logic is invalid ‚Äî but the conclusion happens to be true for cats, so the model doesn't catch the error."
    },
    "buildup": {
      "visual": "üß©",
      "text": "LLMs struggle with formal logic: they confuse correlation with causation, miss logical negations, and accept invalid syllogisms when the conclusion sounds plausible."
    },
    "discovery": {
      "visual": "üí°",
      "text": "The model reasons by pattern: if the conclusion sounds like something that's usually true, it agrees. It doesn't actually trace the logical chain. Change 'cats' to 'aardvarks' and the error becomes obvious to the model too."
    },
    "twist": {
      "visual": "‚ö°",
      "text": "This makes LLMs particularly dangerous for legal, medical, or financial reasoning where the chain of logic matters as much as the conclusion. A correct-sounding answer with faulty logic is still wrong."
    },
    "climax": {
      "visual": "üèÅ",
      "text": "For logic-critical applications: validate reasoning chains, not just conclusions. Use structured prompting that forces explicit logical steps. Or use traditional logic engines for formal reasoning."
    },
    "punchline": {
      "visual": "üé¨",
      "text": "The model pattern-matches its way to answers. When the pattern fails, the logic fails. Check the chain, not just the conclusion."
    }
  },
  "quiz": {
    "question": "Why might an LLM accept an invalid logical argument?",
    "options": [
      "It always checks formal logic rigorously",
      "If the conclusion sounds plausible based on training patterns, it agrees regardless of logical validity",
      "LLMs don't process logical arguments",
      "Invalid logic is always caught"
    ],
    "correct": 1
  }
}
