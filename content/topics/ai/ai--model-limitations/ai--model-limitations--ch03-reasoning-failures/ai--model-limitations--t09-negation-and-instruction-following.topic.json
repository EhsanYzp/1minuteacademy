{
  "id": "ai--model-limitations--t09-negation-and-instruction-following",
  "version": 1,
  "subject": "AI",
  "subcategory": "Model Limitations",
  "course_id": "ai--model-limitations",
  "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
  "title": "Negation Problems",
  "emoji": "ðŸš§",
  "color": "#EF4444",
  "description": "Why 'don't do X' often results in the model doing X.",
  "difficulty": "Intermediate",
  "published": true,
  "story": {
    "hook": {
      "visual": "ðŸš«",
      "text": "Prompt: 'Do NOT mention competitor products.' Response: 'While I won't mention competitor products like ProductX and ProductY...' â€” it mentioned them while saying it wouldn't."
    },
    "buildup": {
      "visual": "ðŸ“‹",
      "text": "Negation is hard for LLMs: 'don't mention X' puts X in the model's attention. The model is now thinking about X and is more likely to generate text involving X."
    },
    "discovery": {
      "visual": "ðŸ’¡",
      "text": "Reframe negatively stated instructions as positive ones: instead of 'Don't mention competitors,' say 'Only discuss our own products.' Tell the model what to do, not what to avoid."
    },
    "twist": {
      "visual": "âš¡",
      "text": "Long lists of 'don'ts' are the worst: 'Don't mention pricing, competitors, legal issues, or internal processes.' Each item primes the model to think about exactly those topics."
    },
    "climax": {
      "visual": "ðŸ",
      "text": "Positive framing > negative framing. 'Talk only about [topic]' works better than 'Don't talk about [everything else].' Constrain the model's scope rather than listing exceptions."
    },
    "punchline": {
      "visual": "ðŸŽ¬",
      "text": "Don't tell the model what to avoid. Tell it what to do. The brain â€” and the model â€” can't unhear a suggestion."
    }
  },
  "quiz": {
    "question": "Why does 'Don't mention X' often cause the model to mention X?",
    "options": [
      "The model intentionally disobeys",
      "Mentioning X in the prompt puts it in the model's attention, making it more likely to generate text about X",
      "Negation instructions are always ignored",
      "This only happens with small models"
    ],
    "correct": 1
  }
}
