{
  "id": "critical-thinking--evidence-and-truth--t-citizen-science-and-crowdsourced-evidence",
  "version": 1,
  "subject": "Critical Thinking",
  "subcategory": "Evidence & Truth: How We Know What's Real",
  "course_id": "critical-thinking--evidence-and-truth",
  "chapter_id": "critical-thinking--evidence-and-truth--ch06-evidence-in-practice",
  "title": "Citizen Science and Crowdsourced Evidence",
  "emoji": "ğŸ”¬",
  "color": "#27AE60",
  "description": "A 60-second lesson on Citizen Science and Crowdsourced Evidence.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Amateur birdwatchers discovered a New Zealand storm petrel thought extinct for 150 years.",
      "visual": "ğŸ¦"
    },
    "buildup": {
      "text": "Citizen science enlists ordinary people to collect data. Millions of observations create datasets no lab could match.",
      "visual": "ğŸ”¬"
    },
    "discovery": {
      "text": "Projects like eBird & Galaxy Zoo produce real papers. Quantity of observers compensates for individual imprecision.",
      "visual": "ğŸ’¡"
    },
    "twist": {
      "text": "But crowdsourced data has biases: people observe near roads, in daylight, in wealthy countries.",
      "visual": "âš ï¸"
    },
    "climax": {
      "text": "Statistical corrections for observer bias turn messy crowd data into rigorous evidence.",
      "visual": "ğŸ“Š"
    },
    "punchline": {
      "text": "A million amateurs can outperform one expert.",
      "visual": "ğŸ‘¥"
    }
  },
  "quiz": {
    "question": "What is a key challenge of citizen science data?",
    "options": [
      "It's always more accurate than professional data",
      "Observer biases like location and timing skew results",
      "Amateurs cannot identify species"
    ],
    "correct": 1
  }
}
