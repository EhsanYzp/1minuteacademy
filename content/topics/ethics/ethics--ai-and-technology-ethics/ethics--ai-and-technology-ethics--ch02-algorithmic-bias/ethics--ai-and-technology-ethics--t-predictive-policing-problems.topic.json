{
  "id": "ethics--ai-and-technology-ethics--t-predictive-policing-problems",
  "version": 1,
  "subject": "Ethics",
  "subcategory": "AI & Technology Ethics",
  "course_id": "ethics--ai-and-technology-ethics",
  "chapter_id": "ethics--ai-and-technology-ethics--ch02-algorithmic-bias",
  "title": "Predictive Policing Problems",
  "emoji": "ğŸ¤–",
  "color": "#0891B2",
  "description": "Learn Predictive Policing Problems in one minute.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "Police sent extra patrols to a neighborhood. More arrests happened. The AI said: send even more.",
      "visual": "ğŸš¨"
    },
    "buildup": {
      "text": "Predictive policing uses AI to forecast where crime will occur, based on historical arrest data.",
      "visual": "ğŸ“"
    },
    "discovery": {
      "text": "But arrest data reflects where police already patrol, not where crime actually happens most.",
      "visual": "ğŸ—ºï¸"
    },
    "twist": {
      "text": "Over-policed communities get more data points, which sends more policeâ€”a self-fulfilling prophecy.",
      "visual": "ğŸ”"
    },
    "climax": {
      "text": "The LAPD quietly abandoned its predictive policing program in 2020 after audits found deep racial bias.",
      "visual": "ğŸ“‹"
    },
    "punchline": {
      "text": "The algorithm didn't predict crime. It predicted policing.",
      "visual": "ğŸ”®"
    }
  },
  "quiz": {
    "question": "What is the fundamental flaw in predictive policing algorithms?",
    "options": [
      "They use data from too few cities",
      "They require too much computing power",
      "They rely on biased historical arrest data, creating feedback loops"
    ],
    "correct": 2
  }
}
