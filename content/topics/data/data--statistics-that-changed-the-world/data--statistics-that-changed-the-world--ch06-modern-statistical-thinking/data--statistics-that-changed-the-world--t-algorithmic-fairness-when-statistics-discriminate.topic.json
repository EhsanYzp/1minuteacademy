{
  "id": "data--statistics-that-changed-the-world--t-algorithmic-fairness-when-statistics-discriminate",
  "version": 1,
  "subject": "Data",
  "subcategory": "Statistics That Changed the World",
  "course_id": "data--statistics-that-changed-the-world",
  "chapter_id": "data--statistics-that-changed-the-world--ch06-modern-statistical-thinking",
  "title": "Algorithmic Fairness: When Statistics Discriminate",
  "emoji": "ğŸ²",
  "color": "#8E44AD",
  "description": "A fast breakdown of Algorithmic Fairness: When Statistics Discriminate for builders.",
  "is_free": false,
  "published": true,
  "story": {
    "hook": {
      "text": "An AI hiring tool at Amazon learned to reject rÃ©sumÃ©s containing the word 'women's.' It was trained on past data.",
      "visual": "ğŸ¤–"
    },
    "buildup": {
      "text": "Statistical models trained on biased historical data reproduce and amplify those biases.",
      "visual": "ğŸ“Š"
    },
    "discovery": {
      "text": "Algorithmic fairness is a new field trying to define mathematically what 'fair' means in data-driven decisions.",
      "visual": "âš–ï¸"
    },
    "twist": {
      "text": "There are multiple incompatible mathematical definitions of fairness. You can't satisfy them all simultaneously.",
      "visual": "ğŸ˜°"
    },
    "climax": {
      "text": "As algorithms make more decisions about our lives, statistical fairness becomes a civil rights issue.",
      "visual": "âœŠ"
    },
    "punchline": {
      "text": "Algorithms aren't neutral. They inherit our biases.",
      "visual": "ğŸª"
    }
  },
  "quiz": {
    "question": "Why did Amazon's AI hiring tool discriminate?",
    "options": [
      "It was intentionally programmed to",
      "It learned biases from historical hiring data",
      "It only analyzed names"
    ],
    "correct": 1
  }
}
