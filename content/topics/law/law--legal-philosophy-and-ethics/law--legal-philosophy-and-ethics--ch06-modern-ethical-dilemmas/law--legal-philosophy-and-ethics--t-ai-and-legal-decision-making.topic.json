{
  "id": "law--legal-philosophy-and-ethics--t-ai-and-legal-decision-making",
  "version": 1,
  "subject": "Law",
  "subcategory": "Legal Philosophy & Ethics",
  "course_id": "law--legal-philosophy-and-ethics",
  "chapter_id": "law--legal-philosophy-and-ethics--ch06-modern-ethical-dilemmas",
  "title": "AI and Legal Decision-Making",
  "emoji": "üß†",
  "color": "#6C3483",
  "description": "A 1-minute de-risking session on AI and Legal Decision-Making.",
  "is_free": true,
  "published": true,
  "story": {
    "hook": {
      "text": "An algorithm recommends a longer sentence for a defendant. The judge follows it. Who made the decision?",
      "visual": "ü§ñ"
    },
    "buildup": {
      "text": "AI tools are used in bail decisions, sentencing, and legal research. They promise efficiency and consistency.",
      "visual": "üìä"
    },
    "discovery": {
      "text": "COMPAS, a risk assessment tool, predicts recidivism. Studies found it was biased against Black defendants.",
      "visual": "‚ö†Ô∏è"
    },
    "twist": {
      "text": "AI can't explain its reasoning like humans can. Due process may require defendants understand the basis for decisions.",
      "visual": "üîç"
    },
    "climax": {
      "text": "Courts must decide: is AI a tool that aids judgment, or a replacement that undermines human accountability?",
      "visual": "‚öñÔ∏è"
    },
    "punchline": {
      "text": "Machines can calculate. Only humans can be held accountable.",
      "visual": "üßë‚Äç‚öñÔ∏è"
    }
  },
  "quiz": {
    "question": "What problem was found with the COMPAS recidivism tool?",
    "options": [
      "It was too slow",
      "It was biased against Black defendants",
      "It was too expensive"
    ],
    "correct": 1
  }
}
