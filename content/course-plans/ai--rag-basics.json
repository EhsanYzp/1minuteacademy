{
  "categoryId": "ai",
  "subject": "AI",
  "courseId": "ai--rag-basics",
  "courseTitle": "RAG Basics",
  "emoji": "ğŸ”",
  "color": "#EF4444",
  "requireAuthoredStory": true,
  "chapters": [
    {
      "id": "ai--rag-basics--ch01-why-rag",
      "title": "Why RAG?",
      "position": 1
    },
    {
      "id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "Embeddings & Vectors",
      "position": 2
    },
    {
      "id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Chunking & Indexing",
      "position": 3
    },
    {
      "id": "ai--rag-basics--ch04-retrieval",
      "title": "Retrieval",
      "position": 4
    },
    {
      "id": "ai--rag-basics--ch05-generation",
      "title": "Generation",
      "position": 5
    },
    {
      "id": "ai--rag-basics--ch06-evaluation-and-ops",
      "title": "Evaluation & Ops",
      "position": 6
    }
  ],
  "topics": [
    {
      "id": "ai--rag-basics--t01-what-is-rag",
      "chapter_id": "ai--rag-basics--ch01-why-rag",
      "title": "What Is RAG?",
      "description": "The pattern that grounds LLMs in your actual data.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ“š", "text": "Your chatbot confidently tells a customer your return policy is 30 days. It's actually 14. The model was guessing â€” it doesn't know your policies." },
        "buildup": { "visual": "ğŸ”", "text": "Retrieval-Augmented Generation (RAG) fixes this by fetching relevant documents before the model answers. The model reads your real data, then responds based on what it found." },
        "discovery": { "visual": "ğŸ’¡", "text": "Step 1: User asks a question. Step 2: Search your knowledge base for relevant passages. Step 3: Inject those passages into the prompt. Step 4: The model answers using the retrieved context." },
        "twist": { "visual": "âš¡", "text": "RAG doesn't change the model's weights â€” it changes the model's input. Same model, but now it has the right information at generation time." },
        "climax": { "visual": "ğŸ", "text": "RAG is the most practical way to make LLMs answer questions about your data without fine-tuning. It's fast to build and easy to update." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't teach the model your data. Show it your data at query time." }
      },
      "quiz": {
        "question": "What does RAG do differently from a standalone LLM?",
        "options": [
          "It retrains the model on new data",
          "It retrieves relevant documents and includes them in the prompt",
          "It reduces the model's parameter count",
          "It replaces the LLM with a search engine"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t02-rag-vs-fine-tuning",
      "chapter_id": "ai--rag-basics--ch01-why-rag",
      "title": "RAG vs Fine-Tuning",
      "description": "When to retrieve and when to retrain.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "âš–ï¸", "text": "Your boss says 'make the model know our product catalog.' Do you fine-tune or build RAG? The answer depends on what 'know' means." },
        "buildup": { "visual": "ğŸ“‹", "text": "Fine-tuning bakes knowledge into model weights â€” permanent but expensive to update. RAG fetches knowledge at query time â€” always current but dependent on retrieval quality." },
        "discovery": { "visual": "ğŸ’¡", "text": "If your data changes weekly (inventory, policies, prices), RAG wins â€” update the index, not the model. If you need a specific writing style or behavior, fine-tuning wins." },
        "twist": { "visual": "âš¡", "text": "They're not mutually exclusive. The best systems use both: fine-tuning for behavior and style, RAG for up-to-date factual grounding." },
        "climax": { "visual": "ğŸ", "text": "Start with RAG â€” it's faster to build and easier to debug. Add fine-tuning only if RAG alone doesn't get the quality you need." },
        "punchline": { "visual": "ğŸ¬", "text": "RAG for facts that change. Fine-tuning for behavior that stays. Most projects only need the first." }
      },
      "quiz": {
        "question": "When is RAG preferred over fine-tuning?",
        "options": [
          "When you need to change the model's writing style",
          "When the knowledge base changes frequently",
          "When you have unlimited compute budget",
          "When you don't have any documents"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t03-the-rag-pipeline",
      "chapter_id": "ai--rag-basics--ch01-why-rag",
      "title": "The RAG Pipeline",
      "description": "End-to-end flow from user question to grounded answer.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "RAG sounds simple until you build it. There are five moving parts, and each one can break independently." },
        "buildup": { "visual": "ğŸ§©", "text": "The pipeline: (1) Ingest documents, (2) Chunk them, (3) Embed chunks into vectors, (4) At query time, embed the question and search for similar chunks, (5) Feed retrieved chunks + question to the LLM." },
        "discovery": { "visual": "ğŸ’¡", "text": "Each step has its own failure mode: bad chunks â†’ irrelevant retrieval. Bad embeddings â†’ semantic mismatches. Bad prompting â†’ the model ignores the context." },
        "twist": { "visual": "âš¡", "text": "Most RAG failures aren't in the LLM â€” they're in retrieval. If the right document isn't retrieved, the model can't use it. Garbage in, hallucination out." },
        "climax": { "visual": "ğŸ", "text": "Debug RAG from retrieval backward: first check if the right chunks are being retrieved. Only then look at the LLM's behavior." },
        "punchline": { "visual": "ğŸ¬", "text": "Five steps, five failure points. Debug from retrieval, not from the answer." }
      },
      "quiz": {
        "question": "Where do most RAG failures originate?",
        "options": [
          "In the LLM's reasoning ability",
          "In the retrieval step â€” the right documents aren't found",
          "In the user's question",
          "In the embedding model's speed"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t04-when-rag-fails",
      "chapter_id": "ai--rag-basics--ch01-why-rag",
      "title": "When RAG Fails",
      "description": "Common RAG failure modes and how to spot them.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ’¥", "text": "Your RAG system retrieves five chunks, but none of them answer the user's question. The model reads them anyway and confidently generates a wrong answer." },
        "buildup": { "visual": "ğŸ”", "text": "Failure mode 1: irrelevant retrieval (the query doesn't match good chunks). Failure mode 2: missing documents (the answer isn't in your corpus at all). Failure mode 3: lost in context (the right chunk is retrieved but the model ignores it)." },
        "discovery": { "visual": "ğŸ’¡", "text": "Add a relevance threshold: if no chunk scores above a minimum, say 'I don't have enough information to answer that' instead of hallucinating from bad context." },
        "twist": { "visual": "âš¡", "text": "Users blame the AI when RAG fails, but the root cause is often the knowledge base â€” outdated docs, missing pages, or inconsistent formatting that chunks poorly." },
        "climax": { "visual": "ğŸ", "text": "Log every retrieval: query, retrieved chunks, relevance scores, and final answer. Without these logs, debugging RAG is guessing." },
        "punchline": { "visual": "ğŸ¬", "text": "RAG doesn't eliminate hallucinations â€” it trades 'no context' hallucinations for 'bad context' ones. Plan for both." }
      },
      "quiz": {
        "question": "What should a RAG system do when no relevant documents are found?",
        "options": [
          "Generate an answer from the model's training data",
          "Admit it doesn't have enough information to answer",
          "Return a random document",
          "Increase the temperature to generate something creative"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t05-what-are-embeddings",
      "chapter_id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "What Are Embeddings?",
      "description": "Turning text into numbers that capture meaning.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "You need a computer to understand that 'how to return a product' and 'refund process' mean the same thing. Keywords won't cut it â€” the words are completely different." },
        "buildup": { "visual": "ğŸ—ºï¸", "text": "An embedding model converts text into a vector â€” a list of hundreds of numbers. Similar meanings produce similar vectors, regardless of the exact words used." },
        "discovery": { "visual": "ğŸ’¡", "text": "'How to return a product' and 'refund process' end up as nearby points in vector space. 'Best pizza recipe' ends up far away. The model learned semantic similarity." },
        "twist": { "visual": "âš¡", "text": "Embedding models are different from generation models. They don't produce text â€” they produce coordinates. Small, specialized embedding models often beat large LLMs at retrieval." },
        "climax": { "visual": "ğŸ", "text": "Embeddings are the foundation of RAG search. Get them right and retrieval works well. Get them wrong and no amount of prompt engineering saves the pipeline." },
        "punchline": { "visual": "ğŸ¬", "text": "Same meaning, similar numbers. That's the whole idea behind semantic search." }
      },
      "quiz": {
        "question": "How do embeddings enable semantic search?",
        "options": [
          "By matching exact keywords between query and document",
          "By converting text to numerical vectors where similar meanings are nearby",
          "By ranking documents by word count",
          "By translating text into multiple languages"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t06-embedding-models",
      "chapter_id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "Embedding Models",
      "description": "Which embedding model to use and why it matters.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸª", "text": "OpenAI's text-embedding-3-small, Cohere's embed-v3, BGE, E5 â€” dozens of embedding models exist. Does the choice actually matter?" },
        "buildup": { "visual": "ğŸ“Š", "text": "Yes. Different embedding models encode meaning differently. Some excel at short queries, others at long documents. Some handle code well, others focus on natural language." },
        "discovery": { "visual": "ğŸ’¡", "text": "The MTEB leaderboard ranks embedding models on retrieval benchmarks. But benchmark scores don't always match your domain. Test a few on your actual data." },
        "twist": { "visual": "âš¡", "text": "You can't mix embedding models. Once your documents are indexed with model A, queries must use model A too. Switching models means re-embedding everything." },
        "climax": { "visual": "ğŸ", "text": "Pick an embedding model early and test it on real queries before indexing millions of documents. The switch cost is high once you're committed." },
        "punchline": { "visual": "ğŸ¬", "text": "The embedding model is a one-way door. Choose wisely, test early." }
      },
      "quiz": {
        "question": "Why can't you mix different embedding models in a RAG system?",
        "options": [
          "It's too expensive",
          "Different models produce incompatible vector spaces",
          "APIs don't allow multiple models",
          "It would make queries too fast"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t07-vector-databases",
      "chapter_id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "Vector Databases",
      "description": "Where embeddings live and how you search them fast.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ—„ï¸", "text": "You have 10 million embedded chunks. A user asks a question. You need to find the 5 most similar chunks in under 100ms. Regular databases can't do this." },
        "buildup": { "visual": "âš¡", "text": "Vector databases (Pinecone, Weaviate, Qdrant, pgvector) are built for similarity search. They index vectors using algorithms like HNSW that find approximate nearest neighbors in milliseconds." },
        "discovery": { "visual": "ğŸ’¡", "text": "They don't find the absolute closest vectors (that would be slow). They find approximately closest â€” usually 95%+ accuracy at 100x the speed of brute-force search." },
        "twist": { "visual": "âš¡", "text": "You might not need a dedicated vector DB. For small datasets (under 100K chunks), pgvector in your existing Postgres database works fine and avoids another service to manage." },
        "climax": { "visual": "ğŸ", "text": "Match the tool to your scale: pgvector for small to medium, dedicated vector DB for millions of vectors, and managed services when you don't want to run infrastructure." },
        "punchline": { "visual": "ğŸ¬", "text": "Vectors need a home. Pick one that matches your scale â€” not the one with the best landing page." }
      },
      "quiz": {
        "question": "When is pgvector sufficient instead of a dedicated vector database?",
        "options": [
          "Never â€” always use a dedicated vector DB",
          "For small to medium datasets under ~100K chunks",
          "Only for non-AI applications",
          "When you don't have embeddings"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t08-similarity-metrics",
      "chapter_id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "Similarity Metrics",
      "description": "Cosine similarity, dot product, and how to measure closeness.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "Two vectors are 'close' â€” but close how? Euclidean distance and cosine similarity can give different rankings for the same vectors." },
        "buildup": { "visual": "ğŸ”¢", "text": "Cosine similarity measures the angle between vectors (ignoring magnitude). Dot product measures both angle and magnitude. Euclidean distance measures straight-line distance." },
        "discovery": { "visual": "ğŸ’¡", "text": "For normalized embeddings (most embedding APIs return these), cosine similarity and dot product give identical rankings. Cosine is the default choice for text similarity." },
        "twist": { "visual": "âš¡", "text": "If your embeddings aren't normalized, cosine and dot product diverge. A longer vector (more 'confident' embedding) gets boosted by dot product but not by cosine. Know your embeddings." },
        "climax": { "visual": "ğŸ", "text": "Use cosine similarity unless you have a specific reason not to. It's robust, well-understood, and what most tutorials and vector databases default to." },
        "punchline": { "visual": "ğŸ¬", "text": "Cosine similarity: direction matters, magnitude doesn't. For text search, that's usually what you want." }
      },
      "quiz": {
        "question": "What does cosine similarity measure?",
        "options": [
          "The Euclidean distance between two vectors",
          "The angle between two vectors, ignoring their magnitude",
          "The total number of dimensions in a vector",
          "The average value of all vector components"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t09-why-chunking-matters",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Why Chunking Matters",
      "description": "Breaking documents into retrieval-friendly pieces.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âœ‚ï¸", "text": "You embed an entire 50-page PDF as one vector. The user asks about a paragraph on page 37. The single vector averages the whole document and misses the specific paragraph entirely." },
        "buildup": { "visual": "ğŸ“„", "text": "Chunking splits documents into smaller pieces â€” paragraphs, sections, or fixed-size blocks â€” each with its own embedding. Retrieval then finds the right piece, not just the right document." },
        "discovery": { "visual": "ğŸ’¡", "text": "Good chunk size is a goldilocks problem: too small and you lose context. Too large and the embedding becomes vague. Typical sweet spot: 200â€“500 tokens per chunk." },
        "twist": { "visual": "âš¡", "text": "Chunking strategy has more impact on RAG quality than the choice of embedding model or vector database. It's the most underrated component in the pipeline." },
        "climax": { "visual": "ğŸ", "text": "Test different chunk sizes on your actual data and queries. Measure retrieval precision before touching anything else." },
        "punchline": { "visual": "ğŸ¬", "text": "Bad chunks â†’ bad retrieval â†’ bad answers. Chunking is where RAG quality starts." }
      },
      "quiz": {
        "question": "Why is chunking important in RAG?",
        "options": [
          "It reduces the total number of documents",
          "It creates focused pieces that can be individually matched to specific queries",
          "It compresses the documents to save storage",
          "It is only needed for very short documents"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t10-chunking-strategies",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Chunking Strategies",
      "description": "Fixed-size, semantic, and recursive approaches compared.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”§", "text": "You split every document at exactly 500 characters. A sentence gets cut in half mid-word. The embedding is nonsense. There's a better way." },
        "buildup": { "visual": "ğŸ“", "text": "Fixed-size chunking is fast but dumb â€” it ignores sentence boundaries. Recursive chunking splits by paragraphs first, then sentences, then characters as fallback. Semantic chunking uses embedding similarity to detect natural topic breaks." },
        "discovery": { "visual": "ğŸ’¡", "text": "Recursive chunking (what LangChain uses by default) handles most document types well: it preserves paragraph structure while keeping chunks under a size limit." },
        "twist": { "visual": "âš¡", "text": "Markdown, HTML, and code need different splitters. Splitting Python code at arbitrary character counts breaks function definitions. Use language-aware chunkers for structured content." },
        "climax": { "visual": "ğŸ", "text": "Match your chunking strategy to your content type: recursive for prose, header-based for markdown, AST-based for code. One size doesn't fit all." },
        "punchline": { "visual": "ğŸ¬", "text": "Respect the structure of your content. Smart splits beat arbitrary cuts every time." }
      },
      "quiz": {
        "question": "Why should different content types use different chunking strategies?",
        "options": [
          "All content should be chunked the same way",
          "Different structures (prose, code, markdown) break meaningfully at different boundaries",
          "Only code needs special chunking",
          "Chunking strategy doesn't affect retrieval quality"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t11-overlapping-chunks",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Overlapping Chunks",
      "description": "Use overlap to preserve context across chunk boundaries.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”—", "text": "A critical sentence spans two chunks. Chunk A has the beginning, Chunk B has the ending. Neither chunk alone makes sense. You've created a retrieval blind spot." },
        "buildup": { "visual": "ğŸ“", "text": "Overlap means each chunk includes some tokens from the previous chunk. With 50-token overlap on 300-token chunks, boundary sentences appear in both adjacent chunks." },
        "discovery": { "visual": "ğŸ’¡", "text": "Overlap costs extra storage and compute (more chunks to index and search), but it dramatically reduces information loss at boundaries. 10â€“20% overlap is a common starting point." },
        "twist": { "visual": "âš¡", "text": "Too much overlap creates near-duplicate chunks that clutter retrieval results. If 80% of two chunks are identical, you're wasting retrieval slots on redundant information." },
        "climax": { "visual": "ğŸ", "text": "Use enough overlap to capture boundary context (50â€“100 tokens) but not so much that chunks become redundant. Deduplicate results after retrieval if needed." },
        "punchline": { "visual": "ğŸ¬", "text": "A little overlap prevents lost context at boundaries. A lot of overlap wastes everything else." }
      },
      "quiz": {
        "question": "What is the purpose of chunk overlap in RAG?",
        "options": [
          "To increase the total document count",
          "To preserve context that spans chunk boundaries",
          "To reduce the number of embeddings needed",
          "To speed up indexing"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t12-metadata-enrichment",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Metadata Enrichment",
      "description": "Attach metadata to chunks to improve filtering and retrieval.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ·ï¸", "text": "A user asks about 'return policy for electronics.' Your vector search finds similar text in 'return policy for groceries.' Semantic similarity doesn't know about product categories." },
        "buildup": { "visual": "ğŸ“‹", "text": "Metadata enrichment tags each chunk with structured attributes: source document, section title, category, date, product type. You can then filter by metadata before or after vector search." },
        "discovery": { "visual": "ğŸ’¡", "text": "Hybrid retrieval: first filter by metadata ('category = electronics'), then run vector search within that subset. This combines precision of structured queries with flexibility of semantic search." },
        "twist": { "visual": "âš¡", "text": "Over-filtering can exclude relevant results. If no chunk matches the filter, the system returns nothing. Always have a fallback: broaden filters or fall back to pure semantic search." },
        "climax": { "visual": "ğŸ", "text": "Every chunk should carry at least: source URL, document title, section header, and creation date. These cost nothing to store and unlock powerful filtering." },
        "punchline": { "visual": "ğŸ¬", "text": "Vectors find similar meaning. Metadata finds the right context. Use both." }
      },
      "quiz": {
        "question": "How does metadata improve RAG retrieval?",
        "options": [
          "It replaces vector search entirely",
          "It enables filtering by structured attributes before or alongside semantic search",
          "It reduces the size of embeddings",
          "It is only useful for sorting results by date"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t13-semantic-search",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Semantic Search",
      "description": "Finding documents by meaning, not just keywords.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "The user types 'how do I send back a product I bought?' Your keyword search for 'send back' returns nothing. Your knowledge base calls it 'returns.' Semantic search finds it anyway." },
        "buildup": { "visual": "ğŸ§ ", "text": "Semantic search embeds the query into the same vector space as your chunks and finds the nearest neighbors. It matches meaning, not exact words." },
        "discovery": { "visual": "ğŸ’¡", "text": "This is why RAG works for real users: people describe things in their own words, not in your documentation's terminology. Semantic search bridges that vocabulary gap." },
        "twist": { "visual": "âš¡", "text": "Semantic search can match too broadly. 'What's your refund policy?' might retrieve 'What's your privacy policy?' because the sentence structure is similar. Relevance scoring helps but isn't perfect." },
        "climax": { "visual": "ğŸ", "text": "Semantic search is your primary retrieval method in RAG. Pair it with keyword search (hybrid retrieval) for cases where exact terminology matters." },
        "punchline": { "visual": "ğŸ¬", "text": "People ask in their words. Your docs answer in yours. Semantic search translates between them." }
      },
      "quiz": {
        "question": "What advantage does semantic search have over keyword search?",
        "options": [
          "It's faster to compute",
          "It matches meaning rather than exact words, bridging vocabulary gaps",
          "It doesn't require embeddings",
          "It always returns more results"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t14-hybrid-search",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Hybrid Search",
      "description": "Combine keyword and semantic search for the best of both.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”€", "text": "Semantic search finds conceptually related chunks. But when the user searches for 'error code E-4021,' you need exact keyword matching. One method alone isn't enough." },
        "buildup": { "visual": "âš–ï¸", "text": "Hybrid search runs both keyword (BM25) and semantic (vector) search in parallel, then combines the results. You get exact matches when they exist and fuzzy matches when they don't." },
        "discovery": { "visual": "ğŸ’¡", "text": "Reciprocal Rank Fusion (RRF) is a common merging strategy: it weights results by their rank in each search and produces a unified ranking. No magic tuning needed." },
        "twist": { "visual": "âš¡", "text": "The weight between keyword and semantic matters. For technical docs with specific terms, lean toward keywords. For conversational queries, lean semantic. One ratio doesn't fit all use cases." },
        "climax": { "visual": "ğŸ", "text": "Most production RAG systems use hybrid search. It's more robust than either method alone and only slightly more complex to implement." },
        "punchline": { "visual": "ğŸ¬", "text": "Keywords for precision. Semantics for recall. Hybrid for both." }
      },
      "quiz": {
        "question": "What does hybrid search combine?",
        "options": [
          "Two different LLMs",
          "Keyword search (BM25) and semantic vector search",
          "Two different embedding models",
          "Search and generation in one step"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t15-reranking",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Reranking",
      "description": "Use a second model to refine the order of retrieved results.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ†", "text": "Your vector search returns 20 chunks. The best answer is chunk #14. The LLM only sees the top 5. The answer is retrieved but never used." },
        "buildup": { "visual": "ğŸ”„", "text": "A reranker takes the initial retrieval results and re-scores them using a more powerful model that reads both the query and each chunk together." },
        "discovery": { "visual": "ğŸ’¡", "text": "Cross-encoder rerankers (like Cohere Rerank or BGE-reranker) compare query-chunk pairs with much higher accuracy than embedding similarity. They're slower but much more precise." },
        "twist": { "visual": "âš¡", "text": "Reranking only works on the initial result set. If the right chunk wasn't retrieved at all, reranking can't save you. Retrieve broadly (top 50), then rerank to top 5." },
        "climax": { "visual": "ğŸ", "text": "The pattern: fast initial retrieval (top 50 by vector similarity) â†’ slow reranking (top 5 by cross-encoder) â†’ inject top 5 into the prompt. This is production-grade RAG." },
        "punchline": { "visual": "ğŸ¬", "text": "Retrieve wide, rerank tight. Two stages beat one." }
      },
      "quiz": {
        "question": "Why should you retrieve more results than you need before reranking?",
        "options": [
          "To use more tokens",
          "To ensure the relevant chunk is in the initial set so the reranker can find it",
          "To slow down the system",
          "Reranking only works with exactly 50 results"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t16-query-transformation",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Query Transformation",
      "description": "Rewrite the user's question to improve retrieval.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "The user asks: 'Why isn't my thing working?' Your knowledge base has articles about 'troubleshooting widget connectivity errors.' The semantic gap is too wide for raw search." },
        "buildup": { "visual": "âœï¸", "text": "Query transformation uses an LLM to rewrite the user's question into something more searchable: 'widget not working â†’ troubleshooting steps for widget connectivity issues.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "HyDE (Hypothetical Document Embeddings) is an advanced variant: generate a hypothetical answer, embed that, and search for real documents similar to the hypothetical answer." },
        "twist": { "visual": "âš¡", "text": "Query transformation adds an LLM call before retrieval â€” extra latency and cost. Only use it when raw queries consistently fail to retrieve relevant results." },
        "climax": { "visual": "ğŸ", "text": "Start without query transformation. Add it when you see a pattern: users ask in casual language, your docs are formal, and semantic search alone can't bridge the gap." },
        "punchline": { "visual": "ğŸ¬", "text": "If the user's words don't match your docs, rewrite the question â€” not the docs." }
      },
      "quiz": {
        "question": "What is the purpose of query transformation in RAG?",
        "options": [
          "To correct the user's grammar",
          "To rewrite queries into forms that better match the knowledge base",
          "To translate queries into other languages",
          "To reduce the number of tokens in the query"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t17-grounded-generation",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Grounded Generation",
      "description": "Making the model answer from retrieved context, not from memory.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âš“", "text": "You retrieved the perfect chunk. You put it in the prompt. The model ignores it and answers from its own training data. Why?" },
        "buildup": { "visual": "ğŸ“", "text": "Grounded generation requires explicit instructions: 'Answer ONLY based on the provided context. If the context doesn't contain the answer, say you don't know.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Place the context before the question in the prompt. Use clear delimiters: 'Context: [chunks]. Question: [query]. Answer based only on the context above.'" },
        "twist": { "visual": "âš¡", "text": "Even with strong instructions, models sometimes blend retrieved context with training knowledge â€” especially when the context is vague and the model's prior knowledge is strong." },
        "climax": { "visual": "ğŸ", "text": "Add a citation requirement: 'Cite which context passage supports each claim.' This forces the model to trace its answers back to specific chunks, reducing unsupported claims." },
        "punchline": { "visual": "ğŸ¬", "text": "Retrieval gets the data in. The prompt decides whether the model uses it." }
      },
      "quiz": {
        "question": "How do you ensure the LLM uses retrieved context instead of its training data?",
        "options": [
          "Use a larger model",
          "Explicitly instruct the model to answer only from the provided context",
          "Remove the context from the prompt",
          "Increase the temperature"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t18-citing-sources",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Citing Sources",
      "description": "Make the model point to where it found the answer.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "The user asks about your pricing plan. The model says '$49/month.' Is that from your docs or hallucinated? Without citations, there's no way to verify." },
        "buildup": { "visual": "ğŸ“‹", "text": "Include chunk labels in the prompt: '[Source A: pricing.md] Our plans start at $49/month...' Then instruct the model: 'Cite sources by label (e.g., [Source A]) for each claim.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Citations serve two purposes: users can verify the answer, and developers can audit which chunks the model actually used. Both build trust." },
        "twist": { "visual": "âš¡", "text": "Models sometimes cite the wrong source â€” attributing a claim to Source A when it came from Source B. Validate citations programmatically by checking if the cited source actually contains the claim." },
        "climax": { "visual": "ğŸ", "text": "Link citations to the original document. Users click a citation and see the full source. This turns a chatbot answer into a verifiable reference." },
        "punchline": { "visual": "ğŸ¬", "text": "If you can't point to where the answer came from, you can't trust it. Citations make RAG trustworthy." }
      },
      "quiz": {
        "question": "Why should RAG systems include citations in their answers?",
        "options": [
          "To increase the response length",
          "To let users verify claims and developers audit model behavior",
          "To reduce hallucinations entirely",
          "Citations are only needed for academic papers"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t19-context-window-management",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Context Window Management",
      "description": "Fit the right amount of context without overflowing the window.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“¦", "text": "You stuff 15 retrieved chunks into the prompt. The model runs out of space for its answer. Or worse, it ignores chunks 8â€“14 completely." },
        "buildup": { "visual": "ğŸ“", "text": "Budget your context window: system prompt + retrieved chunks + conversation history + the model's response all share the same limit. Reserve enough tokens for the answer." },
        "discovery": { "visual": "ğŸ’¡", "text": "A practical budget: 20% for system prompt and instructions, 50% for retrieved context, 10% for conversation history, 20% reserved for the model's response." },
        "twist": { "visual": "âš¡", "text": "More context isn't always better. After about 5 well-chosen chunks, adding more can actually dilute the signal and reduce answer quality. Quality of retrieval beats quantity." },
        "climax": { "visual": "ğŸ", "text": "Rerank aggressively and include only the top 3â€“5 most relevant chunks. This leaves room for a good answer and keeps the model focused." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't fill the window. Budget it. The model needs room to think and respond." }
      },
      "quiz": {
        "question": "Why shouldn't you fill the entire context window with retrieved chunks?",
        "options": [
          "It's against API terms of service",
          "The model needs room for its response, and too many chunks dilute signal quality",
          "More chunks always improve answer quality",
          "The context window is unlimited"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t20-answer-synthesis",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Answer Synthesis",
      "description": "Combining information from multiple chunks into one coherent answer.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ§©", "text": "The answer requires information from three different documents. Chunk 1 has the price, Chunk 2 has the features, Chunk 3 has the limitations. The model needs to weave them into one response." },
        "buildup": { "visual": "âœï¸", "text": "Answer synthesis is the generation step where the model reads multiple chunks and produces a unified, coherent answer â€” not just regurgitating individual chunks." },
        "discovery": { "visual": "ğŸ’¡", "text": "Instruct the model: 'Synthesize information from all provided sources into a single coherent answer. Do not copy chunks verbatim â€” explain in your own words, citing sources.'" },
        "twist": { "visual": "âš¡", "text": "When chunks contradict each other (outdated policy vs. new policy), the model might pick the wrong one. Add metadata (dates) and instruct: 'Prefer the most recent source when information conflicts.'" },
        "climax": { "visual": "ğŸ", "text": "Good synthesis requires good retrieval and good prompting. The model must understand what to combine, what to prioritize, and when to flag contradictions." },
        "punchline": { "visual": "ğŸ¬", "text": "RAG isn't copy-paste from docs. It's synthesis: reading, combining, and explaining. That's where the real value lives." }
      },
      "quiz": {
        "question": "What should a RAG system do when retrieved chunks contradict each other?",
        "options": [
          "Pick the first chunk found",
          "Prefer the most recent source or flag the contradiction",
          "Ignore all contradicting chunks",
          "Return an error message"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t21-measuring-retrieval-quality",
      "chapter_id": "ai--rag-basics--ch06-evaluation-and-ops",
      "title": "Measuring Retrieval Quality",
      "description": "Metrics that tell you if your retrieval pipeline works.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "Users say answers are wrong but your LLM prompts look perfect. The problem isn't generation â€” it's retrieval. But how do you measure retrieval separately?" },
        "buildup": { "visual": "ğŸ“", "text": "Key metrics: Recall@K (was the correct chunk in the top K results?), Precision@K (what fraction of top K results are relevant?), and MRR (how high was the first relevant result ranked?)." },
        "discovery": { "visual": "ğŸ’¡", "text": "Build a retrieval eval set: 50 questions paired with the chunks that should be retrieved. Run queries, check if the right chunks appear in the top 5. That's your Recall@5." },
        "twist": { "visual": "âš¡", "text": "High recall with low precision means you're retrieving the right chunk plus a lot of noise. The LLM gets confused by irrelevant chunks. Both metrics matter." },
        "climax": { "visual": "ğŸ", "text": "Evaluate retrieval and generation separately. If retrieval quality is low, no amount of prompt engineering fixes the answers. Fix retrieval first." },
        "punchline": { "visual": "ğŸ¬", "text": "You can't improve what you don't measure. Track retrieval quality independently." }
      },
      "quiz": {
        "question": "What does Recall@K measure in RAG evaluation?",
        "options": [
          "The speed of retrieval",
          "Whether the correct chunk appears in the top K retrieved results",
          "The total number of chunks in the database",
          "How fast the LLM generates an answer"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t22-measuring-answer-quality",
      "chapter_id": "ai--rag-basics--ch06-evaluation-and-ops",
      "title": "Measuring Answer Quality",
      "description": "Evaluate the final generated answer, not just the retrieval.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "âœ…", "text": "The retrieval is perfect â€” the right chunk is in the top 3. But the model still gives a wrong answer. Retrieval quality alone doesn't guarantee answer quality." },
        "buildup": { "visual": "ğŸ“‹", "text": "Answer quality metrics: faithfulness (does the answer only use information from the context?), relevance (does it actually address the question?), and completeness (does it cover all necessary details?)." },
        "discovery": { "visual": "ğŸ’¡", "text": "Use an LLM-as-judge: a second model evaluates the answer against the source context and question. 'Is this answer supported by the provided context? Score 1â€“5.' Automate the evaluation loop." },
        "twist": { "visual": "âš¡", "text": "LLM judges have their own biases â€” they tend to rate longer, more detailed answers higher. Calibrate your judge with human-annotated examples." },
        "climax": { "visual": "ğŸ", "text": "Combine automated metrics (LLM-as-judge) with periodic human review. Neither alone is sufficient; together they catch most quality issues." },
        "punchline": { "visual": "ğŸ¬", "text": "Measure the whole pipeline: retrieval and generation. A chain is only as strong as its weakest link." }
      },
      "quiz": {
        "question": "What does 'faithfulness' measure in RAG evaluation?",
        "options": [
          "How fast the answer is generated",
          "Whether the answer only uses information from the retrieved context",
          "How many chunks were retrieved",
          "Whether the user liked the answer"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t23-keeping-the-index-fresh",
      "chapter_id": "ai--rag-basics--ch06-evaluation-and-ops",
      "title": "Keeping the Index Fresh",
      "description": "Update your knowledge base without rebuilding everything.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "Your company updates its pricing page. The RAG system still quotes last month's prices. The knowledge base is stale â€” and users notice." },
        "buildup": { "visual": "ğŸ“‹", "text": "Incremental indexing: track which documents changed, re-chunk and re-embed only those, and update the vector index without rebuilding from scratch." },
        "discovery": { "visual": "ğŸ’¡", "text": "Use document hashes or last-modified timestamps to detect changes. A nightly job checks for updates, re-processes changed docs, and swaps in new chunks." },
        "twist": { "visual": "âš¡", "text": "Deleting old chunks is as important as adding new ones. If an outdated chunk stays in the index alongside the new one, the model might retrieve the stale version." },
        "climax": { "visual": "ğŸ", "text": "Build a refresh pipeline: detect changes â†’ re-chunk â†’ re-embed â†’ upsert â†’ delete old chunks. Run it on a schedule and monitor for drift." },
        "punchline": { "visual": "ğŸ¬", "text": "A RAG system is only as current as its index. Stale data = stale answers." }
      },
      "quiz": {
        "question": "What must happen when a document is updated in a RAG system?",
        "options": [
          "Nothing â€” the model will figure it out",
          "Re-chunk, re-embed, upsert new chunks, and delete old ones",
          "Retrain the LLM from scratch",
          "Delete the entire vector database and rebuild"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t24-rag-in-production",
      "chapter_id": "ai--rag-basics--ch06-evaluation-and-ops",
      "title": "RAG in Production",
      "description": "Operational considerations for production RAG systems.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ğŸ­", "text": "Your RAG prototype works perfectly on 10 test queries. You deploy it and real users break it in ways you never imagined." },
        "buildup": { "visual": "ğŸ“‹", "text": "Production RAG needs: monitoring (latency, retrieval quality, answer quality), logging (every query, every retrieval, every response), caching (repeated queries), and graceful degradation (what happens when the vector DB is down?)." },
        "discovery": { "visual": "ğŸ’¡", "text": "Build a feedback loop: let users flag bad answers, log the query + retrieved chunks + response, and review flagged cases weekly. This is your best source of real-world eval data." },
        "twist": { "visual": "âš¡", "text": "The biggest production surprise: users ask questions your knowledge base doesn't cover. Without a 'I don't know' path, the system hallucinates confidently. Handle out-of-scope queries explicitly." },
        "climax": { "visual": "ğŸ", "text": "Treat RAG like any production system: observability, alerts, fallback paths, and continuous improvement. The prototype is 20% of the work." },
        "punchline": { "visual": "ğŸ¬", "text": "Demo-ready is not production-ready. The gap is monitoring, edge cases, and graceful failure." }
      },
      "quiz": {
        "question": "What's the most common production surprise in RAG systems?",
        "options": [
          "The vector database is too fast",
          "Users ask questions the knowledge base doesn't cover",
          "Retrieval always returns perfect results",
          "LLMs never hallucinate in production"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t25-document-loaders",
      "chapter_id": "ai--rag-basics--ch01-why-rag",
      "title": "Document Loaders",
      "description": "How to ingest diverse document types into your RAG pipeline.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ“‚", "text": "Your knowledge lives in PDFs, Google Docs, Notion pages, Slack threads, and Confluence wikis. The RAG pipeline only understands plain text. Now what?" },
        "buildup": { "visual": "ğŸ”Œ", "text": "Document loaders are the first stage of any RAG pipeline. They extract text from structured and unstructured sources into a common format the rest of the pipeline can process." },
        "discovery": { "visual": "ğŸ’¡", "text": "Frameworks like LangChain and LlamaIndex provide loaders for dozens of formats: PDF, DOCX, HTML, CSV, markdown, databases, and APIs. Each loader handles parsing quirks specific to its format." },
        "twist": { "visual": "âš¡", "text": "The hardest part isn't loading â€” it's preserving structure. Tables in PDFs lose their formatting, headers get merged with body text, and images are ignored entirely. Garbage in, garbage out." },
        "climax": { "visual": "ğŸ", "text": "A good ingestion pipeline validates extracted text, preserves metadata (source, date, author), and flags documents that parsed poorly for human review." },
        "punchline": { "visual": "ğŸ¬", "text": "RAG starts before embeddings. It starts with getting clean text out of messy documents." }
      },
      "quiz": {
        "question": "What is the primary purpose of document loaders in RAG?",
        "options": [
          "To generate embeddings from text",
          "To extract text from diverse document formats into a common format",
          "To store documents in a vector database",
          "To train the LLM on new data"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t26-embedding-dimensions",
      "chapter_id": "ai--rag-basics--ch02-embeddings-and-vectors",
      "title": "Embedding Dimensions",
      "description": "How the number of dimensions affects quality, speed, and cost.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "text-embedding-3-small produces 1536-dimensional vectors. text-embedding-3-large produces 3072. Does doubling the dimensions double the quality?" },
        "buildup": { "visual": "ğŸ“Š", "text": "Each dimension captures one facet of meaning. More dimensions let the model encode finer distinctions â€” like having a 3072-colour palette instead of 1536." },
        "discovery": { "visual": "ğŸ’¡", "text": "Higher dimensions generally improve retrieval accuracy, but with diminishing returns. Going from 384 to 768 dimensions helps a lot. Going from 1536 to 3072 helps a little." },
        "twist": { "visual": "âš¡", "text": "More dimensions mean bigger vectors, more storage, slower similarity search, and higher costs. A 3072-dim index costs roughly 2x a 1536-dim index to store and query." },
        "climax": { "visual": "ğŸ", "text": "Many teams use Matryoshka embeddings â€” models trained so you can truncate dimensions (use the first 512 of 1536) with graceful quality degradation." },
        "punchline": { "visual": "ğŸ¬", "text": "More dimensions aren't always worth it. Match your embedding size to your quality needs and your budget." }
      },
      "quiz": {
        "question": "What is the main tradeoff of using higher-dimensional embeddings?",
        "options": [
          "They are always worse at retrieval",
          "They improve accuracy but increase storage and search costs",
          "They require a different LLM",
          "They can only embed English text"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t27-recursive-chunking",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Recursive Chunking",
      "description": "A chunking strategy that respects document structure by splitting at natural boundaries.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸª†", "text": "You split a document every 500 tokens. A paragraph about 'machine learning applications' gets cut in half â€” one chunk says 'applications include' and the other says 'healthcare and finance.' Neither chunk is useful alone." },
        "buildup": { "visual": "ğŸ“„", "text": "Recursive chunking tries to split at natural boundaries: first by section headers, then paragraphs, then sentences, then words â€” only going deeper when a chunk exceeds the target size." },
        "discovery": { "visual": "ğŸ’¡", "text": "The algorithm: try splitting on '\\n\\n' (paragraphs). If any chunk is still too large, split those on '\\n' (lines). Still too large? Split on '. ' (sentences). This preserves semantic coherence." },
        "twist": { "visual": "âš¡", "text": "Recursive chunking is the default strategy in LangChain for a reason â€” it balances chunk size consistency with semantic coherence better than fixed-size splitting for most document types." },
        "climax": { "visual": "ğŸ", "text": "Combine recursive chunking with metadata (source document, section header, position) for chunks that are both well-sized and well-labelled." },
        "punchline": { "visual": "ğŸ¬", "text": "Split where the text naturally breathes. Paragraphs are better boundaries than byte counts." }
      },
      "quiz": {
        "question": "How does recursive chunking differ from fixed-size chunking?",
        "options": [
          "It creates larger chunks",
          "It splits at natural text boundaries like paragraphs and sentences",
          "It doesn't require any configuration",
          "It only works with PDF documents"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t28-parent-document-retrieval",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Parent Document Retrieval",
      "description": "Retrieve small chunks for precision, then expand to parent documents for context.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "Small chunks match queries precisely but lack context. Large chunks have context but match poorly. What if you could have both?" },
        "buildup": { "visual": "ğŸ§©", "text": "Parent document retrieval is a two-stage strategy: index small chunks for precise matching, but when a chunk is retrieved, return its parent (the larger section or full document) to the LLM." },
        "discovery": { "visual": "ğŸ’¡", "text": "Index chunks of ~200 tokens for search. Store a mapping from each child chunk to its parent (the full section of ~2000 tokens). On retrieval, swap the child for its parent before sending to the LLM." },
        "twist": { "visual": "âš¡", "text": "This technique dramatically reduces 'lost context' hallucinations. The model sees the surrounding paragraphs, not just the matching sentence, so it can ground its answer in complete context." },
        "climax": { "visual": "ğŸ", "text": "Parent document retrieval is the best of both worlds: search precision of small chunks + answer quality of large chunks." },
        "punchline": { "visual": "ğŸ¬", "text": "Find with a scalpel, read with a magnifying glass." }
      },
      "quiz": {
        "question": "What does parent document retrieval do after finding a matching small chunk?",
        "options": [
          "Deletes the parent document",
          "Returns only the small chunk to the LLM",
          "Expands to the larger parent section for more context",
          "Re-embeds the chunk with a different model"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--rag-basics--t29-multi-query-retrieval",
      "chapter_id": "ai--rag-basics--ch04-retrieval",
      "title": "Multi-Query Retrieval",
      "description": "Generate multiple query variations to improve retrieval recall.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”€", "text": "A user asks 'How do I fix a memory leak in Python?' The relevant document says 'Python garbage collection and memory management.' One embedding query misses it. Three variations catch it." },
        "buildup": { "visual": "ğŸ“", "text": "Multi-query retrieval uses an LLM to generate 3-5 rephrased versions of the user's query. Each variation captures a different angle of the same intent." },
        "discovery": { "visual": "ğŸ’¡", "text": "'Fix memory leak in Python' becomes: 'Python memory management best practices,' 'garbage collection issues in Python,' and 'diagnosing Python memory problems.' Each query retrieves different relevant chunks." },
        "twist": { "visual": "âš¡", "text": "The cost is 3-5x more embedding calls and vector searches. But the recall improvement is dramatic â€” especially for vague or poorly-worded queries." },
        "climax": { "visual": "ğŸ", "text": "Combine results from all queries, deduplicate, and rank by frequency + score. Chunks retrieved by multiple query variations are almost certainly relevant." },
        "punchline": { "visual": "ğŸ¬", "text": "One question, many angles. Multi-query turns bad searches into great ones." }
      },
      "quiz": {
        "question": "What is the main benefit of multi-query retrieval?",
        "options": [
          "It reduces the number of API calls",
          "It improves recall by searching from multiple semantic angles",
          "It eliminates the need for a vector database",
          "It makes responses faster"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t30-prompt-engineering-for-rag",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Prompt Engineering for RAG",
      "description": "Craft generation prompts that keep the LLM faithful to retrieved context.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âœï¸", "text": "You retrieve perfect chunks but the LLM ignores them and answers from its training data instead. The retrieval worked â€” the prompt didn't." },
        "buildup": { "visual": "ğŸ“‹", "text": "The generation prompt must explicitly instruct the model: 'Answer ONLY based on the context below. If the context doesn't contain the answer, say you don't know.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Structure matters: place the context before the question, use XML tags or clear delimiters to separate context from instructions, and repeat key constraints at the end of the prompt." },
        "twist": { "visual": "âš¡", "text": "Even with perfect prompts, models sometimes 'leak' training knowledge into RAG answers. Adding 'Do not use prior knowledge' helps but isn't bulletproof â€” always verify critical outputs." },
        "climax": { "visual": "ğŸ", "text": "The best RAG prompts are specific, structured, and defensive: they tell the model what to do, how to format it, and what to do when context is insufficient." },
        "punchline": { "visual": "ğŸ¬", "text": "The retrieval finds the signal. The prompt determines whether the model listens." }
      },
      "quiz": {
        "question": "What is a key instruction to include in a RAG generation prompt?",
        "options": [
          "Always use your training data",
          "Answer only based on the provided context",
          "Ignore the retrieved documents",
          "Generate the longest possible response"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--rag-basics--t31-handling-no-results",
      "chapter_id": "ai--rag-basics--ch05-generation",
      "title": "Handling No Results",
      "description": "What your RAG system should do when retrieval comes up empty.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ•³ï¸", "text": "User asks a question. The retriever finds nothing relevant. Without a fallback, the LLM hallucinates an answer and presents it with full confidence." },
        "buildup": { "visual": "ğŸš§", "text": "Every RAG system needs a 'no results' path. Check the similarity scores of retrieved chunks â€” if the best score is below your threshold, trigger the fallback." },
        "discovery": { "visual": "ğŸ’¡", "text": "Good fallbacks: 'I don't have information about that topic,' redirect to search, suggest related topics, or escalate to a human. The worst fallback is pretending you found something." },
        "twist": { "visual": "âš¡", "text": "Setting the similarity threshold is tricky. Too high and you reject valid results. Too low and you pass irrelevant chunks. Calibrate it using your eval set, not gut instinct." },
        "climax": { "visual": "ğŸ", "text": "A RAG system that says 'I don't know' when appropriate is more trustworthy than one that always has an answer. Users learn to trust the system because its answers are reliable." },
        "punchline": { "visual": "ğŸ¬", "text": "The most honest answer is sometimes 'I don't know.' Build that path explicitly." }
      },
      "quiz": {
        "question": "What should a RAG system do when no relevant chunks are retrieved?",
        "options": [
          "Hallucinate an answer from training data",
          "Return an error and crash",
          "Trigger a fallback like 'I don't have information about that'",
          "Search the entire internet instead"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--rag-basics--t32-chunking-for-tables-and-code",
      "chapter_id": "ai--rag-basics--ch03-chunking-and-indexing",
      "title": "Chunking Tables & Code",
      "description": "Special chunking strategies for structured content like tables and code blocks.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "Your document has a pricing table. The chunker splits it after row 3. The retrieved chunk says 'Enterprise plan: $â€”' because the price was in the next chunk." },
        "buildup": { "visual": "ğŸ§±", "text": "Tables, code blocks, and lists are fundamentally different from prose. Splitting them mid-structure destroys their meaning. Standard chunkers treat them as regular text and break them apart." },
        "discovery": { "visual": "ğŸ’¡", "text": "Detect structured elements first: use regex or HTML/Markdown parsing to identify tables, code fences, and lists. Keep them as atomic units â€” never split inside a table or code block." },
        "twist": { "visual": "âš¡", "text": "Sometimes a table exceeds your chunk size limit. Convert it to natural language ('The Enterprise plan costs $499/month and includes...') or split by rows with repeated headers." },
        "climax": { "visual": "ğŸ", "text": "A chunking pipeline should have element-aware splitting: prose gets recursive splitting, tables stay whole, code blocks stay whole, and oversized elements get special handling." },
        "punchline": { "visual": "ğŸ¬", "text": "Not all text is paragraphs. Chunk tables and code as the structured data they are." }
      },
      "quiz": {
        "question": "Why do tables need special chunking treatment in RAG?",
        "options": [
          "Tables are always too small to chunk",
          "Splitting mid-table destroys the relationship between rows and columns",
          "Tables don't contain useful information",
          "Standard chunkers already handle tables perfectly"
        ],
        "correct": 1
      }
    }
  ]
}
