{
  "categoryId": "ai",
  "subject": "AI",
  "courseId": "ai--ai-agents",
  "courseTitle": "AI Agents",
  "emoji": "ğŸ¤–",
  "color": "#EF4444",
  "requireAuthoredStory": true,
  "chapters": [
    {
      "id": "ai--ai-agents--ch01-what-are-agents",
      "title": "What Are Agents?",
      "position": 1
    },
    {
      "id": "ai--ai-agents--ch02-tool-use",
      "title": "Tool Use",
      "position": 2
    },
    {
      "id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "Planning & Reasoning",
      "position": 3
    },
    {
      "id": "ai--ai-agents--ch04-memory",
      "title": "Memory",
      "position": 4
    },
    {
      "id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Multi-Agent Systems",
      "position": 5
    },
    {
      "id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Reliability & Safety",
      "position": 6
    }
  ],
  "topics": [
    {
      "id": "ai--ai-agents--t01-what-is-an-ai-agent",
      "chapter_id": "ai--ai-agents--ch01-what-are-agents",
      "title": "What Is an AI Agent?",
      "description": "Software that perceives, decides, and acts autonomously.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ¤–", "text": "A chatbot answers questions you type. An agent books your flight, checks your calendar, and sends you a confirmation â€” without you asking for each step." },
        "buildup": { "visual": "ğŸ§ ", "text": "An AI agent is a system that takes a goal, breaks it into tasks, decides which actions to take, executes them, observes the results, and adjusts. The key difference from a chatbot: autonomy." },
        "discovery": { "visual": "ğŸ’¡", "text": "The loop: Perceive â†’ Think â†’ Act â†’ Observe â†’ Repeat. The agent keeps going until the goal is met or it decides it can't continue." },
        "twist": { "visual": "âš¡", "text": "Most 'agents' today are really just LLMs with tool access and a loop. True autonomous reasoning is still limited â€” the planning ability of current models is the bottleneck." },
        "climax": { "visual": "ğŸ", "text": "Agents are the next step from chatbots: instead of single-turn Q&A, they chain multiple actions to accomplish complex goals." },
        "punchline": { "visual": "ğŸ¬", "text": "Chatbots talk. Agents do. The difference is a loop that plans, acts, and adapts." }
      },
      "quiz": {
        "question": "What fundamentally distinguishes an AI agent from a chatbot?",
        "options": [
          "Agents use larger language models",
          "Agents autonomously plan and take multiple actions to achieve a goal",
          "Agents don't use natural language",
          "Agents work offline only"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t02-the-agent-loop",
      "chapter_id": "ai--ai-agents--ch01-what-are-agents",
      "title": "The Agent Loop",
      "description": "Perceive, decide, act, observe â€” the core cycle.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "You ask an agent to 'find the cheapest flight to Tokyo next week.' It doesn't answer in one shot â€” it runs a loop of searches, comparisons, and decisions." },
        "buildup": { "visual": "ğŸ“‹", "text": "Step 1: Parse the goal. Step 2: Decide which tool to call (search flights). Step 3: Read the results. Step 4: Decide the next action (filter by price). Step 5: Repeat until done." },
        "discovery": { "visual": "ğŸ’¡", "text": "Each iteration of the loop is one 'thought-action-observation' cycle. The LLM reasons about what to do, calls a tool, reads the output, and decides the next step." },
        "twist": { "visual": "âš¡", "text": "The loop can run forever if the agent never decides it's done. Every agent needs a termination condition: max iterations, goal achieved, or explicit 'give up' logic." },
        "climax": { "visual": "ğŸ", "text": "The agent loop is simple in concept but tricky in practice. Getting the model to loop correctly â€” knowing when to act, when to think, and when to stop â€” is the hard part." },
        "punchline": { "visual": "ğŸ¬", "text": "Think. Act. Observe. Repeat. Simple loop, powerful results â€” when it terminates." }
      },
      "quiz": {
        "question": "Why does every agent loop need a termination condition?",
        "options": [
          "To save API costs",
          "To prevent infinite loops when the agent can't achieve its goal",
          "Because LLMs can only process one request",
          "Termination conditions are optional"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t03-agent-architectures",
      "chapter_id": "ai--ai-agents--ch01-what-are-agents",
      "title": "Agent Architectures",
      "description": "ReAct, Plan-and-Execute, and other design patterns.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ—ï¸", "text": "Should the agent plan everything upfront, or figure it out one step at a time? Both approaches exist, and they work differently." },
        "buildup": { "visual": "ğŸ“", "text": "ReAct agents interleave reasoning and acting â€” one step at a time. Plan-and-Execute agents create a full plan first, then execute each step. Each has tradeoffs." },
        "discovery": { "visual": "ğŸ’¡", "text": "ReAct is flexible: it can adapt to unexpected results. Plan-and-Execute is more reliable for well-defined tasks. Some systems combine both: plan first, then use ReAct for each step." },
        "twist": { "visual": "âš¡", "text": "The architecture matters less than the quality of your tool descriptions and prompts. A simple ReAct agent with well-described tools often outperforms a complex architecture with vague tool specs." },
        "climax": { "visual": "ğŸ", "text": "Start with ReAct â€” it's simple and well-supported by frameworks. Move to Plan-and-Execute when your tasks are predictable and multi-step coordination matters." },
        "punchline": { "visual": "ğŸ¬", "text": "Simple architecture, good tools. That beats complex architecture, vague tools every time." }
      },
      "quiz": {
        "question": "How does a ReAct agent differ from a Plan-and-Execute agent?",
        "options": [
          "ReAct agents can't use tools",
          "ReAct interleaves reasoning and acting step-by-step; Plan-and-Execute creates the full plan first",
          "Plan-and-Execute agents never adapt their plans",
          "They are identical approaches"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t04-agent-frameworks",
      "chapter_id": "ai--ai-agents--ch01-what-are-agents",
      "title": "Agent Frameworks",
      "description": "LangGraph, CrewAI, AutoGen, and the ecosystem.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ§°", "text": "You could build an agent from scratch with raw API calls and a while loop. Or you could use a framework. The ecosystem has exploded â€” here's what matters." },
        "buildup": { "visual": "ğŸ“š", "text": "LangGraph gives you explicit control over agent state and flow. CrewAI makes multi-agent collaboration easy. AutoGen (Microsoft) focuses on conversational agents. OpenAI's SDK has built-in agent features." },
        "discovery": { "visual": "ğŸ’¡", "text": "Frameworks handle the boilerplate: tool calling, state management, retry logic, conversation history. You focus on the business logic and tool definitions." },
        "twist": { "visual": "âš¡", "text": "Frameworks add abstraction, and abstractions leak. When debugging, you'll need to understand what the framework is actually sending to the LLM. Don't use a framework you can't debug." },
        "climax": { "visual": "ğŸ", "text": "Pick a framework based on your needs: LangGraph for complex flows, CrewAI for multi-agent, or just the raw OpenAI/Anthropic SDK for simple single-agent tools." },
        "punchline": { "visual": "ğŸ¬", "text": "Frameworks save time until they don't. Know what's happening under the hood." }
      },
      "quiz": {
        "question": "What is the main benefit of using an agent framework?",
        "options": [
          "Frameworks eliminate all bugs",
          "They handle boilerplate like tool calling, state management, and retries",
          "They make agents faster than raw API calls",
          "They are required to build any agent"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t05-function-calling",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Function Calling",
      "description": "How LLMs invoke external tools through structured outputs.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "The model can't check the weather. But it can output: {\"tool\": \"get_weather\", \"args\": {\"city\": \"Tokyo\"}}. Your code calls the API. The model uses the result." },
        "buildup": { "visual": "ğŸ”§", "text": "Function calling lets the LLM request tool invocations in a structured format. You define available tools (name, description, parameters), the model picks which to call, and your code executes it." },
        "discovery": { "visual": "ğŸ’¡", "text": "The model doesn't run any code. It outputs a JSON payload saying 'call this function with these arguments.' Your application code does the actual execution and feeds the result back." },
        "twist": { "visual": "âš¡", "text": "The model can hallucinate function calls â€” calling tools that don't exist or passing wrong arguments. Validate every tool call before execution. Never trust model output as safe to run." },
        "climax": { "visual": "ğŸ", "text": "Function calling is the foundation of agent tool use. Define tools clearly, validate calls strictly, and always handle errors gracefully." },
        "punchline": { "visual": "ğŸ¬", "text": "The model suggests. Your code executes. Never let the model run code it dreamed up." }
      },
      "quiz": {
        "question": "What actually happens during LLM function calling?",
        "options": [
          "The LLM directly executes code on a server",
          "The LLM outputs a structured request; your application code executes the function",
          "The LLM downloads and runs the tool",
          "Function calling requires no application code"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t06-tool-descriptions",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Writing Tool Descriptions",
      "description": "Why tool descriptions are the most important agent prompt.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "Your agent has access to 'search_database.' It never calls it. Why? Because the description says 'searches the database.' The model doesn't know when to use it or what it returns." },
        "buildup": { "visual": "ğŸ“‹", "text": "A good tool description explains: what the tool does, when to use it, what each parameter means, and what the output looks like. It's a prompt for the model â€” treat it as one." },
        "discovery": { "visual": "ğŸ’¡", "text": "Bad: 'Searches products.' Good: 'Search the product catalog by name or category. Returns up to 10 results with name, price, and availability. Use when the user asks about specific products.'" },
        "twist": { "visual": "âš¡", "text": "Tool descriptions eat context window tokens. 20 tools with long descriptions can use 3,000+ tokens before any conversation happens. Keep descriptions precise but not verbose." },
        "climax": { "visual": "ğŸ", "text": "The quality of your tool descriptions directly determines how well the agent selects and invokes tools. This is the highest-leverage prompt in your agent system." },
        "punchline": { "visual": "ğŸ¬", "text": "Your tools are only as useful as their descriptions. Describe them like you're training a new coworker." }
      },
      "quiz": {
        "question": "What should a good tool description include?",
        "options": [
          "Just the tool name",
          "What the tool does, when to use it, parameter meanings, and output format",
          "The tool's source code",
          "Only the parameter types"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t07-tool-selection",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Tool Selection",
      "description": "How agents decide which tool to use and when.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ¯", "text": "Your agent has 15 tools. The user says 'find revenue data for Q3.' The agent calls the email tool instead of the database tool. Tool selection went wrong." },
        "buildup": { "visual": "âš–ï¸", "text": "The model reads all tool descriptions, the user's query, and any conversation history, then decides which tool (if any) to call. It's essentially a classification problem." },
        "discovery": { "visual": "ğŸ’¡", "text": "Reduce ambiguity: give tools distinct, non-overlapping descriptions. If two tools sound similar, the model will pick randomly. Differentiate by when and why, not just what." },
        "twist": { "visual": "âš¡", "text": "More tools doesn't mean more capable. After about 10â€“15 tools, selection accuracy drops. Group related tools into namespaces or use a routing agent that narrows the tool set." },
        "climax": { "visual": "ğŸ", "text": "Test tool selection explicitly: give the agent 20 test queries and check if it picks the right tool each time. This is a separate eval from end-to-end task success." },
        "punchline": { "visual": "ğŸ¬", "text": "Fewer tools, clearer descriptions. The agent picks correctly when the choices are obvious." }
      },
      "quiz": {
        "question": "What happens to tool selection accuracy as the number of tools increases?",
        "options": [
          "It always improves",
          "It tends to decrease after about 10-15 tools due to ambiguity",
          "It stays exactly the same",
          "The number of tools doesn't affect selection"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t08-parallel-tool-calls",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Parallel Tool Calls",
      "description": "Call multiple tools at once to speed up the agent.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "âš¡", "text": "The agent needs weather in Tokyo, a flight search, and hotel availability. It calls them one at a time: 3 seconds each, 9 seconds total. It could have called all three at once." },
        "buildup": { "visual": "ğŸ”€", "text": "Modern LLM APIs support parallel tool calls â€” the model can request multiple tool invocations in a single response. Your code executes them concurrently and returns all results." },
        "discovery": { "visual": "ğŸ’¡", "text": "Enable parallel tool calling when tools are independent: weather, flights, and hotels don't depend on each other. The model outputs all three call requests in one turn." },
        "twist": { "visual": "âš¡", "text": "Not all tools can run in parallel. If tool B needs tool A's output (search for flights â†’ then book the cheapest), the model must call them sequentially. The model usually gets dependency ordering right, but verify." },
        "climax": { "visual": "ğŸ", "text": "Parallel tool calls cut agent latency dramatically for independent tasks. Enable it by default and let the model decide when to parallelize." },
        "punchline": { "visual": "ğŸ¬", "text": "Independent tools? Call them at the same time. Dependent tools? One at a time. Let the model figure out which is which." }
      },
      "quiz": {
        "question": "When should tool calls be parallelized?",
        "options": [
          "Always, regardless of dependencies",
          "When the tools are independent and don't need each other's output",
          "Never â€” sequential is always better",
          "Only when there are exactly 2 tools"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t09-step-by-step-reasoning",
      "chapter_id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "Step-by-Step Reasoning",
      "description": "How agents break complex goals into manageable steps.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "'Prepare a market analysis report on competitor pricing.' The agent can't do this in one step. It needs to decompose the goal into a sequence of smaller tasks." },
        "buildup": { "visual": "ğŸ§©", "text": "The agent reasons: (1) identify competitors, (2) search for their pricing pages, (3) extract pricing data, (4) compare with our pricing, (5) draft the analysis. Each step uses different tools." },
        "discovery": { "visual": "ğŸ’¡", "text": "Chain-of-thought prompting helps: instruct the agent to 'Think step by step about what you need to do before taking any action.' This produces better plans than diving straight into tool calls." },
        "twist": { "visual": "âš¡", "text": "The initial plan is often wrong. A good agent adapts: if step 2 fails (no pricing page found), it adjusts â€” maybe searching news articles or SEC filings instead." },
        "climax": { "visual": "ğŸ", "text": "Step-by-step reasoning is the foundation of agent planning. The model must be able to decompose goals and adapt when individual steps fail." },
        "punchline": { "visual": "ğŸ¬", "text": "Big goals, small steps. And the flexibility to change the plan when step 3 doesn't work." }
      },
      "quiz": {
        "question": "What makes agent planning effective?",
        "options": [
          "Creating a fixed plan that never changes",
          "Breaking goals into steps and adapting when individual steps fail",
          "Skipping directly to the final answer",
          "Using the largest possible model"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t10-react-pattern",
      "chapter_id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "The ReAct Pattern",
      "description": "Reasoning and Acting interleaved for step-by-step execution.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "Thought: I need to find the latest sales numbers. Action: search_database('sales', 'Q3 2024'). Observation: Revenue was $2.3M. Thought: Now I need to compare with Q2..." },
        "buildup": { "visual": "ğŸ“‹", "text": "ReAct (Reasoning + Acting) alternates: the model reasons about what to do (Thought), takes an action (Action), reads the result (Observation), then reasons again. This is the standard agent pattern." },
        "discovery": { "visual": "ğŸ’¡", "text": "The explicit 'Thought' step forces the model to reason before acting, reducing impulsive tool calls. Without it, models sometimes call tools blindly without considering whether it's the right next step." },
        "twist": { "visual": "âš¡", "text": "ReAct can get stuck in loops â€” the model repeats the same thought-action-observation cycle without making progress. Set a max iteration count and add 'if you're not making progress, summarize what you've tried and stop.'" },
        "climax": { "visual": "ğŸ", "text": "ReAct is the most widely used agent pattern. It works well for tasks that unfold one step at a time and where each step informs the next." },
        "punchline": { "visual": "ğŸ¬", "text": "Think, act, observe, repeat. The most popular agent pattern for a reason â€” it works." }
      },
      "quiz": {
        "question": "What is the core cycle of the ReAct pattern?",
        "options": [
          "Plan â†’ Execute â†’ Done",
          "Thought â†’ Action â†’ Observation â†’ repeat",
          "Input â†’ Output â†’ End",
          "Train â†’ Test â†’ Deploy"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t11-planning-with-reflection",
      "chapter_id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "Planning with Reflection",
      "description": "Agents that critique and revise their own plans.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸª", "text": "The agent creates a plan, executes it, and gets a bad result. Instead of giving up, it reviews what went wrong and creates a better plan. That's reflection." },
        "buildup": { "visual": "ğŸ”", "text": "Reflection adds a self-critique step: after each action (or after the full plan), the agent asks 'Did this work? What could be improved? Should I try a different approach?'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Implementation: add a reflection prompt after tool results: 'Review the result. Was this what you expected? If not, what should you do differently?' Feed the critique back into the next planning step." },
        "twist": { "visual": "âš¡", "text": "Reflection doubles the number of LLM calls â€” and the agent can reflect poorly, convincing itself a bad plan is good. Limit reflection to critical checkpoints, not every single step." },
        "climax": { "visual": "ğŸ", "text": "Use reflection for high-stakes tasks or when the first attempt fails. For straightforward tool calls, reflection just adds latency without value." },
        "punchline": { "visual": "ğŸ¬", "text": "Smart agents don't just act. They review, critique, and revise. But only when it matters." }
      },
      "quiz": {
        "question": "When is agent reflection most valuable?",
        "options": [
          "After every single tool call",
          "For high-stakes tasks or after a first attempt fails",
          "Only before the agent starts",
          "Reflection is never useful"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t12-short-term-memory",
      "chapter_id": "ai--ai-agents--ch04-memory",
      "title": "Short-Term Memory",
      "description": "Conversation history and scratchpad within a single session.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "The agent calls 5 tools and gets 5 results. By the time it reaches tool call 6, has it forgotten what tool call 2 returned? It depends on how you manage memory." },
        "buildup": { "visual": "ğŸ§ ", "text": "Short-term memory is the agent's working context: conversation history, tool call results, and any intermediate reasoning. It lives in the prompt and grows with each step." },
        "discovery": { "visual": "ğŸ’¡", "text": "The simplest implementation: append every thought, action, and observation to the message history. The model sees the full trace when deciding the next step." },
        "twist": { "visual": "âš¡", "text": "Short-term memory fills the context window. After 20 tool calls with verbose results, you run out of space. Summarize earlier steps or drop irrelevant ones to stay within limits." },
        "climax": { "visual": "ğŸ", "text": "Manage short-term memory actively: keep recent steps in full, summarize older steps, and drop tool results that are no longer relevant." },
        "punchline": { "visual": "ğŸ¬", "text": "An agent that forgets what it did 3 steps ago makes circles. Manage the scratchpad." }
      },
      "quiz": {
        "question": "What is the main challenge with agent short-term memory?",
        "options": [
          "LLMs have perfect memory",
          "The growing message history eventually exceeds the context window",
          "Short-term memory is stored in a database",
          "Agents don't need memory"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t13-long-term-memory",
      "chapter_id": "ai--ai-agents--ch04-memory",
      "title": "Long-Term Memory",
      "description": "Persisting knowledge across sessions and conversations.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ’¾", "text": "The user talked to your agent yesterday about Project Alpha. Today they say 'continue where we left off.' The agent has no memory of yesterday â€” the session is gone." },
        "buildup": { "visual": "ğŸ—„ï¸", "text": "Long-term memory stores information between sessions: user preferences, past conversations, learned facts, completed tasks. It lives in a database, not in the prompt." },
        "discovery": { "visual": "ğŸ’¡", "text": "At each session start, retrieve relevant memories: 'What do we know about this user? What were the recent conversations about?' Inject a summary into the system prompt." },
        "twist": { "visual": "âš¡", "text": "Storing everything is easy. Retrieving the right memory at the right time is hard. Use semantic search over memories â€” the same RAG techniques that work for documents work for memories." },
        "climax": { "visual": "ğŸ", "text": "Long-term memory turns a stateless agent into a persistent assistant. Implement it as a RAG system over past interactions â€” embed, store, retrieve, inject." },
        "punchline": { "visual": "ğŸ¬", "text": "Memory is what turns an agent from a tool into an assistant. It remembers so the user doesn't have to repeat." }
      },
      "quiz": {
        "question": "How is long-term agent memory typically implemented?",
        "options": [
          "By increasing the context window indefinitely",
          "By storing past interactions in a database and retrieving relevant ones per session",
          "By fine-tuning the model after every conversation",
          "LLMs have built-in long-term memory"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t14-state-management",
      "chapter_id": "ai--ai-agents--ch04-memory",
      "title": "State Management",
      "description": "Tracking what the agent knows and where it is in a task.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "Your agent is halfway through a 10-step task. The API call fails. When you restart the agent, it starts from step 1. All progress lost." },
        "buildup": { "visual": "ğŸ’¾", "text": "State management persists the agent's progress: which steps are complete, what data has been collected, what the current plan is. Think of it as a save file for the agent." },
        "discovery": { "visual": "ğŸ’¡", "text": "LangGraph models state as a graph: nodes are steps, edges are transitions, and the current state is a checkpoint. If the agent crashes, resume from the last checkpoint." },
        "twist": { "visual": "âš¡", "text": "State adds complexity: now you need to serialize intermediate results, handle partial failures, and manage state cleanup. For simple agents, a message history is sufficient state." },
        "climax": { "visual": "ğŸ", "text": "Use explicit state management for multi-step tasks that take more than a few seconds. For quick single-turn tool calls, the message history is your state." },
        "punchline": { "visual": "ğŸ¬", "text": "Can your agent survive a crash and pick up where it left off? That's the state management test." }
      },
      "quiz": {
        "question": "Why is state management important for agents?",
        "options": [
          "It makes the agent faster",
          "It allows resuming from where the agent left off after failures",
          "It reduces the number of API calls",
          "State management is only for databases"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t15-multi-agent-basics",
      "chapter_id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Multi-Agent Basics",
      "description": "Why and when to use multiple agents instead of one.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ‘¥", "text": "One agent handles research, writing, code review, and email. It gets confused about which role it's in. What if each role was a separate agent?" },
        "buildup": { "visual": "ğŸ§©", "text": "Multi-agent systems split responsibilities: a researcher agent finds information, a writer agent drafts the document, a reviewer agent critiques it. Each has focused tools and instructions." },
        "discovery": { "visual": "ğŸ’¡", "text": "Specialization helps: a coding agent with only code tools and a coding-focused system prompt performs better than a general-purpose agent that also writes emails." },
        "twist": { "visual": "âš¡", "text": "Multi-agent adds coordination overhead: who runs when? How do agents share information? How do you handle disagreements? Sometimes one well-prompted agent is simpler and faster." },
        "climax": { "visual": "ğŸ", "text": "Use multi-agent when tasks naturally decompose into distinct roles with different tool sets. Use a single agent when the task is coherent and doesn't benefit from specialization." },
        "punchline": { "visual": "ğŸ¬", "text": "One agent can do many things okay. Multiple specialists can do each thing well. Pick based on task complexity." }
      },
      "quiz": {
        "question": "When should you use multiple agents instead of one?",
        "options": [
          "Always â€” more agents is always better",
          "When tasks naturally decompose into distinct roles with different tool sets",
          "Only when you have unlimited compute",
          "Never â€” one agent is always sufficient"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t16-agent-communication",
      "chapter_id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Agent Communication",
      "description": "How agents pass messages and coordinate.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ’¬", "text": "The research agent found three relevant articles. The writer agent needs them. How do they communicate? There's no shared brain â€” they're separate LLM calls." },
        "buildup": { "visual": "ğŸ“¨", "text": "Common patterns: (1) Shared memory â€” agents read/write to a shared state object. (2) Message passing â€” agents send structured messages to each other. (3) Orchestrator â€” a manager agent coordinates and relays information." },
        "discovery": { "visual": "ğŸ’¡", "text": "The orchestrator pattern is most common: one 'manager' agent decides which specialist to call next and passes context between them. It's like a project manager routing tasks." },
        "twist": { "visual": "âš¡", "text": "Information loss at handoffs is real. When the research agent's output is summarized for the writer agent, important details can be dropped. Pass raw data when possible." },
        "climax": { "visual": "ğŸ", "text": "Keep communication structured: define clear input/output schemas for each agent. Unstructured message passing leads to confusion and dropped information." },
        "punchline": { "visual": "ğŸ¬", "text": "Multi-agent coordination is a communication problem. Clear interfaces between agents matter more than the agents themselves." }
      },
      "quiz": {
        "question": "What is the orchestrator pattern in multi-agent systems?",
        "options": [
          "All agents communicate directly with each other",
          "A manager agent coordinates and routes tasks between specialist agents",
          "Agents share the same LLM instance",
          "Agents run without any coordination"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t17-human-in-the-loop",
      "chapter_id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Human-in-the-Loop",
      "description": "Pausing the agent for human approval before critical actions.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ–ï¸", "text": "The agent is about to send an email to your CEO with its analysis. Are you sure it's right? Human-in-the-loop pauses the agent for approval before irreversible actions." },
        "buildup": { "visual": "â¸ï¸", "text": "Define which actions are 'safe' (read database, search web) and which need approval (send email, delete files, make purchases). The agent pauses before approved actions." },
        "discovery": { "visual": "ğŸ’¡", "text": "Implementation: tag tools as 'auto-approve' or 'require-approval.' When the agent calls a require-approval tool, show the user the proposed action and wait for confirmation." },
        "twist": { "visual": "âš¡", "text": "Too many approval gates make the agent useless â€” the user might as well do it themselves. Too few make the agent dangerous. Calibrate based on reversibility: can you undo this action?" },
        "climax": { "visual": "ğŸ", "text": "The rule of thumb: auto-approve reads, require approval for writes and deletes, always require approval for external communication." },
        "punchline": { "visual": "ğŸ¬", "text": "Let the agent run freely on safe actions. Put guardrails on the dangerous ones. Trust is earned, not assumed." }
      },
      "quiz": {
        "question": "Which actions should typically require human approval?",
        "options": [
          "All actions, no matter how small",
          "Irreversible or externally-facing actions like sending emails or deleting data",
          "No actions â€” agents should be fully autonomous",
          "Only database reads"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t18-error-handling",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Error Handling",
      "description": "What happens when a tool call fails or returns unexpected results.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ’¥", "text": "The agent calls the weather API. It's down. The agent crashes. Users see 'Internal Server Error.' A 30-second outage in one tool killed the entire agent." },
        "buildup": { "visual": "ğŸ›¡ï¸", "text": "Every tool call can fail: timeouts, bad data, rate limits, permissions errors. The agent needs to handle each gracefully â€” retry, use an alternative, or inform the user." },
        "discovery": { "visual": "ğŸ’¡", "text": "Return errors as observations: instead of crashing, tell the model 'The weather API returned an error: service unavailable. Try a different approach or inform the user.'" },
        "twist": { "visual": "âš¡", "text": "Some models handle errors well â€” they adapt and try alternatives. Others get stuck in retry loops. Test your agent's error recovery explicitly with intentionally broken tools." },
        "climax": { "visual": "ğŸ", "text": "Wrap every tool call in try-catch. Return structured error messages to the model. Set retry limits. And always have a 'graceful failure' path that explains the issue to the user." },
        "punchline": { "visual": "ğŸ¬", "text": "The best agents aren't the ones that never fail. They're the ones that handle failure gracefully." }
      },
      "quiz": {
        "question": "How should agent tool errors be handled?",
        "options": [
          "Crash the entire agent",
          "Return structured error messages to the model so it can adapt or inform the user",
          "Ignore all errors",
          "Retry infinitely until it works"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t19-guardrails",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Agent Guardrails",
      "description": "Constraining what agents can do to prevent harmful actions.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸš§", "text": "Your agent has database access. A user asks it to 'delete all records from last month.' The agent happily calls DELETE FROM orders WHERE... That's not what guardrails look like." },
        "buildup": { "visual": "ğŸ”’", "text": "Guardrails constrain agent behavior: read-only database access, spending limits on purchases, approved recipient lists for emails, file system sandboxes." },
        "discovery": { "visual": "ğŸ’¡", "text": "Layer guardrails: (1) System prompt â€” 'Never delete records.' (2) Tool-level â€” the database tool only accepts SELECT queries. (3) Application-level â€” a middleware checks all actions against a policy." },
        "twist": { "visual": "âš¡", "text": "Prompt-level guardrails are suggestions, not enforcement. A clever prompt injection can bypass 'never delete records.' Tool-level and application-level guardrails are the only reliable constraints." },
        "climax": { "visual": "ğŸ", "text": "Never rely solely on the prompt for safety. Enforce constraints in code: read-only database roles, API rate limits, action allowlists. The model decides what to try; your code decides what's allowed." },
        "punchline": { "visual": "ğŸ¬", "text": "Prompts suggest. Code enforces. For safety, always choose enforcement." }
      },
      "quiz": {
        "question": "Why shouldn't you rely only on prompt-level guardrails?",
        "options": [
          "Prompts are too long",
          "Prompt instructions can be bypassed by injection; code-level enforcement is more reliable",
          "Guardrails aren't needed for agents",
          "Prompts are always followed perfectly"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t20-testing-agents",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Testing Agents",
      "description": "How to evaluate whether an agent works correctly.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ§ª", "text": "Your agent passes 10 test cases beautifully. User #1 asks something slightly different and the agent takes 47 steps and never finishes. How do you test something non-deterministic?" },
        "buildup": { "visual": "ğŸ“‹", "text": "Agent testing requires: (1) Task completion tests â€” does it achieve the goal? (2) Trajectory tests â€” did it take reasonable steps? (3) Safety tests â€” does it respect constraints? (4) Edge case tests â€” what happens with adversarial or ambiguous inputs?" },
        "discovery": { "visual": "ğŸ’¡", "text": "Mock your tools for testing: replace real APIs with deterministic fakes that return known results. This lets you test the agent's reasoning and planning without external dependencies." },
        "twist": { "visual": "âš¡", "text": "Agents are non-deterministic â€” the same input might produce different tool call sequences. Test for outcomes (was the task completed correctly?) rather than exact step sequences." },
        "climax": { "visual": "ğŸ", "text": "Build an eval suite: 50+ test cases covering happy paths, edge cases, and adversarial inputs. Run it after every change. Measure task success rate, average steps, and safety violations." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't test exact steps. Test outcomes: did the agent finish? Was it correct? Did it stay safe?" }
      },
      "quiz": {
        "question": "How should agent testing handle non-deterministic behavior?",
        "options": [
          "Only test once and assume it works",
          "Test for task completion outcomes rather than exact step sequences",
          "Make agents deterministic by setting temperature to 0",
          "Agent testing is impossible"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t21-cost-management",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Cost Management",
      "description": "Controlling how much an agent spends on LLM calls.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ğŸ’¸", "text": "Your agent ran overnight. It got stuck in a loop: 200 LLM calls, each with a full context window. Your API bill for one user's query: $47." },
        "buildup": { "visual": "ğŸ“Š", "text": "Agent costs compound: each loop iteration is an LLM call. With tool results injected into the context, each call uses more tokens than the last. 20 iterations can cost 50x a single chat message." },
        "discovery": { "visual": "ğŸ’¡", "text": "Budget controls: max iterations per run, max tokens per session, max cost per user per day. Log costs per agent run and alert on outliers." },
        "twist": { "visual": "âš¡", "text": "Use cheaper models for simple tool routing and expensive models for complex reasoning. A tiered approach â€” fast model picks the tool, strong model synthesizes the final answer â€” cuts costs dramatically." },
        "climax": { "visual": "ğŸ", "text": "Set hard limits: max 15 iterations, max 50K tokens per session. Kill runs that exceed the budget. It's better to fail fast than to burn money on a stuck agent." },
        "punchline": { "visual": "ğŸ¬", "text": "Agents with no budget limits are agents with no future. Set caps, log costs, tier your models." }
      },
      "quiz": {
        "question": "How can you reduce agent costs without reducing capability?",
        "options": [
          "Remove all tools",
          "Use cheaper models for simple routing and expensive models for complex reasoning",
          "Never use agents",
          "Increase the context window size"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t22-agent-vs-workflow",
      "chapter_id": "ai--ai-agents--ch01-what-are-agents",
      "title": "Agent vs Workflow",
      "description": "When to use an autonomous agent versus a fixed AI workflow.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ”€", "text": "A developer builds an agent for a task that always follows the same three steps. It works, but it's 5x slower and 10x more expensive than a simple pipeline." },
        "buildup": { "visual": "ğŸ“‹", "text": "Workflows are predefined step sequences: extract â†’ classify â†’ respond. Agents are dynamic: observe â†’ think â†’ act â†’ loop. Workflows are predictable; agents are flexible." },
        "discovery": { "visual": "ğŸ’¡", "text": "Use workflows when the steps are known in advance. Use agents when the task requires dynamic decisions â€” like deciding which tool to call based on intermediate results." },
        "twist": { "visual": "âš¡", "text": "Most production AI systems are workflows, not agents. Agents shine for open-ended tasks (research, complex troubleshooting) but are overkill for well-defined pipelines." },
        "climax": { "visual": "ğŸ", "text": "The decision framework: Can you draw a flowchart? Use a workflow. Does the path depend on each step's result? Consider an agent." },
        "punchline": { "visual": "ğŸ¬", "text": "Not everything needs an agent. Sometimes a good pipeline is the smarter choice." }
      },
      "quiz": {
        "question": "When should you prefer a workflow over an agent?",
        "options": [
          "When the task requires open-ended exploration",
          "When the steps are known in advance and follow a fixed sequence",
          "When you need maximum flexibility",
          "When the LLM needs to choose tools dynamically"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t23-structured-output-for-agents",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Structured Output for Agents",
      "description": "Forcing agents to produce parseable JSON for reliable tool invocation.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "The agent decides to call the search tool. But instead of structured JSON, it writes: 'I'll search for Python tutorials now.' Your tool parser crashes." },
        "buildup": { "visual": "ğŸ”§", "text": "Tool calls must be structured: function name, parameter names, parameter values â€” all in valid JSON. Free-text intent is useless to your code." },
        "discovery": { "visual": "ğŸ’¡", "text": "Use the model's native function calling or structured output mode. Define your tools as JSON schemas. The model is forced to respond in the exact format your code expects." },
        "twist": { "visual": "âš¡", "text": "Even with structured output, values can be wrong â€” the model might call search with the wrong query. Validate parameter values, not just structure, before executing." },
        "climax": { "visual": "ğŸ", "text": "Reliable agents require structured contracts between the model and your code. JSON schemas are those contracts." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't ask the model to format tool calls. Force it with schemas." }
      },
      "quiz": {
        "question": "Why is structured output important for agent tool calls?",
        "options": [
          "It makes the agent more creative",
          "It ensures tool invocations are parseable and executable by code",
          "It reduces the model's token usage",
          "It prevents the agent from using any tools"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t24-tool-error-recovery",
      "chapter_id": "ai--ai-agents--ch02-tool-use",
      "title": "Tool Error Recovery",
      "description": "How agents should handle failed tool calls gracefully.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ’¥", "text": "The agent calls an API. It returns a 500 error. The agent passes the raw error HTML to the LLM, which tries to parse HTML as data and hallucinates an answer." },
        "buildup": { "visual": "ğŸ”„", "text": "Tools fail: APIs time out, databases go down, files don't exist. An agent without error handling will spiral â€” retrying endlessly, misinterpreting errors, or giving up entirely." },
        "discovery": { "visual": "ğŸ’¡", "text": "Feed clear error messages back to the agent: 'Tool search_api failed: HTTP 429 rate limited. Try again in 30 seconds or use a different approach.' Let the agent decide how to adapt." },
        "twist": { "visual": "âš¡", "text": "The agent might decide the best recovery is a completely different strategy â€” querying a cache instead of the API, or asking the user for clarification. Good error messages enable good recovery." },
        "climax": { "visual": "ğŸ", "text": "Build a tool result wrapper: { success: bool, data: any, error: string }. The agent always gets a clean signal about what happened and can reason about next steps." },
        "punchline": { "visual": "ğŸ¬", "text": "The mark of a good agent isn't zero errors â€” it's graceful recovery." }
      },
      "quiz": {
        "question": "How should tool errors be communicated to an agent?",
        "options": [
          "Pass raw error HTML to the LLM",
          "Silently swallow the error",
          "Provide a clear, structured error message the agent can reason about",
          "Immediately terminate the agent run"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-agents--t25-chain-of-thought",
      "chapter_id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "Chain of Thought",
      "description": "How making agents show their work dramatically improves reasoning.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ§ ", "text": "Ask a model: 'If I have 23 apples and give away 7, then buy 12 more, how many do I have?' Without chain of thought, it often guesses wrong. With it, accuracy jumps 40%." },
        "buildup": { "visual": "ğŸ“", "text": "Chain of thought (CoT) means prompting the model to reason step by step before giving its final answer. 'Think through this step by step' is the simplest CoT prompt." },
        "discovery": { "visual": "ğŸ’¡", "text": "For agents, CoT means the model explains its plan before acting: 'First, I'll search for the user's order. Then I'll check its status. If it's shipped, I'll provide the tracking number.'" },
        "twist": { "visual": "âš¡", "text": "CoT uses more tokens (the reasoning is generated text), but the accuracy improvement usually more than pays for itself. The tradeoff is cost vs reliability." },
        "climax": { "visual": "ğŸ", "text": "In agent systems, CoT serves double duty: it improves reasoning AND provides an audit trail. You can inspect the agent's thought process when debugging." },
        "punchline": { "visual": "ğŸ¬", "text": "Thinking out loud isn't just for humans. It makes AI more reliable too." }
      },
      "quiz": {
        "question": "What is the main benefit of chain-of-thought prompting for agents?",
        "options": [
          "It makes responses shorter",
          "It improves reasoning accuracy by forcing step-by-step thinking",
          "It eliminates the need for tools",
          "It reduces API costs"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t26-task-decomposition",
      "chapter_id": "ai--ai-agents--ch03-planning-and-reasoning",
      "title": "Task Decomposition",
      "description": "Breaking complex tasks into manageable subtasks an agent can execute.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ§©", "text": "'Write a market analysis report' is too complex for a single LLM call. But 'find competitor prices,' 'summarize market trends,' and 'compile into sections' are each achievable." },
        "buildup": { "visual": "ğŸ“‹", "text": "Task decomposition is the agent's ability to break a high-level goal into an ordered list of subtasks, then execute them sequentially or in parallel." },
        "discovery": { "visual": "ğŸ’¡", "text": "Prompt the agent with: 'Break this task into numbered steps before starting. Execute each step, then move to the next.' The agent generates its own plan, then follows it." },
        "twist": { "visual": "âš¡", "text": "The plan often needs revision mid-execution. Step 3's result might invalidate step 4. Good agents re-plan after unexpected results instead of blindly following the original plan." },
        "climax": { "visual": "ğŸ", "text": "Task decomposition + replanning = adaptive execution. The agent plans ahead, acts, evaluates, and adjusts. This is the core of sophisticated agent behavior." },
        "punchline": { "visual": "ğŸ¬", "text": "Divide and conquer. The best agents plan before they act, and replan when reality surprises them." }
      },
      "quiz": {
        "question": "Why is replanning important in task decomposition?",
        "options": [
          "It's not â€” agents should always follow the original plan",
          "Intermediate results may invalidate later steps, requiring adaptation",
          "It reduces the number of subtasks",
          "It eliminates the need for tools"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t27-conversation-memory",
      "chapter_id": "ai--ai-agents--ch04-memory",
      "title": "Conversation Memory",
      "description": "Maintaining coherent multi-turn conversations within an agent session.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ’¬", "text": "Turn 1: 'Book me a flight to Paris.' Turn 5: 'What about hotels there?' The agent asks: 'Hotels where?' It forgot Paris." },
        "buildup": { "visual": "ğŸ“œ", "text": "Conversation memory keeps the full message history available to the agent. Each new turn includes all previous turns as context." },
        "discovery": { "visual": "ğŸ’¡", "text": "The simplest approach: append every user message and agent response to a messages array. Send the full array with each LLM call. The model sees the complete conversation." },
        "twist": { "visual": "âš¡", "text": "Long conversations hit the context window limit. Use a sliding window (last N messages), summary-based compression (summarize old messages), or a hybrid of both." },
        "climax": { "visual": "ğŸ", "text": "Good conversation memory creates the illusion of continuous understanding. The user feels like they're talking to something that remembers, even though each call is stateless." },
        "punchline": { "visual": "ğŸ¬", "text": "Memory makes agents feel intelligent. Without it, every turn is a fresh stranger." }
      },
      "quiz": {
        "question": "What happens when conversation history exceeds the context window?",
        "options": [
          "The model automatically summarizes old messages",
          "You must use a sliding window or summary compression to fit within limits",
          "The conversation restarts automatically",
          "The context window expands on demand"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t28-knowledge-retrieval",
      "chapter_id": "ai--ai-agents--ch04-memory",
      "title": "Knowledge Retrieval",
      "description": "Giving agents access to external knowledge bases for grounded answers.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“š", "text": "A customer support agent knows the product documentation â€” all 500 pages of it. How? It doesn't memorize; it searches." },
        "buildup": { "visual": "ğŸ”", "text": "Knowledge retrieval gives agents a search tool backed by a vector database or traditional search index. The agent queries for relevant information before answering." },
        "discovery": { "visual": "ğŸ’¡", "text": "The agent decides WHEN to search (not every turn needs retrieval), WHAT to search for (it formulates the query), and HOW to use results (incorporating retrieved context into its response)." },
        "twist": { "visual": "âš¡", "text": "Agents that always search are slow and expensive. Agents that never search hallucinate. The skill is knowing when retrieval is needed â€” and the agent can learn this through examples in its system prompt." },
        "climax": { "visual": "ğŸ", "text": "A knowledge-augmented agent combines reasoning with grounded information. It can answer 'What's our refund policy for enterprise accounts?' by searching docs, not guessing." },
        "punchline": { "visual": "ğŸ¬", "text": "The smartest agent isn't the one that knows everything â€” it's the one that knows when to look things up." }
      },
      "quiz": {
        "question": "How does an agent decide when to use knowledge retrieval?",
        "options": [
          "It searches on every single turn regardless",
          "It never searches â€” it uses training data only",
          "It evaluates whether the question requires external information before deciding to search",
          "A separate model makes the search decision"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-agents--t29-supervisor-patterns",
      "chapter_id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Supervisor Patterns",
      "description": "Using a supervisor agent to orchestrate and coordinate worker agents.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ‘”", "text": "Three specialist agents: one for search, one for analysis, one for writing. Who decides the order? Who resolves conflicts? Enter the supervisor." },
        "buildup": { "visual": "ğŸ—ï¸", "text": "A supervisor pattern has one orchestrator agent that receives the task, breaks it into subtasks, assigns each to the right specialist agent, and combines their outputs." },
        "discovery": { "visual": "ğŸ’¡", "text": "The supervisor doesn't do the work â€” it delegates. 'Search agent: find the top 5 competitors. Analysis agent: compare their pricing. Writer agent: draft the executive summary.'" },
        "twist": { "visual": "âš¡", "text": "The supervisor must handle failures: if the search agent returns garbage, do you retry, ask a different agent, or fail gracefully? Error handling at the orchestration layer is critical." },
        "climax": { "visual": "ğŸ", "text": "Supervisor patterns scale naturally: add new specialist agents without changing the orchestrator. The supervisor routes based on capabilities, not hardcoded logic." },
        "punchline": { "visual": "ğŸ¬", "text": "Every great team needs a coordinator. Multi-agent systems are no different." }
      },
      "quiz": {
        "question": "What is the role of a supervisor agent?",
        "options": [
          "To perform all tasks by itself",
          "To delegate subtasks to specialist agents and coordinate their outputs",
          "To replace human oversight entirely",
          "To reduce the number of LLM calls to zero"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t30-agent-handoffs",
      "chapter_id": "ai--ai-agents--ch05-multi-agent-systems",
      "title": "Agent Handoffs",
      "description": "Transferring context and control between agents seamlessly.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ¤", "text": "A triage agent determines the user needs billing help. It hands off to the billing agent. But the billing agent doesn't know the user's name or what they already asked." },
        "buildup": { "visual": "ğŸ“¦", "text": "Handoffs transfer control from one agent to another. The critical question: what context travels with the handoff? Too little and the new agent is clueless. Too much and you waste tokens." },
        "discovery": { "visual": "ğŸ’¡", "text": "A good handoff payload includes: a summary of the conversation so far, the specific task being delegated, and any relevant extracted data (user ID, issue type, etc.)." },
        "twist": { "visual": "âš¡", "text": "Handoff failures are invisible to users â€” they just feel like the system forgot everything. Test handoffs specifically: does the receiving agent have enough context to continue naturally?" },
        "climax": { "visual": "ğŸ", "text": "Design a handoff protocol: structured payload, required fields, optional context, and a return path so the new agent can hand back if needed." },
        "punchline": { "visual": "ğŸ¬", "text": "A seamless handoff feels like one agent. A bad one feels like being transferred to 5 departments." }
      },
      "quiz": {
        "question": "What is the most important element of an agent handoff?",
        "options": [
          "The speed of the transfer",
          "Passing sufficient conversation context and task details to the receiving agent",
          "Using the same model for both agents",
          "Clearing all previous conversation history"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t31-output-validation",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Output Validation",
      "description": "Checking agent outputs before they reach the user.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "The agent generates a SQL query and runs it. It works â€” but it returns every customer's email address. No one checked whether the output was safe before executing." },
        "buildup": { "visual": "ğŸ›¡ï¸", "text": "Output validation is a safety layer between the agent's response and the user (or system). It checks for correctness, safety, and policy compliance." },
        "discovery": { "visual": "ğŸ’¡", "text": "Validate at multiple levels: schema validation (is the output well-formed?), content validation (does it contain PII or harmful content?), and business logic validation (does the action make sense?)." },
        "twist": { "visual": "âš¡", "text": "You can use a second, cheaper LLM as a validator: 'Does this response contain personal information? Does it match our tone guidelines? Is it factually consistent with the provided context?'" },
        "climax": { "visual": "ğŸ", "text": "Think of output validation as the agent's editor. The agent drafts; the validator reviews. Only approved outputs reach the user." },
        "punchline": { "visual": "ğŸ¬", "text": "Trust but verify. Every agent output should pass through a checkpoint." }
      },
      "quiz": {
        "question": "What are the levels of agent output validation?",
        "options": [
          "Only spell-checking",
          "Schema, content safety, and business logic validation",
          "Just checking for JSON syntax",
          "There is no need for output validation"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t32-observability-and-tracing",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Observability & Tracing",
      "description": "Logging and monitoring every step of an agent's execution.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "An agent gave a wrong answer to a customer. Your boss asks: 'What happened?' Without tracing, you have no idea which tool it called, what it retrieved, or why it decided that." },
        "buildup": { "visual": "ğŸ”¬", "text": "Agent observability means logging every step: each LLM call (prompt + response), each tool invocation (input + output), each decision point, and the final result." },
        "discovery": { "visual": "ğŸ’¡", "text": "Tools like LangSmith, Langfuse, and Arize let you trace agent runs end-to-end. Each run becomes a tree of steps you can inspect, replay, and debug." },
        "twist": { "visual": "âš¡", "text": "Tracing also reveals cost and performance: which steps are slow? Which tools fail most? Where are tokens wasted? Observability drives optimisation." },
        "climax": { "visual": "ğŸ", "text": "Production agents without observability are black boxes. With tracing, every failure becomes a learning opportunity, and every success can be replicated." },
        "punchline": { "visual": "ğŸ¬", "text": "You can't fix what you can't see. Trace everything." }
      },
      "quiz": {
        "question": "What does agent tracing help you debug?",
        "options": [
          "Only the final output",
          "Each step: LLM calls, tool invocations, decisions, and results",
          "Only the cost of the run",
          "The model's training data"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-agents--t33-agent-benchmarking",
      "chapter_id": "ai--ai-agents--ch06-reliability-and-safety",
      "title": "Agent Benchmarking",
      "description": "Measuring agent performance across a standardised set of tasks.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ğŸ‹ï¸", "text": "You upgraded the model from GPT-4 to GPT-4o-mini. It's cheaper â€” but is the agent still good? Without benchmarks, you're guessing." },
        "buildup": { "visual": "ğŸ“‹", "text": "Agent benchmarks are curated sets of tasks with expected outcomes. Run the agent against all tasks, score the results, and compare across model versions, prompt changes, or architecture tweaks." },
        "discovery": { "visual": "ğŸ’¡", "text": "Build benchmarks from real user queries: 50-100 representative tasks with expected final answers. Score on task completion rate, step efficiency, cost per task, and error rate." },
        "twist": { "visual": "âš¡", "text": "Agent performance is non-deterministic â€” run each benchmark task 3-5 times and report the median. A single run can be misleadingly good or bad." },
        "climax": { "visual": "ğŸ", "text": "Benchmarks are your safety net for change. Swap a model? Run the benchmark. Change a prompt? Run the benchmark. Add a tool? Run the benchmark." },
        "punchline": { "visual": "ğŸ¬", "text": "If you can't measure it, you can't improve it. Benchmark your agents." }
      },
      "quiz": {
        "question": "Why should agent benchmarks be run multiple times per task?",
        "options": [
          "To increase API costs",
          "Because agent behavior is non-deterministic, and single runs can be misleading",
          "To train the model on the benchmark",
          "Because the first run always fails"
        ],
        "correct": 1
      }
    }
  ]
}
