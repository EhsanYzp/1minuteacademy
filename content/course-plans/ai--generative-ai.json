{
  "categoryId": "ai",
  "subject": "AI & Agents",
  "courseId": "ai--generative-ai",
  "courseTitle": "Generative AI",
  "emoji": "ğŸ¨",
  "color": "#DB2777",
  "requireAuthoredStory": true,
  "chapters": [
    { "id": "ai--generative-ai--ch01-foundations", "title": "Generative Foundations", "position": 1 },
    { "id": "ai--generative-ai--ch02-text", "title": "Text Generation", "position": 2 },
    { "id": "ai--generative-ai--ch03-image", "title": "Image Generation", "position": 3 },
    { "id": "ai--generative-ai--ch04-audio-video", "title": "Audio & Video", "position": 4 },
    { "id": "ai--generative-ai--ch05-applications", "title": "Applications & Workflows", "position": 5 },
    { "id": "ai--generative-ai--ch06-risks", "title": "Risks & Responsibility", "position": 6 }
  ],
  "topics": [
    {
      "chapter_id": "ai--generative-ai--ch01-foundations",
      "title": "What Makes AI 'Generative'?",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "Traditional AI classifies cats vs dogs. Generative AI creates a new cat that never existed.", "visual": "ğŸ±" },
        "buildup": { "text": "Generative models learn the distribution of training data, then sample new examples from it.", "visual": "ğŸ“Š" },
        "discovery": { "text": "They produce: text, images, audio, video, code, 3D models, and even molecules.", "visual": "ğŸ§ª" },
        "twist": { "text": "Generation looks creative but it's pattern interpolationâ€”remixing learned structures.", "visual": "ğŸ”€" },
        "climax": { "text": "The breakthrough: models large enough to generate coherent, high-quality output at scale.", "visual": "ğŸ“ˆ" },
        "punchline": { "text": "Not intelligence. Pattern synthesis at scale.", "visual": "âœ¨" }
      },
      "quiz": {
        "question": "What does a generative model learn?",
        "options": ["The distribution of training data to sample new examples", "Fixed classification rules", "Exact copies of training data"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch01-foundations",
      "title": "Autoencoders: Compress and Reconstruct",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Squeeze an image through a tiny bottleneck, then rebuild it. What survives is the essence.", "visual": "ğŸ—œï¸" },
        "buildup": { "text": "Autoencoders have an encoder (compress) and decoder (reconstruct) with a narrow middle layer.", "visual": "ğŸ“" },
        "discovery": { "text": "The bottleneck forces the model to learn a compact, meaningful representation.", "visual": "ğŸ’" },
        "twist": { "text": "Standard autoencoders don't generate well. VAEs add randomness to the bottleneck.", "visual": "ğŸ²" },
        "climax": { "text": "VAEs (variational autoencoders) enable smooth interpolation between data points.", "visual": "ğŸŒŠ" },
        "punchline": { "text": "Compress to understand. Sample to create.", "visual": "ğŸ”®" }
      },
      "quiz": {
        "question": "What do VAEs add to standard autoencoders?",
        "options": ["Randomness in the bottleneck for generation", "More layers", "Faster training"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch01-foundations",
      "title": "The GAN Framework Revisited",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "A counterfeiter vs a detective. The counterfeiter keeps improving until the detective gives up.", "visual": "ğŸ­" },
        "buildup": { "text": "GANs use adversarial training: generator creates, discriminator judges, both improve.", "visual": "âš”ï¸" },
        "discovery": { "text": "Nash equilibrium: the generator produces images the discriminator can't distinguish from real.", "visual": "ğŸ¤" },
        "twist": { "text": "Training instability plagued early GANs. Wasserstein loss and spectral normalization helped.", "visual": "ğŸ“‰" },
        "climax": { "text": "GANs excel at sharp, photorealistic output but are largely replaced by diffusion models now.", "visual": "ğŸ”„" },
        "punchline": { "text": "The rivalry that birthed an era of generation.", "visual": "ğŸ›ï¸" }
      },
      "quiz": {
        "question": "What stabilized GAN training?",
        "options": ["Wasserstein loss and spectral normalization", "Larger datasets only", "Removing the discriminator"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch01-foundations",
      "title": "Diffusion Models: The New Default",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Add noise until an image becomes static. Learn to reverse it. That's diffusion.", "visual": "ğŸ“º" },
        "buildup": { "text": "Forward process: gradually destroy data with Gaussian noise over T timesteps.", "visual": "ğŸŒ«ï¸" },
        "discovery": { "text": "Reverse process: a neural network learns to predict and remove the noise at each step.", "visual": "ğŸ§¹" },
        "twist": { "text": "Latent diffusion (Stable Diffusion) runs in compressed spaceâ€”10Ã— faster than pixel-level.", "visual": "âš¡" },
        "climax": { "text": "Diffusion models are now the backbone of DALL-E 3, Midjourney, and Stable Diffusion.", "visual": "ğŸ¨" },
        "punchline": { "text": "Destroy systematically. Reconstruct creatively.", "visual": "ğŸ”„" }
      },
      "quiz": {
        "question": "What makes latent diffusion faster?",
        "options": ["It operates in compressed latent space instead of pixel space", "It uses fewer timesteps", "It skips the reverse process"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch01-foundations",
      "title": "Flow Matching: Straight-Line Generation",
      "difficulty": "Premium",
      "story": {
        "hook": { "text": "What if instead of T noisy steps, you could go from noise to image in a straight line?", "visual": "ğŸ“" },
        "buildup": { "text": "Flow matching learns a velocity field that transports noise to data in continuous time.", "visual": "ğŸŒŠ" },
        "discovery": { "text": "Unlike diffusion, flow paths can be straightâ€”faster sampling with fewer function evaluations.", "visual": "âš¡" },
        "twist": { "text": "Stable Diffusion 3 and Flux switched from DDPM-style diffusion to flow matching.", "visual": "ğŸ”€" },
        "climax": { "text": "Flow matching offers a cleaner theoretical framework and better training stability.", "visual": "ğŸ“" },
        "punchline": { "text": "Why zigzag when you can go straight?", "visual": "â¡ï¸" }
      },
      "quiz": {
        "question": "How does flow matching differ from traditional diffusion?",
        "options": ["It uses straight transport paths instead of noisy zigzag steps", "It only works with text", "It requires more compute"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "Autoregressive Text Generation",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "The model writes one word, then uses it to predict the next. Repeat until done.", "visual": "âœï¸" },
        "buildup": { "text": "Autoregressive generation produces tokens left to right, each conditioned on all previous ones.", "visual": "â¡ï¸" },
        "discovery": { "text": "This is how GPT, Claude, and Llama generate every response you read.", "visual": "ğŸ’¬" },
        "twist": { "text": "Errors compound: one bad token early on can derail the entire generation.", "visual": "ğŸ’¥" },
        "climax": { "text": "Techniques like beam search and nucleus sampling balance quality and diversity.", "visual": "âš–ï¸" },
        "punchline": { "text": "One token at a time. That's all it ever does.", "visual": "ğŸ”¤" }
      },
      "quiz": {
        "question": "How does autoregressive generation work?",
        "options": ["Produces tokens one at a time, each conditioned on previous ones", "Generates all tokens at once", "Copies from training data"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "Sampling Strategies: Temperature and Top-p",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Temperature 0: always the same answer. Temperature 2: creative nonsense.", "visual": "ğŸŒ¡ï¸" },
        "buildup": { "text": "Temperature scales the probability distribution before sampling. Lower = more deterministic.", "visual": "ğŸ“Š" },
        "discovery": { "text": "Top-p (nucleus sampling) cuts the tail: only sample from tokens covering p% of probability.", "visual": "âœ‚ï¸" },
        "twist": { "text": "Top-k limits to the k most likely tokens. Combine top-p and top-k for best control.", "visual": "ğŸšï¸" },
        "climax": { "text": "For factual tasks: temperature 0. For creative writing: temperature 0.7â€“1.0 + top-p 0.9.", "visual": "ğŸ¯" },
        "punchline": { "text": "Tune the randomness. Shape the creativity.", "visual": "ğŸ¨" }
      },
      "quiz": {
        "question": "What does a lower temperature produce?",
        "options": ["More deterministic, predictable output", "More creative, diverse output", "Longer responses"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "Structured Output: JSON, XML, and Schemas",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "You need valid JSON. The model returns: {name: 'Alice'â€”missing quotes and closing brace.", "visual": "ğŸ”§" },
        "buildup": { "text": "LLMs generate free-form text. Structured output forces a specific format.", "visual": "ğŸ“‹" },
        "discovery": { "text": "Constrained decoding only allows tokens that keep the output valid at every step.", "visual": "ğŸ”’" },
        "twist": { "text": "OpenAI's JSON mode and tools like Outlines guarantee syntactic validity. No retry needed.", "visual": "âœ…" },
        "climax": { "text": "Define a Pydantic or JSON Schema. Let the library enforce it during generation.", "visual": "ğŸ“" },
        "punchline": { "text": "Don't parse and pray. Constrain and ship.", "visual": "ğŸš€" }
      },
      "quiz": {
        "question": "What does constrained decoding guarantee?",
        "options": ["Output always matches the required format", "Faster generation speed", "Smaller model size"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "Code Generation: From Prompt to Program",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "'Write a Python function to sort a list of dicts by key.' Done in 3 seconds.", "visual": "ğŸ’»" },
        "buildup": { "text": "Code LLMs are trained on billions of lines of code from GitHub, Stack Overflow, and docs.", "visual": "ğŸ“š" },
        "discovery": { "text": "They handle: code completion, bug fixing, refactoring, test generation, and documentation.", "visual": "ğŸ”§" },
        "twist": { "text": "Generated code can contain bugs, security flaws, and license violations. Always review.", "visual": "âš ï¸" },
        "climax": { "text": "GitHub Copilot, Cursor, and Codeium integrate code generation directly into your editor.", "visual": "ğŸ–¥ï¸" },
        "punchline": { "text": "AI writes the first draft. You write the final version.", "visual": "âœï¸" }
      },
      "quiz": {
        "question": "What risk does AI-generated code carry?",
        "options": ["Bugs, security flaws, and license violations", "It's always perfect", "It only works in Python"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "RAG: Grounding Generation in Facts",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "The model makes up a legal citation. With RAG, it quotes the actual statute.", "visual": "âš–ï¸" },
        "buildup": { "text": "RAG retrieves relevant documents from a knowledge base and injects them into the prompt.", "visual": "ğŸ”" },
        "discovery": { "text": "Pipeline: query â†’ embed â†’ retrieve top-k chunks â†’ append to prompt â†’ generate answer.", "visual": "ğŸ”—" },
        "twist": { "text": "Retrieval quality is the bottleneck. Bad retrieval = bad answers, regardless of model quality.", "visual": "ğŸ“‰" },
        "climax": { "text": "Hybrid search + reranking + citation verification = production-grade RAG.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "Don't generate from memory. Generate from evidence.", "visual": "ğŸ“„" }
      },
      "quiz": {
        "question": "What is the biggest bottleneck in RAG systems?",
        "options": ["Retrieval quality determines answer quality", "Model size", "Prompt length"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch03-image",
      "title": "Text-to-Image: How It Works",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Type 'sunset over a cyberpunk city, neon lights, rain.' An image materializes.", "visual": "ğŸŒ†" },
        "buildup": { "text": "A text encoder (CLIP) converts the prompt into an embedding that guides diffusion.", "visual": "ğŸ§­" },
        "discovery": { "text": "Cross-attention layers let the denoising network 'see' the text at every step.", "visual": "ğŸ‘ï¸" },
        "twist": { "text": "Classifier-free guidance (CFG) strengthens text adherenceâ€”higher CFG = more prompt-faithful.", "visual": "ğŸšï¸" },
        "climax": { "text": "SDXL, DALL-E 3, and Midjourney v6 produce near-photorealistic results from text alone.", "visual": "ğŸ“¸" },
        "punchline": { "text": "Words in. Worlds out.", "visual": "ğŸŒ" }
      },
      "quiz": {
        "question": "What does classifier-free guidance control?",
        "options": ["How closely the image follows the text prompt", "The number of diffusion steps", "Image resolution"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch03-image",
      "title": "Image-to-Image: Editing with AI",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Upload a sketch. The model turns it into a photorealistic scene.", "visual": "ğŸ–Œï¸" },
        "buildup": { "text": "Image-to-image starts from a partially noised input image instead of pure noise.", "visual": "ğŸ”„" },
        "discovery": { "text": "The denoising strength controls how much the model changes: low = minor edits, high = full redraw.", "visual": "ğŸšï¸" },
        "twist": { "text": "Inpainting masks specific regions for editing while keeping the rest untouched.", "visual": "ğŸ­" },
        "climax": { "text": "ControlNet adds structural guidance: edges, poses, and depth maps constrain the output.", "visual": "ğŸ“" },
        "punchline": { "text": "Edit images with words and masks. No Photoshop needed.", "visual": "âœ¨" }
      },
      "quiz": {
        "question": "What does ControlNet provide?",
        "options": ["Structural guidance like edges and poses for image generation", "Text-only prompting", "Faster training"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch03-image",
      "title": "Style Transfer and LoRA Customization",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Train on 20 photos of your art style. Now the model generates new art in that style.", "visual": "ğŸ¨" },
        "buildup": { "text": "LoRA (Low-Rank Adaptation) fine-tunes a tiny fraction of model weights for a custom concept.", "visual": "ğŸ”§" },
        "discovery": { "text": "LoRA files are small (10â€“100 MB) and can be swapped in and out of the base model.", "visual": "ğŸ”€" },
        "twist": { "text": "Multiple LoRAs can be combined: one for style, one for character, one for lighting.", "visual": "ğŸ§©" },
        "climax": { "text": "Civitai and HuggingFace host thousands of community LoRAs for every style imaginable.", "visual": "ğŸŒ" },
        "punchline": { "text": "Your style. The model's speed. Together.", "visual": "ğŸ¤" }
      },
      "quiz": {
        "question": "What makes LoRA efficient for customization?",
        "options": ["It only updates a tiny fraction of model weights", "It retrains the entire model", "It doesn't need training data"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch03-image",
      "title": "Upscaling and Super-Resolution",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "A 256Ã—256 image becomes 2048Ã—2048 with sharp details that weren't in the original.", "visual": "ğŸ”" },
        "buildup": { "text": "Super-resolution models hallucinate plausible high-frequency details during upscaling.", "visual": "ğŸ“ˆ" },
        "discovery": { "text": "Real-ESRGAN and diffusion-based upscalers add texture, edges, and detail convincingly.", "visual": "âœ¨" },
        "twist": { "text": "The model invents details. Upscaled medical or forensic images can be misleading.", "visual": "âš ï¸" },
        "climax": { "text": "Tiled upscaling processes large images in overlapping patches to avoid memory limits.", "visual": "ğŸ§©" },
        "punchline": { "text": "More pixels. Invented, not discovered.", "visual": "ğŸ”®" }
      },
      "quiz": {
        "question": "Why can upscaled images be misleading?",
        "options": ["The model invents details that weren't in the original", "They lose all quality", "They only work on faces"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch04-audio-video",
      "title": "AI Music Generation",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "'Lo-fi jazz with rain sounds, 90 BPM.' An original track plays in seconds.", "visual": "ğŸµ" },
        "buildup": { "text": "Music generation models learn patterns from millions of songs across genres and instruments.", "visual": "ğŸ¹" },
        "discovery": { "text": "Suno and Udio generate full songs with vocals, instruments, and structure from text prompts.", "visual": "ğŸ¤" },
        "twist": { "text": "Copyright questions loom: who owns AI-generated music trained on copyrighted songs?", "visual": "Â©ï¸" },
        "climax": { "text": "Music AI is useful for background tracks, prototyping, and inspirationâ€”not replacing artists.", "visual": "ğŸ¨" },
        "punchline": { "text": "Describe the mood. Hear the track.", "visual": "ğŸ§" }
      },
      "quiz": {
        "question": "What legal concern does AI music generation raise?",
        "options": ["Copyright ownership of AI-generated works", "The music is always bad", "It only works in one genre"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch04-audio-video",
      "title": "Voice Cloning and Speech Synthesis",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Three seconds of your voice. Now the AI speaks anything in your exact voice.", "visual": "ğŸ—£ï¸" },
        "buildup": { "text": "Voice cloning extracts speaker characteristics and applies them to synthesized speech.", "visual": "ğŸ§¬" },
        "discovery": { "text": "Zero-shot cloning needs minimal audio. Fine-tuned cloning needs minutes but sounds better.", "visual": "ğŸ¤" },
        "twist": { "text": "Voice cloning enables fraud: fake CEO calls, phishing, and identity theft.", "visual": "ğŸš¨" },
        "climax": { "text": "Defenses: voice watermarking, liveness detection, and speaker verification challenges.", "visual": "ğŸ›¡ï¸" },
        "punchline": { "text": "Your voice is now a clonable asset. Guard it.", "visual": "ğŸ”" }
      },
      "quiz": {
        "question": "What risk does voice cloning create?",
        "options": ["Fraud through fake calls and identity theft", "Better podcast audio", "Slower speech synthesis"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch04-audio-video",
      "title": "Video Generation: Text to Motion",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "'A golden retriever surfing a wave at sunset.' A 4-second video clip appears.", "visual": "ğŸ„" },
        "buildup": { "text": "Video generation extends image diffusion with temporal consistency across frames.", "visual": "ğŸ¬" },
        "discovery": { "text": "Sora, Runway Gen-3, and Kling generate coherent short clips from text descriptions.", "visual": "ğŸ“¹" },
        "twist": { "text": "Temporal coherence is hard: objects morph, physics break, and people gain extra limbs.", "visual": "ğŸ¤ª" },
        "climax": { "text": "Current models handle 4â€“60 seconds. Feature-length video generation is still years away.", "visual": "â±ï¸" },
        "punchline": { "text": "Still images learned to move. Imperfectly, but improving fast.", "visual": "ğŸš€" }
      },
      "quiz": {
        "question": "What is the main challenge in video generation?",
        "options": ["Maintaining temporal consistency across frames", "Generating individual frames", "File compression"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch04-audio-video",
      "title": "3D Generation: Objects from Text",
      "difficulty": "Premium",
      "story": {
        "hook": { "text": "'A medieval sword with a dragon-head pommel.' A 3D model rotates on screen.", "visual": "âš”ï¸" },
        "buildup": { "text": "3D generation creates meshes, point clouds, or NeRFs from text or image prompts.", "visual": "ğŸ§Š" },
        "discovery": { "text": "Score distillation sampling uses a 2D diffusion model to guide 3D shape optimization.", "visual": "ğŸ“" },
        "twist": { "text": "Generated 3D models often have artifacts: holes, flat textures, and the 'Janus problem' (multi-faced).", "visual": "ğŸ‘¹" },
        "climax": { "text": "Instant3D and TripoSR generate usable 3D assets in under a minute from a single image.", "visual": "âš¡" },
        "punchline": { "text": "Flat prompts create 3D worlds. Almost.", "visual": "ğŸŒ" }
      },
      "quiz": {
        "question": "What is the Janus problem in 3D generation?",
        "options": ["Objects having multiple faces or duplicate features", "Slow rendering speed", "Low polygon count"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch05-applications",
      "title": "AI Agents: Models That Take Actions",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "'Book me a flight to Tokyo under $800.' The agent searches, compares, and books.", "visual": "âœˆï¸" },
        "buildup": { "text": "AI agents combine LLMs with tools: web search, APIs, code execution, and databases.", "visual": "ğŸ”§" },
        "discovery": { "text": "The agent loop: observe â†’ think â†’ act â†’ observe result â†’ repeat until goal is achieved.", "visual": "ğŸ”" },
        "twist": { "text": "Agents hallucinate actions too. Without guardrails, they can send wrong emails or delete files.", "visual": "âš ï¸" },
        "climax": { "text": "Frameworks: LangChain, CrewAI, and AutoGen orchestrate multi-step agent workflows.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "Not just answers. Actions.", "visual": "âš¡" }
      },
      "quiz": {
        "question": "What distinguishes AI agents from basic LLMs?",
        "options": ["Agents take actions using tools and APIs", "Agents are always more accurate", "Agents don't use language models"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch05-applications",
      "title": "Chatbots and Conversational AI",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "You message customer support. The 'agent' replies in 2 seconds. It's not human.", "visual": "ğŸ¤–" },
        "buildup": { "text": "Conversational AI uses LLMs to maintain multi-turn dialogues with context memory.", "visual": "ğŸ’¬" },
        "discovery": { "text": "System prompts define personality, boundaries, and domain knowledge for the chatbot.", "visual": "ğŸ“‹" },
        "twist": { "text": "Memory management is key: too much history overflows context, too little loses coherence.", "visual": "ğŸ§ " },
        "climax": { "text": "Handoff to humans when confidence is lowâ€”the bot should know when it doesn't know.", "visual": "ğŸ‘¤" },
        "punchline": { "text": "Talk to a model. Get real answers. Escalate when needed.", "visual": "ğŸ“" }
      },
      "quiz": {
        "question": "What should a chatbot do when uncertain?",
        "options": ["Escalate to a human agent", "Guess confidently", "Shut down"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch05-applications",
      "title": "Content Creation Workflows",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Outline â†’ draft â†’ edit â†’ illustrate â†’ publish. AI assists at every step.", "visual": "ğŸ“" },
        "buildup": { "text": "LLMs generate outlines, first drafts, social media posts, and email campaigns.", "visual": "ğŸ“§" },
        "discovery": { "text": "Image models create illustrations, thumbnails, and ad creatives from text descriptions.", "visual": "ğŸ–¼ï¸" },
        "twist": { "text": "AI-generated content needs human editing: fact-checking, tone adjustment, and brand alignment.", "visual": "âœï¸" },
        "climax": { "text": "The workflow: AI generates 80%, human polishes 20%. Speed Ã— quality.", "visual": "âš¡" },
        "punchline": { "text": "AI is the first draft machine. You're the editor.", "visual": "ğŸ¯" }
      },
      "quiz": {
        "question": "What role should humans play in AI content creation?",
        "options": ["Editing for facts, tone, and brand alignment", "No roleâ€”AI handles everything", "Only writing the prompt"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch05-applications",
      "title": "Synthetic Data Generation",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "No real data? Generate synthetic data that looks real enough to train on.", "visual": "ğŸ­" },
        "buildup": { "text": "Synthetic data generation creates artificial training examples that mimic real distributions.", "visual": "ğŸ“Š" },
        "discovery": { "text": "Use cases: privacy (no real PII), rare events (fraud simulation), and data augmentation.", "visual": "ğŸ”’" },
        "twist": { "text": "Models trained on synthetic data can inherit the generator's biases and blind spots.", "visual": "âš ï¸" },
        "climax": { "text": "Validate synthetic data against real data distributions before using it for training.", "visual": "âœ…" },
        "punchline": { "text": "Fake data, real utility. If done carefully.", "visual": "ğŸ¯" }
      },
      "quiz": {
        "question": "Why is synthetic data useful for privacy?",
        "options": ["It contains no real personal information", "It's always more accurate", "It's smaller in size"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch06-risks",
      "title": "Copyright and Training Data",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "An artist's exact style appears in AI art. They never consented to their work being used.", "visual": "ğŸ¨" },
        "buildup": { "text": "Generative models train on web-scraped data that includes copyrighted works.", "visual": "ğŸŒ" },
        "discovery": { "text": "Lawsuits: NYT vs OpenAI, Getty vs Stability AIâ€”the legal battles have begun.", "visual": "âš–ï¸" },
        "twist": { "text": "Fair use is unclear for AI training. Courts in different countries may rule differently.", "visual": "ğŸŒ" },
        "climax": { "text": "Some models now train on licensed or public domain data to avoid legal risk.", "visual": "ğŸ“œ" },
        "punchline": { "text": "The law hasn't caught up. But it's coming.", "visual": "â³" }
      },
      "quiz": {
        "question": "Why are AI companies facing copyright lawsuits?",
        "options": ["Training on copyrighted works without consent", "Making models too small", "Generating only text"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch06-risks",
      "title": "Watermarking AI-Generated Content",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Is this photo real or AI-generated? A hidden watermark could tell you instantly.", "visual": "ğŸ”" },
        "buildup": { "text": "Watermarking embeds invisible signals in generated content to prove AI origin.", "visual": "ğŸ”¬" },
        "discovery": { "text": "Text watermarks bias token selection in statistically detectable ways.", "visual": "ğŸ“Š" },
        "twist": { "text": "Watermarks can be removed by paraphrasing text or re-encoding images. It's an arms race.", "visual": "âš”ï¸" },
        "climax": { "text": "C2PA and SynthID aim to become standards for AI content provenance tracking.", "visual": "ğŸ·ï¸" },
        "punchline": { "text": "Invisible marks for an invisible problem.", "visual": "ğŸ‘ï¸" }
      },
      "quiz": {
        "question": "How can text watermarks be defeated?",
        "options": ["By paraphrasing the text", "By reading it aloud", "By printing it"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch06-risks",
      "title": "Model Collapse: When AI Trains on AI",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "The internet fills with AI text. New models train on it. Quality spirals downward.", "visual": "ğŸŒ€" },
        "buildup": { "text": "Model collapse happens when generative models train on data produced by earlier models.", "visual": "ğŸ“‰" },
        "discovery": { "text": "Each generation amplifies errors and narrows diversityâ€”like a photocopy of a photocopy.", "visual": "ğŸ“ " },
        "twist": { "text": "Without fresh human-generated data, future models may degrade rather than improve.", "visual": "âš ï¸" },
        "climax": { "text": "Solutions: curate human-only data, label synthetic content, and diversify training sources.", "visual": "ğŸ§¹" },
        "punchline": { "text": "AI eating AI output. The ouroboros of data.", "visual": "ğŸ" }
      },
      "quiz": {
        "question": "What causes model collapse?",
        "options": ["Training on AI-generated data from earlier models", "Using too much human data", "Making models too large"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch06-risks",
      "title": "Environmental Cost of Generative AI",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Generating one image uses as much energy as charging your phone. Multiply by millions.", "visual": "ğŸ”‹" },
        "buildup": { "text": "Training large generative models emits hundreds of tons of CO2 equivalent.", "visual": "ğŸ­" },
        "discovery": { "text": "Inference costs add up: billions of API calls daily across all generative AI services.", "visual": "ğŸ“ˆ" },
        "twist": { "text": "Efficiency improvements (distillation, quantization) offset some growth but usage scales faster.", "visual": "ğŸ“‰" },
        "climax": { "text": "Measure and report compute costs. Choose efficient models when quality is comparable.", "visual": "ğŸ“Š" },
        "punchline": { "text": "Every generation has a carbon cost. Make it count.", "visual": "ğŸŒ¿" }
      },
      "quiz": {
        "question": "What contributes most to generative AI's environmental cost?",
        "options": ["Billions of inference API calls daily", "Model download sizes", "Storing training data"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch02-text",
      "title": "Instruction Tuning: Teaching Models to Follow Orders",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Base GPT-3 rambles endlessly. Instruction-tuned GPT-3.5 answers questions precisely. What changed?", "visual": "ğŸ¯" },
        "buildup": { "text": "Instruction tuning fine-tunes base models on prompt-response pairs to follow user instructions.", "visual": "ğŸ“" },
        "discovery": { "text": "RLHF and DPO further align outputs with human preferences for helpfulness and safety.", "visual": "ğŸ‘¤" },
        "twist": { "text": "Instruction tuning sacrifices some creativity for reliability. The base model is wilder but less useful.", "visual": "ğŸ­" },
        "climax": { "text": "The instruction-tuned version is what users interact with. The base model is the foundation.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "Raw knowledge is useless without the ability to follow directions.", "visual": "ğŸ§­" }
      },
      "quiz": {
        "question": "What does instruction tuning achieve?",
        "options": ["It teaches models to follow user instructions reliably", "It increases model size", "It removes all model knowledge"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch03-image",
      "title": "ControlNet: Guiding Image Generation",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "You want the AI to generate an image in a specific pose. Without guidance, it ignores your layout.", "visual": "ğŸ¨" },
        "buildup": { "text": "ControlNet adds spatial conditioning to diffusion models using edges, depth maps, or poses.", "visual": "ğŸ“" },
        "discovery": { "text": "It takes a reference structure (skeleton, edge map) and generates images matching that structure.", "visual": "ğŸ¦´" },
        "twist": { "text": "ControlNet preserves creativityâ€”same structure, infinite styles. Control without killing imagination.", "visual": "âœ¨" },
        "climax": { "text": "Use cases: consistent character poses, architectural rendering, and product mockups.", "visual": "ğŸ¢" },
        "punchline": { "text": "Guide the canvas. Let creativity fill the rest.", "visual": "ğŸ–Œï¸" }
      },
      "quiz": {
        "question": "What does ControlNet add to image generation?",
        "options": ["Spatial conditioning from reference structures like poses or edges", "Random noise", "Text-only prompts"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch04-audio-video",
      "title": "Music Generation: AI as Composer",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "An AI composed a 3-minute orchestral piece in 10 seconds. A human composer takes weeks.", "visual": "ğŸµ" },
        "buildup": { "text": "Music generation models create melodies, harmonies, and full arrangements from text prompts.", "visual": "ğŸ¹" },
        "discovery": { "text": "Models like MusicGen and Suno learn musical structure from millions of songs and compositions.", "visual": "ğŸ“š" },
        "twist": { "text": "Copyright is murky: who owns AI-generated music? The user, the model creator, or no one?", "visual": "âš–ï¸" },
        "climax": { "text": "Current use: background music, prototyping, and inspiration. Full replacement of composers is distant.", "visual": "ğŸ§" },
        "punchline": { "text": "AI composes the draft. Humans add the soul.", "visual": "â¤ï¸" }
      },
      "quiz": {
        "question": "What is a key challenge with AI-generated music?",
        "options": ["Copyright ownership is unclear", "AI can't generate any music", "It always sounds identical"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--generative-ai--ch05-applications",
      "title": "Code Generation: AI as Developer Assistant",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Copilot suggests 10 lines of code. You accept 7 and fix 3. Net productivity gain: massive.", "visual": "ğŸ’»" },
        "buildup": { "text": "Code generation models autocomplete, explain, refactor, and even write tests from descriptions.", "visual": "ğŸ“" },
        "discovery": { "text": "Models trained on open-source code learn APIs, patterns, and idioms across hundreds of languages.", "visual": "ğŸŒ" },
        "twist": { "text": "Generated code can have bugs, security flaws, or license issues. Review is non-negotiable.", "visual": "ğŸ”" },
        "climax": { "text": "Best use: boilerplate, repetitive tasks, and exploration. Worst use: blindly trusting complex logic.", "visual": "âš–ï¸" },
        "punchline": { "text": "AI writes the first draft. You own the final version.", "visual": "âœï¸" }
      },
      "quiz": {
        "question": "Why must AI-generated code be reviewed?",
        "options": ["It can contain bugs, security flaws, or license issues", "It's always perfect", "Reviews are optional"],
        "correct": 0
      }
    }
  ]
}
