{
  "categoryId": "ai",
  "subject": "AI & Agents",
  "courseId": "ai--ai-ethics-and-society",
  "courseTitle": "AI Ethics and Society",
  "emoji": "âš–ï¸",
  "color": "#7C3AED",
  "requireAuthoredStory": true,
  "chapters": [
    {
      "id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "Fairness and Bias",
      "position": 1
    },
    {
      "id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "Privacy in the Age of AI",
      "position": 2
    },
    {
      "id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "AI and the Future of Work",
      "position": 3
    },
    {
      "id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "Accountability and Transparency",
      "position": 4
    },
    {
      "id": "ai--ai-ethics-and-society--ch05-power",
      "title": "Power and Control",
      "position": 5
    },
    {
      "id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "Governing AI",
      "position": 6
    }
  ],
  "topics": [
    {
      "chapter_id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "When Algorithms Discriminate",
      "story": {
        "hook": { "text": "A hiring algorithm at Amazon downranked every resume that mentioned 'women's' â€” automatically.", "visual": "ğŸ“‹" },
        "buildup": { "text": "The model learned from 10 years of hiring data where men dominated technical roles.", "visual": "ğŸ“Š" },
        "discovery": { "text": "Algorithmic bias happens when training data reflects historical inequalities.", "visual": "âš–ï¸" },
        "twist": { "text": "Amazon scrapped the tool. But how many biased systems are running right now undetected?", "visual": "ğŸ‘ï¸" },
        "climax": { "text": "Bias isn't a bug â€” it's a mirror of the data we feed into these systems.", "visual": "ğŸª" },
        "punchline": { "text": "Fair data in, fair decisions out. But the data is rarely fair.", "visual": "ğŸ¯" }
      },
      "quiz": {
        "question": "Why did Amazon's hiring algorithm discriminate against women?",
        "options": ["It learned from biased historical hiring data", "It was programmed to exclude women", "Women submitted fewer resumes"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "Racial Bias in Facial Recognition",
      "story": {
        "hook": { "text": "A study found facial recognition misidentified Black women 35% of the time. For white men? Under 1%.", "visual": "ğŸ“¸" },
        "buildup": { "text": "Training datasets overrepresented lighter-skinned faces, skewing the model's accuracy.", "visual": "ğŸ“‚" },
        "discovery": { "text": "Joy Buolamwini's Gender Shades study exposed this gap and sparked global awareness.", "visual": "ğŸ”¬" },
        "twist": { "text": "Police departments deployed these systems anyway. Innocent people were arrested.", "visual": "ğŸš”" },
        "climax": { "text": "Several cities have banned facial recognition by police. Others expanded its use.", "visual": "ğŸ›ï¸" },
        "punchline": { "text": "If the training data doesn't see you, the AI doesn't see you.", "visual": "ğŸ‘¤" }
      },
      "quiz": {
        "question": "What caused facial recognition to perform worse on darker skin?",
        "options": ["Training data overrepresented lighter-skinned faces", "The cameras couldn't detect darker skin", "Darker skin has fewer distinguishing features"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "The Problem with Predictive Policing",
      "story": {
        "hook": { "text": "An AI tells police to patrol a neighborhood more. Arrests go up. The AI says 'see, I was right.'", "visual": "ğŸ”„" },
        "buildup": { "text": "Predictive policing uses historical crime data to forecast where crimes will occur.", "visual": "ğŸ“" },
        "discovery": { "text": "But historical data reflects where police looked hardest, not where crime actually happens most.", "visual": "ğŸ”" },
        "twist": { "text": "More patrols in an area find more crime. This feeds back into the model, creating a vicious cycle.", "visual": "ğŸŒ€" },
        "climax": { "text": "Los Angeles and other cities abandoned predictive policing after studies exposed the feedback loop.", "visual": "ğŸš«" },
        "punchline": { "text": "The algorithm didn't predict crime. It perpetuated policing patterns.", "visual": "ğŸ”—" }
      },
      "quiz": {
        "question": "What is the main problem with predictive policing?",
        "options": ["It creates feedback loops from biased historical data", "It is too expensive to implement", "Criminals learn to avoid predicted areas"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "Defining Fairness for Machines",
      "story": {
        "hook": { "text": "Equal accuracy? Equal outcomes? Equal opportunity? There are over 20 mathematical definitions of fairness.", "visual": "ğŸ“" },
        "buildup": { "text": "Researchers have formalized fairness into precise metrics that algorithms can be tested against.", "visual": "ğŸ“Š" },
        "discovery": { "text": "Some definitions contradict each other â€” you literally can't satisfy all of them at once.", "visual": "âŒ" },
        "twist": { "text": "Choosing which fairness metric to use is a human decision, not a technical one.", "visual": "ğŸ¤”" },
        "climax": { "text": "The math is clear. The values behind the math are where the real debate lives.", "visual": "ğŸ’¬" },
        "punchline": { "text": "Fairness isn't a formula. It's a choice.", "visual": "âš–ï¸" }
      },
      "quiz": {
        "question": "Why is defining fairness for AI difficult?",
        "options": ["Different mathematical definitions of fairness can contradict each other", "There is only one definition that everyone agrees on", "Fairness is not measurable"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch01-fairness",
      "title": "Healthcare AI and Unequal Treatment",
      "story": {
        "hook": { "text": "A hospital algorithm ranked Black patients as healthier than equally sick white patients.", "visual": "ğŸ¥" },
        "buildup": { "text": "The system used healthcare spending as a proxy for health needs.", "visual": "ğŸ’°" },
        "discovery": { "text": "Due to systemic inequality, Black patients had less spent on them, so the AI assumed less need.", "visual": "ğŸ“‰" },
        "twist": { "text": "The bias affected 200 million patients across the US healthcare system.", "visual": "ğŸ‡ºğŸ‡¸" },
        "climax": { "text": "Researchers fixed it by replacing the spending proxy with actual health measurements.", "visual": "ğŸ”§" },
        "punchline": { "text": "The wrong proxy turns inequality into a diagnosis.", "visual": "ğŸ©º" }
      },
      "quiz": {
        "question": "Why did the healthcare algorithm underestimate Black patients' needs?",
        "options": ["It used spending as a proxy, which reflected systemic inequality", "It was deliberately programmed to discriminate", "Black patients had fewer medical records"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "Surveillance and the All-Seeing Eye",
      "story": {
        "hook": { "text": "China has 700 million surveillance cameras. AI connects them into one system that tracks everyone.", "visual": "ğŸ“¹" },
        "buildup": { "text": "AI-powered surveillance can identify faces, track movements, and predict behavior at scale.", "visual": "ğŸ”" },
        "discovery": { "text": "Social credit systems use AI to score citizens based on behavior â€” jaywalking, debt, online posts.", "visual": "ğŸ“Š" },
        "twist": { "text": "Western democracies use similar technology for 'smart cities' and 'public safety.'", "visual": "ğŸ™ï¸" },
        "climax": { "text": "The difference between safety and control depends on who holds the camera.", "visual": "ğŸ“·" },
        "punchline": { "text": "Privacy dies one camera at a time.", "visual": "ğŸ‘ï¸" }
      },
      "quiz": {
        "question": "What concern do AI surveillance systems raise?",
        "options": ["They can track and score citizens at massive scale", "They only work in authoritarian countries", "They are too expensive to be widespread"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "Your Data Is the Product",
      "story": {
        "hook": { "text": "Every search, click, and scroll trains an AI model. Your behavior is the raw material.", "visual": "â›ï¸" },
        "buildup": { "text": "Companies collect vast datasets of user behavior to train recommendation and ad-targeting models.", "visual": "ğŸ“Š" },
        "discovery": { "text": "Terms of service agreements grant broad data usage rights that few people read.", "visual": "ğŸ“œ" },
        "twist": { "text": "Your data doesn't just improve your experience â€” it trains models that affect millions of others.", "visual": "ğŸŒ" },
        "climax": { "text": "GDPR and other regulations try to give users control, but enforcement is inconsistent.", "visual": "âš–ï¸" },
        "punchline": { "text": "Free apps aren't free. You pay with your data.", "visual": "ğŸ’³" }
      },
      "quiz": {
        "question": "How do companies use your behavioral data?",
        "options": ["To train AI models for recommendations and ad targeting", "Only to improve your personal experience", "They delete it immediately after collection"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "AI and the Right to Be Forgotten",
      "story": {
        "hook": { "text": "You delete your social media account. But an AI trained on your posts still remembers your patterns.", "visual": "ğŸ—‘ï¸" },
        "buildup": { "text": "Models absorb training data into their weights. Individual data points can't be cleanly extracted.", "visual": "ğŸ§ " },
        "discovery": { "text": "Machine unlearning is an emerging field trying to remove specific data from trained models.", "visual": "ğŸ”¬" },
        "twist": { "text": "Retraining from scratch is the only guaranteed method â€” and it costs millions of dollars.", "visual": "ğŸ’¸" },
        "climax": { "text": "The EU's right to erasure clashes with the technical reality of how AI stores knowledge.", "visual": "ğŸ‡ªğŸ‡º" },
        "punchline": { "text": "Deleting data is easy. Deleting what AI learned from it isn't.", "visual": "ğŸ”„" }
      },
      "quiz": {
        "question": "Why is the right to be forgotten challenging for AI?",
        "options": ["Models can't easily remove specific data from their weights", "Laws don't apply to AI companies", "Users never request data deletion"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "Deepfakes and Consent",
      "story": {
        "hook": { "text": "A student finds a realistic fake video of herself online. She never consented. Nobody told her.", "visual": "ğŸ˜°" },
        "buildup": { "text": "Deepfake technology can generate realistic images and videos of anyone from a few photos.", "visual": "ğŸ“¸" },
        "discovery": { "text": "Non-consensual deepfakes disproportionately target women and are used for harassment.", "visual": "âš ï¸" },
        "twist": { "text": "The technology is free and easy to use. Creating a deepfake takes minutes, not expertise.", "visual": "â±ï¸" },
        "climax": { "text": "Laws are catching up. Several countries now criminalize non-consensual deepfakes.", "visual": "âš–ï¸" },
        "punchline": { "text": "Technology moved faster than consent. The law races to catch up.", "visual": "ğŸƒ" }
      },
      "quiz": {
        "question": "Who are disproportionately targeted by non-consensual deepfakes?",
        "options": ["Women, often for harassment", "Public officials only", "Corporations for financial fraud"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "Which Jobs Will AI Replace?",
      "story": {
        "hook": { "text": "Goldman Sachs predicts 300 million jobs will be affected by AI automation. Affected, not eliminated.", "visual": "ğŸ“‰" },
        "buildup": { "text": "AI automates tasks, not entire jobs. Most jobs are bundles of tasks, some automatable, some not.", "visual": "ğŸ“¦" },
        "discovery": { "text": "Routine, repetitive tasks go first â€” data entry, basic analysis, scheduling.", "visual": "ğŸ”„" },
        "twist": { "text": "Creative and social tasks were supposed to be safe. Then AI started writing and creating art.", "visual": "ğŸ¨" },
        "climax": { "text": "The safest jobs require physical dexterity, emotional intelligence, and novel problem-solving.", "visual": "ğŸ¤" },
        "punchline": { "text": "AI won't replace you. Someone using AI might.", "visual": "ğŸ’¼" }
      },
      "quiz": {
        "question": "What types of tasks does AI automate first?",
        "options": ["Routine and repetitive tasks", "All creative tasks", "Only manual labor"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "The Productivity Paradox",
      "story": {
        "hook": { "text": "Every company deploys AI. Productivity should soar. Instead, gains are modest. Why?", "visual": "ğŸ“Š" },
        "buildup": { "text": "New technology takes decades to reshape workflows. Computers took 20 years to boost productivity.", "visual": "ğŸ–¥ï¸" },
        "discovery": { "text": "AI tools save time on individual tasks but organizations haven't redesigned processes around them.", "visual": "ğŸ”§" },
        "twist": { "text": "Workers sometimes spend the saved time checking AI outputs â€” a hidden tax on automation.", "visual": "ğŸ”" },
        "climax": { "text": "The real gains will come when entire workflows, not just tasks, are reimagined.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "The tool is fast. The organization is slow.", "visual": "ğŸ¢" }
      },
      "quiz": {
        "question": "Why hasn't AI dramatically boosted productivity yet?",
        "options": ["Organizations haven't redesigned workflows around AI", "AI tools don't actually work", "Workers refuse to use AI"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "AI and the Skills Gap",
      "story": {
        "hook": { "text": "A factory worker loses her job to a robot. Retraining programs offer coding bootcamps. She's 54.", "visual": "ğŸ‘©â€ğŸ­" },
        "buildup": { "text": "AI displacement creates demand for new skills â€” but displaced workers often can't access retraining.", "visual": "ğŸ“" },
        "discovery": { "text": "The skills gap isn't about intelligence. It's about access, time, and financial safety nets.", "visual": "ğŸ•³ï¸" },
        "twist": { "text": "Ironically, AI tutoring tools could be the best way to retrain displaced workers.", "visual": "ğŸ¤–" },
        "climax": { "text": "Countries investing in lifelong learning programs may weather AI disruption best.", "visual": "ğŸŒ" },
        "punchline": { "text": "The future belongs to those who never stop learning.", "visual": "ğŸ“š" }
      },
      "quiz": {
        "question": "What is the main challenge of the AI skills gap?",
        "options": ["Displaced workers often lack access to retraining", "There are no new jobs being created", "AI cannot be used for education"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "Universal Basic Income and AI",
      "story": {
        "hook": { "text": "If AI creates massive wealth but eliminates millions of jobs, who gets the money?", "visual": "ğŸ’°" },
        "buildup": { "text": "Universal Basic Income proposes giving every citizen a regular payment regardless of employment.", "visual": "ğŸ¦" },
        "discovery": { "text": "Proponents argue AI profits should fund UBI â€” a dividend from automation.", "visual": "ğŸ“ˆ" },
        "twist": { "text": "Critics say UBI removes the incentive to work. Pilots show mixed but promising results.", "visual": "ğŸ”¬" },
        "climax": { "text": "Sam Altman and Elon Musk both support UBI as a response to AI-driven job displacement.", "visual": "ğŸ—£ï¸" },
        "punchline": { "text": "If robots do the work, humans need a new social contract.", "visual": "ğŸ“œ" }
      },
      "quiz": {
        "question": "Why do some tech leaders support Universal Basic Income?",
        "options": ["As a response to potential AI-driven job displacement", "To reduce AI research funding", "To replace all existing social programs"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "When AI Makes a Mistake, Who Is Responsible?",
      "story": {
        "hook": { "text": "A self-driving car hits a pedestrian. Who's liable â€” the driver, the manufacturer, or the algorithm?", "visual": "ğŸš—" },
        "buildup": { "text": "Current legal frameworks weren't built for autonomous decision-making systems.", "visual": "âš–ï¸" },
        "discovery": { "text": "Product liability, negligence, and strict liability each point to different parties.", "visual": "ğŸ‘ˆ" },
        "twist": { "text": "The AI itself can't be held responsible â€” it has no legal personhood. Someone must be accountable.", "visual": "ğŸ¤·" },
        "climax": { "text": "The EU AI Act introduces the concept of AI system provider liability for the first time.", "visual": "ğŸ‡ªğŸ‡º" },
        "punchline": { "text": "AI makes the decision. Humans take the blame.", "visual": "ğŸ¯" }
      },
      "quiz": {
        "question": "Why is AI accountability legally challenging?",
        "options": ["AI has no legal personhood and existing laws weren't designed for it", "AI always makes perfect decisions", "Courts refuse to hear AI-related cases"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "The Black Box Problem",
      "story": {
        "hook": { "text": "A loan application is denied. The bank says 'the algorithm decided.' Nobody can explain why.", "visual": "ğŸ“¦" },
        "buildup": { "text": "Many AI models are black boxes â€” they produce outputs but can't explain their reasoning.", "visual": "ğŸ•³ï¸" },
        "discovery": { "text": "Explainable AI (XAI) techniques try to reveal which factors most influenced a decision.", "visual": "ğŸ”¬" },
        "twist": { "text": "Simpler models are more explainable but less accurate. There's a trade-off.", "visual": "âš–ï¸" },
        "climax": { "text": "Regulations now require explanations for automated decisions that affect people's lives.", "visual": "ğŸ“‹" },
        "punchline": { "text": "If you can't explain it, should you deploy it?", "visual": "ğŸ¤”" }
      },
      "quiz": {
        "question": "What is the black box problem in AI?",
        "options": ["Models produce decisions without explainable reasoning", "AI hardware is literally black", "Users can't see the computer running the model"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "Auditing AI Systems",
      "story": {
        "hook": { "text": "We audit financial systems annually. Why don't we audit AI systems the same way?", "visual": "ğŸ”" },
        "buildup": { "text": "AI audits test systems for bias, accuracy, security, and compliance with regulations.", "visual": "ğŸ“‹" },
        "discovery": { "text": "Third-party auditors can probe models with test cases designed to expose failure modes.", "visual": "ğŸ§ª" },
        "twist": { "text": "Many companies resist audits, claiming models are proprietary trade secrets.", "visual": "ğŸ”’" },
        "climax": { "text": "New York City passed a law requiring audits of AI hiring tools. Others may follow.", "visual": "ğŸ›ï¸" },
        "punchline": { "text": "Trust but verify â€” especially when the decision-maker is a machine.", "visual": "âœ…" }
      },
      "quiz": {
        "question": "What do AI audits test for?",
        "options": ["Bias, accuracy, security, and regulatory compliance", "Only processing speed", "The number of users"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "AI in Criminal Justice",
      "story": {
        "hook": { "text": "A judge uses an algorithm to decide bail. The defendant never learns what factors sealed their fate.", "visual": "ğŸ”¨" },
        "buildup": { "text": "Risk assessment tools like COMPAS score defendants' likelihood of reoffending.", "visual": "ğŸ“Š" },
        "discovery": { "text": "ProPublica found COMPAS wrongly labeled Black defendants as high-risk nearly twice as often.", "visual": "ğŸ“°" },
        "twist": { "text": "Northpointe (COMPAS maker) argued the tool was calibrated fairly â€” using a different definition of fair.", "visual": "ğŸ“" },
        "climax": { "text": "Both sides were statistically correct. Different fairness metrics give different verdicts.", "visual": "ğŸ¤¯" },
        "punchline": { "text": "When freedom depends on an algorithm, the definition of fair matters most.", "visual": "âš–ï¸" }
      },
      "quiz": {
        "question": "What controversy surrounds the COMPAS risk assessment tool?",
        "options": ["It showed racial disparities in risk predictions", "It was too expensive for courts", "It always recommended maximum sentences"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch05-power",
      "title": "Who Controls the Foundation Models?",
      "story": {
        "hook": { "text": "Five companies control the most powerful AI models on Earth. That's a lot of power for a few hands.", "visual": "ğŸ¢" },
        "buildup": { "text": "Training frontier models costs hundreds of millions, limiting who can compete.", "visual": "ğŸ’°" },
        "discovery": { "text": "OpenAI, Google, Anthropic, Meta, and a few others set the capabilities and limits of AI.", "visual": "ğŸ”‘" },
        "twist": { "text": "Their safety decisions, content policies, and access rules shape what billions of people can do.", "visual": "ğŸ“‹" },
        "climax": { "text": "Open-source models redistribute power, but frontier capabilities remain concentrated.", "visual": "âš–ï¸" },
        "punchline": { "text": "Whoever builds the model writes the rules.", "visual": "ğŸ“" }
      },
      "quiz": {
        "question": "Why is AI power concentrated in few companies?",
        "options": ["Training frontier models costs hundreds of millions of dollars", "Only a few people understand AI", "Governments banned small AI companies"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch05-power",
      "title": "AI and Misinformation at Scale",
      "story": {
        "hook": { "text": "AI can generate a thousand fake news articles in the time it takes a journalist to write one real one.", "visual": "ğŸ“°" },
        "buildup": { "text": "Language models make mass-producing convincing misinformation trivially easy.", "visual": "ğŸ­" },
        "discovery": { "text": "Fake articles, reviews, social media posts, and even scientific abstracts can be auto-generated.", "visual": "ğŸ¤–" },
        "twist": { "text": "Detecting AI-generated text is unreliable. Detectors have high false-positive rates.", "visual": "âŒ" },
        "climax": { "text": "The information ecosystem may shift from 'is this true?' to 'who published this?'", "visual": "ğŸ”" },
        "punchline": { "text": "When everything can be faked, trust becomes the real currency.", "visual": "ğŸª™" }
      },
      "quiz": {
        "question": "Why is AI-generated misinformation hard to combat?",
        "options": ["AI text detectors have high false-positive rates", "AI can only produce obviously fake content", "Nobody is trying to detect it"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch05-power",
      "title": "AI in Warfare",
      "story": {
        "hook": { "text": "Autonomous drones can select and engage targets without a human pressing the trigger.", "visual": "âœˆï¸" },
        "buildup": { "text": "Military AI includes target recognition, drone swarms, cyber warfare, and logistics optimization.", "visual": "ğŸ¯" },
        "discovery": { "text": "Lethal Autonomous Weapons Systems (LAWS) can make kill decisions faster than humans can intervene.", "visual": "âš¡" },
        "twist": { "text": "The UN has debated banning LAWS for years. Major military powers have blocked agreements.", "visual": "ğŸ‡ºğŸ‡³" },
        "climax": { "text": "The race to deploy autonomous weapons is accelerating despite ethical objections.", "visual": "ğŸƒ" },
        "punchline": { "text": "The hardest ethical question: who decides who lives and dies?", "visual": "â“" }
      },
      "quiz": {
        "question": "What are LAWS in the context of military AI?",
        "options": ["Lethal Autonomous Weapons Systems that can make kill decisions", "Legal frameworks for AI warfare", "Laws governing AI development"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch05-power",
      "title": "The Digital Divide and AI Access",
      "story": {
        "hook": { "text": "AI tutors could transform education in rural Kenya. But there's no reliable internet.", "visual": "ğŸŒ" },
        "buildup": { "text": "AI benefits concentrate in wealthy nations and tech hubs, widening global inequality.", "visual": "ğŸ“ˆ" },
        "discovery": { "text": "Languages like Swahili and Bengali have far fewer AI resources than English or Mandarin.", "visual": "ğŸ—£ï¸" },
        "twist": { "text": "Open-source and on-device models could democratize access â€” if infrastructure catches up.", "visual": "ğŸ“±" },
        "climax": { "text": "The countries that need AI most may be the last to get it.", "visual": "â³" },
        "punchline": { "text": "AI amplifies what you have. If you have little, it amplifies that too.", "visual": "ğŸ”Š" }
      },
      "quiz": {
        "question": "What is the digital divide in AI?",
        "options": ["AI benefits concentrate in wealthy nations with infrastructure", "Everyone has equal access to AI tools", "Only programmers can use AI"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "The EU AI Act: A Global First",
      "story": {
        "hook": { "text": "In 2024, the EU passed the world's first comprehensive AI law. It classifies AI by risk level.", "visual": "ğŸ‡ªğŸ‡º" },
        "buildup": { "text": "The AI Act bans some uses outright â€” social scoring, real-time mass surveillance by police.", "visual": "ğŸš«" },
        "discovery": { "text": "High-risk systems like hiring tools and medical devices must meet strict transparency requirements.", "visual": "ğŸ“‹" },
        "twist": { "text": "Critics say it stifles innovation. Supporters say it protects fundamental rights.", "visual": "âš”ï¸" },
        "climax": { "text": "Other countries are using it as a template â€” the 'Brussels Effect' shapes global AI regulation.", "visual": "ğŸŒ" },
        "punchline": { "text": "Europe regulated first. The world is watching.", "visual": "ğŸ‘€" }
      },
      "quiz": {
        "question": "What does the EU AI Act do?",
        "options": ["Classifies AI by risk level and regulates accordingly", "Bans all artificial intelligence in Europe", "Requires all AI to be open source"],
        "correct": 0
      },
      "is_free": true
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "Self-Regulation vs Government Oversight",
      "story": {
        "hook": { "text": "AI companies say 'trust us to self-regulate.' History suggests that rarely works.", "visual": "ğŸ¤" },
        "buildup": { "text": "Tech firms publish AI principles and form safety teams to self-police.", "visual": "ğŸ“œ" },
        "discovery": { "text": "But when safety conflicts with profit, safety teams get sidelined or disbanded.", "visual": "ğŸ’¸" },
        "twist": { "text": "OpenAI's safety team exodus in 2024 showed the tension between safety and commercial pressure.", "visual": "ğŸšª" },
        "climax": { "text": "Most experts now agree that some government oversight is necessary alongside self-regulation.", "visual": "ğŸ›ï¸" },
        "punchline": { "text": "The fox shouldn't guard the henhouse alone.", "visual": "ğŸ¦Š" }
      },
      "quiz": {
        "question": "Why is self-regulation of AI considered insufficient?",
        "options": ["Safety often loses when it conflicts with profit", "Companies always prioritize safety", "Government has no role in technology"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "AI Safety Research",
      "story": {
        "hook": { "text": "A research lab's mission: make sure AI doesn't accidentally destroy humanity. No pressure.", "visual": "ğŸ”¬" },
        "buildup": { "text": "AI safety studies alignment â€” making sure AI systems do what humans actually want.", "visual": "ğŸ¯" },
        "discovery": { "text": "Key problems include reward hacking, goal misalignment, and power-seeking behavior.", "visual": "âš ï¸" },
        "twist": { "text": "We don't know how to verify that a superintelligent system is truly aligned. Not yet.", "visual": "â“" },
        "climax": { "text": "Organizations like MIRI, Anthropic, and DeepMind dedicate teams to these existential questions.", "visual": "ğŸ¢" },
        "punchline": { "text": "The most important engineering problem: make sure AI wants what we want.", "visual": "ğŸ§­" }
      },
      "quiz": {
        "question": "What is the core problem AI safety research addresses?",
        "options": ["Ensuring AI systems are aligned with human intentions", "Making AI faster and cheaper", "Teaching AI to pass exams"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "Your Role in Shaping AI's Future",
      "story": {
        "hook": { "text": "You're not just a user of AI. You're a citizen in a society being reshaped by it.", "visual": "ğŸ—³ï¸" },
        "buildup": { "text": "Voting, advocacy, consumer choices, and public discourse all shape how AI develops.", "visual": "ğŸ“¢" },
        "discovery": { "text": "Demanding transparency, fairness, and accountability from AI companies drives real change.", "visual": "ğŸ’ª" },
        "twist": { "text": "Opting out isn't realistic â€” AI is embedded in too many systems. Informed engagement is key.", "visual": "ğŸ”—" },
        "climax": { "text": "The decisions made in the next decade will shape AI governance for a century.", "visual": "ğŸ›ï¸" },
        "punchline": { "text": "AI's future isn't written by engineers alone. It's written by all of us.", "visual": "âœï¸" }
      },
      "quiz": {
        "question": "How can individuals shape the future of AI?",
        "options": ["Through voting, advocacy, and demanding accountability", "Only by becoming AI researchers", "By avoiding all AI technology"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch02-privacy",
      "title": "Children's Data and AI: Growing Up Tracked",
      "story": {
        "hook": { "text": "By age 13, the average child has 72 million data points collected about them. AI uses every one.", "visual": "ğŸ‘¶" },
        "buildup": { "text": "Educational apps, smart toys, and social media harvest children's data for AI training and targeting.", "visual": "ğŸ“±" },
        "discovery": { "text": "Children can't meaningfully consent to data collection. Their digital footprint is built before they can choose.", "visual": "ğŸ‘£" },
        "twist": { "text": "COPPA and GDPR try to protect kids, but enforcement is weak and workarounds are common.", "visual": "âš–ï¸" },
        "climax": { "text": "A generation is growing up with AI profiles built from birth. The long-term effects are unknown.", "visual": "â“" },
        "punchline": { "text": "They can't vote or drive, but their data is already working.", "visual": "ğŸ“Š" }
      },
      "quiz": {
        "question": "Why is children's data collection by AI especially concerning?",
        "options": ["Children cannot meaningfully consent to data collection", "Children produce less data than adults", "AI never uses data from children"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch03-jobs",
      "title": "The Gig Economy and AI Management",
      "story": {
        "hook": { "text": "A delivery driver is fired by an algorithm. No human reviewed the decision. No appeal is possible.", "visual": "ğŸ“¦" },
        "buildup": { "text": "Gig platforms use AI to assign work, set pay, rate workers, and terminate contracts.", "visual": "ğŸ“‹" },
        "discovery": { "text": "Algorithmic management can optimize efficiency but strips workers of negotiation and human oversight.", "visual": "ğŸ¤–" },
        "twist": { "text": "Workers game the algorithm in return â€” creating a cat-and-mouse cycle of optimization.", "visual": "ğŸ±" },
        "climax": { "text": "Labor movements worldwide are pushing for transparency and human review of algorithmic work decisions.", "visual": "âœŠ" },
        "punchline": { "text": "Your boss is an algorithm. And it doesn't take complaints.", "visual": "ğŸ’»" }
      },
      "quiz": {
        "question": "What problem does algorithmic management create for gig workers?",
        "options": ["Workers face decisions with no human review or appeal process", "It gives workers too much freedom", "Algorithms always pay workers more than human managers"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch04-accountability",
      "title": "AI Insurance: Who Pays When AI Fails?",
      "story": {
        "hook": { "text": "A self-driving car causes an accident. Who pays â€” the owner, the manufacturer, or the AI developer?", "visual": "ğŸš—" },
        "buildup": { "text": "Traditional liability law assumes a responsible human actor. Autonomous systems break this assumption.", "visual": "âš–ï¸" },
        "discovery": { "text": "New AI insurance products are emerging that cover damages caused by autonomous system failures.", "visual": "ğŸ“‹" },
        "twist": { "text": "Proving an AI caused harm requires understanding its decision process â€” which is often a black box.", "visual": "ğŸ”²" },
        "climax": { "text": "The EU's AI Liability Directive shifts the burden of proof to AI deployers for high-risk systems.", "visual": "ğŸ‡ªğŸ‡º" },
        "punchline": { "text": "When no human decided, someone still needs to pay.", "visual": "ğŸ’°" }
      },
      "quiz": {
        "question": "Why is AI liability legally challenging?",
        "options": ["Traditional law assumes a responsible human actor, which AI disrupts", "AI systems never cause any harm", "Insurance companies refuse all AI-related claims"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch05-power",
      "title": "AI and Democracy: Bots in the Ballot Box",
      "story": {
        "hook": { "text": "An army of AI-generated social media accounts floods an election with disinformation. Voters can't tell what's real.", "visual": "ğŸ—³ï¸" },
        "buildup": { "text": "AI can generate persuasive political content, microtarget voters, and create fake grassroots movements.", "visual": "ğŸ“¢" },
        "discovery": { "text": "Astroturfing with AI is cheap, scalable, and nearly undetectable with current platform moderation.", "visual": "ğŸ¤–" },
        "twist": { "text": "The same tools could enhance democracy â€” summarizing legislation, fact-checking claims in real time.", "visual": "âœ…" },
        "climax": { "text": "The outcome depends on whether AI serves informed citizens or manipulated ones.", "visual": "ğŸ›ï¸" },
        "punchline": { "text": "Democracy requires informed consent. AI can give or take it.", "visual": "ğŸ’¡" }
      },
      "quiz": {
        "question": "How can AI threaten democratic processes?",
        "options": ["By generating scalable disinformation and fake grassroots movements", "By making voting impossible", "AI has no effect on politics"],
        "correct": 0
      },
      "is_free": false
    },
    {
      "chapter_id": "ai--ai-ethics-and-society--ch06-governance",
      "title": "Whistleblowers: Speaking Up About AI Harm",
      "story": {
        "hook": { "text": "A Google researcher publishes a paper about AI bias. Google fires her. The AI ethics world erupts.", "visual": "ğŸ“¢" },
        "buildup": { "text": "Timnit Gebru's firing in 2020 spotlighted tensions between AI ethics researchers and their corporate employers.", "visual": "ğŸ¢" },
        "discovery": { "text": "Her paper warned that large language models amplify bias, consume vast energy, and lack accountability.", "visual": "ğŸ“„" },
        "twist": { "text": "The controversy accelerated AI ethics awareness more than any paper could have done alone.", "visual": "ğŸŒŠ" },
        "climax": { "text": "Protecting AI whistleblowers is now seen as essential for responsible AI development.", "visual": "ğŸ›¡ï¸" },
        "punchline": { "text": "The most ethical act: saying what your employer doesn't want to hear.", "visual": "ğŸ’¡" }
      },
      "quiz": {
        "question": "Why are AI whistleblower protections considered important?",
        "options": ["They ensure harmful AI practices can be reported without retaliation", "Whistleblowers always exaggerate risks", "Companies should never be questioned about AI"],
        "correct": 0
      },
      "is_free": false
    }
  ]
}
