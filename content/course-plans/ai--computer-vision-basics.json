{
  "categoryId": "ai",
  "subject": "AI & Agents",
  "courseId": "ai--computer-vision-basics",
  "courseTitle": "Computer Vision Basics",
  "emoji": "ğŸ‘ï¸",
  "color": "#059669",
  "requireAuthoredStory": true,
  "chapters": [
    { "id": "ai--computer-vision-basics--ch01-pixels", "title": "Pixels & Images", "position": 1 },
    { "id": "ai--computer-vision-basics--ch02-cnns", "title": "Convolutional Neural Networks", "position": 2 },
    { "id": "ai--computer-vision-basics--ch03-detection", "title": "Object Detection", "position": 3 },
    { "id": "ai--computer-vision-basics--ch04-segmentation", "title": "Segmentation & Depth", "position": 4 },
    { "id": "ai--computer-vision-basics--ch05-generative", "title": "Generative Vision", "position": 5 },
    { "id": "ai--computer-vision-basics--ch06-practical", "title": "Real-World Applications", "position": 6 }
  ],
  "topics": [
    {
      "chapter_id": "ai--computer-vision-basics--ch01-pixels",
      "title": "What Is a Digital Image?",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "Zoom in far enough on any photo and you'll see colored squares. Those are pixels.", "visual": "ğŸ”" },
        "buildup": { "text": "A digital image is a grid of pixels. Each pixel stores color valuesâ€”red, green, blue.", "visual": "ğŸŸ¥" },
        "discovery": { "text": "A 1080p image has over 2 million pixels, each with 3 color channels (RGB).", "visual": "ğŸ“Š" },
        "twist": { "text": "To a computer, an image is just a 3D array of numbersâ€”height Ã— width Ã— channels.", "visual": "ğŸ”¢" },
        "climax": { "text": "All computer vision starts here: turning pixel arrays into meaning.", "visual": "ğŸ§ " },
        "punchline": { "text": "Photos aren't pictures to machines. They're numbers.", "visual": "ğŸ”¢" }
      },
      "quiz": {
        "question": "How does a computer represent a color image?",
        "options": ["As a 3D array of numbers (height Ã— width Ã— channels)", "As a text file", "As a single number"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch01-pixels",
      "title": "Color Spaces: RGB, HSV, and Grayscale",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "Why does the same red look different on your phone and your monitor?", "visual": "ğŸ“±" },
        "buildup": { "text": "Color spaces define how numbers map to colors. RGB mixes red, green, and blue.", "visual": "ğŸ¨" },
        "discovery": { "text": "HSV separates hue, saturation, and valueâ€”making color-based filtering much easier.", "visual": "ğŸŒˆ" },
        "twist": { "text": "Grayscale drops color entirely, reducing each pixel to one brightness value.", "visual": "â¬›" },
        "climax": { "text": "Choose the color space that matches your task: HSV for color detection, grayscale for edges.", "visual": "ğŸ¯" },
        "punchline": { "text": "Same image, different encoding, different insights.", "visual": "ğŸ”„" }
      },
      "quiz": {
        "question": "Why is HSV useful in computer vision?",
        "options": ["It separates hue from brightness, simplifying color detection", "It uses less memory", "It's the only format cameras output"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch01-pixels",
      "title": "Image Preprocessing: Resize, Crop, Normalize",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Your model expects 224Ã—224 images. Your photo is 4000Ã—3000. What now?", "visual": "ğŸ“" },
        "buildup": { "text": "Preprocessing transforms raw images into the format the model needs.", "visual": "ğŸ”§" },
        "discovery": { "text": "Resize to match input size. Normalize pixel values to 0â€“1 or mean/std of training data.", "visual": "ğŸ“" },
        "twist": { "text": "Bad preprocessing destroys information. Aggressive cropping cuts off the subject.", "visual": "âœ‚ï¸" },
        "climax": { "text": "Always use the exact same preprocessing for training and inference. Mismatch = broken model.", "visual": "âš ï¸" },
        "punchline": { "text": "Preprocessing is boring until you skip it. Then nothing works.", "visual": "ğŸ’¥" }
      },
      "quiz": {
        "question": "Why must preprocessing be identical for training and inference?",
        "options": ["Mismatch causes the model to see different input distributions", "It doesn't matter", "Only training needs preprocessing"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch01-pixels",
      "title": "Data Augmentation: More Data from Less",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "You have 1,000 images. You need 100,000. Augmentation is your multiplier.", "visual": "âœ–ï¸" },
        "buildup": { "text": "Augmentation creates variations: flips, rotations, crops, color jitter, blur.", "visual": "ğŸ”„" },
        "discovery": { "text": "The model sees each augmented version as a new example, learning to ignore irrelevant changes.", "visual": "ğŸ§ " },
        "twist": { "text": "Over-augment and you create unrealistic images the model wastes capacity learning.", "visual": "ğŸ¤ª" },
        "climax": { "text": "Match augmentations to real-world variation. If cameras shake, add blur. If lighting varies, jitter.", "visual": "ğŸ“¸" },
        "punchline": { "text": "Teach the model what doesn't matter.", "visual": "ğŸ“" }
      },
      "quiz": {
        "question": "What is the purpose of data augmentation?",
        "options": ["Create training variations so the model generalizes better", "Make images look artistic", "Reduce dataset size"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Convolutions: The Core Operation",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Slide a small filter across an image and you get edges, textures, or shapes.", "visual": "ğŸ”²" },
        "buildup": { "text": "A convolution multiplies a small kernel with each image patch and sums the result.", "visual": "ğŸ§®" },
        "discovery": { "text": "Different kernels detect different features: edges, corners, blobs, gradients.", "visual": "ğŸ”¬" },
        "twist": { "text": "CNNs learn their own kernels during trainingâ€”no hand-design needed.", "visual": "ğŸ¤–" },
        "climax": { "text": "Stacking convolution layers builds a hierarchy: edges â†’ textures â†’ parts â†’ objects.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "One simple operation, stacked deep, sees everything.", "visual": "ğŸ‘ï¸" }
      },
      "quiz": {
        "question": "What does a convolution kernel detect?",
        "options": ["Features like edges, textures, and shapes", "The image's file format", "The camera model"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Pooling: Shrinking Feature Maps",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "After convolution, your feature map is huge. Pooling compresses it.", "visual": "ğŸ—œï¸" },
        "buildup": { "text": "Max pooling takes the maximum value in each small patch, reducing spatial dimensions.", "visual": "ğŸ“‰" },
        "discovery": { "text": "Pooling makes the network translation-invariantâ€”a cat is a cat regardless of position.", "visual": "ğŸ±" },
        "twist": { "text": "Modern architectures often use strided convolutions instead of pooling for better results.", "visual": "ğŸ”§" },
        "climax": { "text": "Pooling trades spatial detail for efficiency. For dense tasks like segmentation, you want it back.", "visual": "ğŸ”„" },
        "punchline": { "text": "Less resolution, more meaning.", "visual": "ğŸ¯" }
      },
      "quiz": {
        "question": "What does max pooling do?",
        "options": ["Takes the maximum value in each patch to reduce spatial size", "Adds more pixels", "Changes colors"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Classic Architectures: AlexNet to ResNet",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "In 2012, AlexNet crushed ImageNet by 10%. Deep learning was no longer theoretical.", "visual": "ğŸ†" },
        "buildup": { "text": "VGG went deeper (16â€“19 layers). GoogLeNet went wider with inception modules.", "visual": "ğŸ“" },
        "discovery": { "text": "ResNet added skip connections: input + output of a block. Now you could train 152 layers.", "visual": "ğŸ”—" },
        "twist": { "text": "Without skip connections, very deep networks degradedâ€”more layers made them worse.", "visual": "ğŸ“‰" },
        "climax": { "text": "ResNet's skip connections are now in almost every modern architecture.", "visual": "ğŸŒ" },
        "punchline": { "text": "The shortcut that made depth possible.", "visual": "âš¡" }
      },
      "quiz": {
        "question": "What problem do ResNet skip connections solve?",
        "options": ["Degradation in very deep networks", "Slow training speed", "Overfitting on small datasets"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Transfer Learning: Reuse, Don't Retrain",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "You have 500 images of rare birds. Training from scratch won't work.", "visual": "ğŸ¦" },
        "buildup": { "text": "Transfer learning uses a model pre-trained on millions of images as a starting point.", "visual": "ğŸ—ï¸" },
        "discovery": { "text": "Freeze early layers (they detect edges and textures), fine-tune later layers for your task.", "visual": "â„ï¸" },
        "twist": { "text": "Even a model trained on dogs and cars transfers well to medical images. Features are universal.", "visual": "ğŸ¥" },
        "climax": { "text": "With transfer learning, 500 images can outperform 50,000 from scratch.", "visual": "ğŸ“ˆ" },
        "punchline": { "text": "Don't start from zero. Start from ImageNet.", "visual": "ğŸš€" }
      },
      "quiz": {
        "question": "Why does transfer learning work across different domains?",
        "options": ["Early layer features like edges are universal", "All images look the same", "The model memorizes all possible images"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Vision Transformers: CNNs Meet Attention",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "What if you treated an image like a sentenceâ€”splitting it into patches and using attention?", "visual": "ğŸ§©" },
        "buildup": { "text": "Vision Transformers (ViT) divide images into 16Ã—16 patches and process them with self-attention.", "visual": "ğŸ”²" },
        "discovery": { "text": "ViTs match or beat CNNs on image classification when trained on enough data.", "visual": "ğŸ“Š" },
        "twist": { "text": "ViTs need more data than CNNs to learn spatial structure that convolutions get for free.", "visual": "ğŸ“š" },
        "climax": { "text": "Hybrid models combine CNN stems with transformer bodies for the best of both worlds.", "visual": "ğŸ¤" },
        "punchline": { "text": "Patches are the new pixels. Attention is the new convolution.", "visual": "ğŸ”€" }
      },
      "quiz": {
        "question": "How do Vision Transformers process images?",
        "options": ["Split into patches and process with self-attention", "One pixel at a time", "Using only convolutions"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch03-detection",
      "title": "Bounding Boxes: Where Is the Object?",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "Draw a rectangle around the dog in the photo. That rectangle is a bounding box.", "visual": "ğŸ“¦" },
        "buildup": { "text": "Object detection predicts both the class (dog, car, person) and the bounding box location.", "visual": "ğŸ¯" },
        "discovery": { "text": "A bounding box is defined by four numbers: x, y, width, and height.", "visual": "ğŸ“" },
        "twist": { "text": "Overlapping objects need multiple boxes. Occlusion makes the problem much harder.", "visual": "ğŸ”€" },
        "climax": { "text": "IoU (intersection over union) measures how well a predicted box matches the ground truth.", "visual": "ğŸ“Š" },
        "punchline": { "text": "Find it. Box it. Label it.", "visual": "ğŸ·ï¸" }
      },
      "quiz": {
        "question": "What does IoU measure in object detection?",
        "options": ["Overlap between predicted and ground truth bounding boxes", "Image resolution", "Model training speed"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch03-detection",
      "title": "YOLO: Real-Time Detection",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "YOLO detects 80 object types in a single pass at 60 frames per second.", "visual": "âš¡" },
        "buildup": { "text": "YOLO (You Only Look Once) treats detection as a single regression problem, not a pipeline.", "visual": "ğŸ”„" },
        "discovery": { "text": "The image is divided into a grid. Each cell predicts boxes and class probabilities.", "visual": "ğŸ”²" },
        "twist": { "text": "Older methods (R-CNN) used two stages: propose regions, then classify. Much slower.", "visual": "ğŸ¢" },
        "climax": { "text": "YOLOv8 and beyond add instance segmentation and pose estimation to the same architecture.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "One look. Everything detected.", "visual": "ğŸ‘ï¸" }
      },
      "quiz": {
        "question": "What makes YOLO faster than two-stage detectors?",
        "options": ["It predicts boxes and classes in a single pass", "It uses smaller images", "It skips classification"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch03-detection",
      "title": "Non-Maximum Suppression: Removing Duplicates",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "The detector finds the same cat 15 times. You only need one box.", "visual": "ğŸ±" },
        "buildup": { "text": "Detectors generate many overlapping proposals for the same object.", "visual": "ğŸ“¦" },
        "discovery": { "text": "NMS keeps the highest-confidence box and removes all boxes with high IoU overlap.", "visual": "âœ‚ï¸" },
        "twist": { "text": "In crowded scenes, NMS can accidentally remove valid detections of nearby objects.", "visual": "ğŸ‘¥" },
        "climax": { "text": "Soft-NMS reduces scores instead of removing boxes, handling crowded scenes better.", "visual": "ğŸšï¸" },
        "punchline": { "text": "Keep the best. Suppress the rest.", "visual": "ğŸ†" }
      },
      "quiz": {
        "question": "What problem does NMS solve?",
        "options": ["Removing duplicate overlapping detections", "Adding more detections", "Changing bounding box colors"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch03-detection",
      "title": "Anchor Boxes and Feature Pyramids",
      "difficulty": "Premium",
      "story": {
        "hook": { "text": "How do you detect a tiny bird and a huge truck in the same image?", "visual": "ğŸ¦" },
        "buildup": { "text": "Anchor boxes are predefined shapes at multiple scales that the detector refines.", "visual": "ğŸ“" },
        "discovery": { "text": "Feature pyramids process the image at multiple resolutions, detecting objects at each scale.", "visual": "ğŸ”º" },
        "twist": { "text": "Anchor-free detectors (like FCOS) skip anchors entirely and predict centers directly.", "visual": "ğŸ¯" },
        "climax": { "text": "FPN (feature pyramid network) is now standard in most modern detection architectures.", "visual": "ğŸ—ï¸" },
        "punchline": { "text": "Small and big, all in one forward pass.", "visual": "ğŸ”€" }
      },
      "quiz": {
        "question": "What do feature pyramids enable?",
        "options": ["Detection at multiple scales in one pass", "Faster training only", "Color correction"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch04-segmentation",
      "title": "Semantic Segmentation: Label Every Pixel",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Detection boxes the dog. Segmentation colors every dog pixel and every grass pixel.", "visual": "ğŸ¨" },
        "buildup": { "text": "Semantic segmentation assigns a class label to every single pixel in the image.", "visual": "ğŸ–Œï¸" },
        "discovery": { "text": "U-Net uses an encoder-decoder structure with skip connections for precise pixel labels.", "visual": "ğŸ”—" },
        "twist": { "text": "Semantic segmentation doesn't distinguish instancesâ€”two dogs get the same label.", "visual": "ğŸ•" },
        "climax": { "text": "Used in self-driving (road vs sidewalk), medical imaging (tumor vs tissue), and AR filters.", "visual": "ğŸš—" },
        "punchline": { "text": "Every pixel gets a vote.", "visual": "ğŸ—³ï¸" }
      },
      "quiz": {
        "question": "What does semantic segmentation produce?",
        "options": ["A class label for every pixel in the image", "One label for the whole image", "Bounding boxes only"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch04-segmentation",
      "title": "Instance Segmentation: Who Is Who?",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "There are three people in the photo. Instance segmentation colors each one differently.", "visual": "ğŸ‘¥" },
        "buildup": { "text": "Instance segmentation combines detection (find each object) with segmentation (label its pixels).", "visual": "ğŸ”€" },
        "discovery": { "text": "Mask R-CNN adds a pixel mask branch to the Faster R-CNN detection framework.", "visual": "ğŸ­" },
        "twist": { "text": "It's more expensive than semantic segmentationâ€”each instance needs its own mask prediction.", "visual": "ğŸ’°" },
        "climax": { "text": "SAM (Segment Anything Model) generalized this to any object with zero-shot prompting.", "visual": "âœ¨" },
        "punchline": { "text": "Not just what. Which one.", "visual": "â˜ï¸" }
      },
      "quiz": {
        "question": "How does instance segmentation differ from semantic segmentation?",
        "options": ["It distinguishes between individual objects of the same class", "It only detects one object", "It uses bounding boxes only"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch04-segmentation",
      "title": "Depth Estimation from Single Images",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "One photo. No stereo cameras. The model estimates how far away everything is.", "visual": "ğŸ“" },
        "buildup": { "text": "Monocular depth estimation predicts a depth map from a single 2D image.", "visual": "ğŸ—ºï¸" },
        "discovery": { "text": "Models learn depth cues: relative size, occlusion, texture gradients, and perspective.", "visual": "ğŸ”" },
        "twist": { "text": "Predicted depth is relative, not absoluteâ€”it knows A is farther than B but not exact meters.", "visual": "ğŸ“" },
        "climax": { "text": "DepthAnything and MiDaS produce usable depth maps for AR, robotics, and 3D reconstruction.", "visual": "ğŸ¤–" },
        "punchline": { "text": "Flat photos, deep understanding.", "visual": "ğŸŒŠ" }
      },
      "quiz": {
        "question": "What kind of depth does monocular estimation predict?",
        "options": ["Relative depth, not absolute meters", "Exact distances in centimeters", "Only the closest object"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch04-segmentation",
      "title": "Panoptic Segmentation: The Complete Picture",
      "difficulty": "Premium",
      "story": {
        "hook": { "text": "Sky, road, car #1, car #2, pedestrian #3â€”every pixel labeled, every instance separated.", "visual": "ğŸŒ†" },
        "buildup": { "text": "Panoptic segmentation combines semantic (stuff: sky, road) with instance (things: cars, people).", "visual": "ğŸ”€" },
        "discovery": { "text": "Each pixel gets both a class label and an instance ID if it's a countable object.", "visual": "ğŸ·ï¸" },
        "twist": { "text": "Unifying stuff and things required new metricsâ€”panoptic quality (PQ) replaced mIoU + AP.", "visual": "ğŸ“Š" },
        "climax": { "text": "Self-driving cars use panoptic segmentation for full scene understanding.", "visual": "ğŸš—" },
        "punchline": { "text": "Every pixel accounted for. No exceptions.", "visual": "âœ…" }
      },
      "quiz": {
        "question": "What does panoptic segmentation combine?",
        "options": ["Semantic segmentation and instance segmentation", "Detection and classification only", "Depth and color information"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch05-generative",
      "title": "GANs: Generator vs Discriminator",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "One network makes fake images. Another tries to spot them. Both get better.", "visual": "ğŸ­" },
        "buildup": { "text": "GANs pit a generator (creates images) against a discriminator (detects fakes).", "visual": "âš”ï¸" },
        "discovery": { "text": "Training ends when the discriminator can't tell real from fakeâ€”the generator wins.", "visual": "ğŸ†" },
        "twist": { "text": "GANs are notoriously hard to train: mode collapse, instability, and hyperparameter sensitivity.", "visual": "ğŸ’¥" },
        "climax": { "text": "StyleGAN generates photorealistic faces of people who don't exist.", "visual": "ğŸ‘¤" },
        "punchline": { "text": "Two rivals, one masterpiece.", "visual": "ğŸ–¼ï¸" }
      },
      "quiz": {
        "question": "What is mode collapse in GANs?",
        "options": ["The generator produces only a few similar outputs", "The discriminator always wins", "Training completes instantly"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch05-generative",
      "title": "Diffusion Models: From Noise to Image",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Start with pure static. Remove noise step by step. A photo of a mountain appears.", "visual": "ğŸ“º" },
        "buildup": { "text": "Diffusion models learn to reverse a noise-adding process, recovering the original image.", "visual": "ğŸ”„" },
        "discovery": { "text": "At each step, the model predicts and removes a small amount of noise.", "visual": "ğŸ§¹" },
        "twist": { "text": "Diffusion models are slow (many steps) but produce higher quality than GANs with stable training.", "visual": "â³" },
        "climax": { "text": "Stable Diffusion, DALL-E, and Midjourney all use diffusion at their core.", "visual": "ğŸ¨" },
        "punchline": { "text": "Destroy it with noise. Reconstruct it with learning.", "visual": "âœ¨" }
      },
      "quiz": {
        "question": "How do diffusion models generate images?",
        "options": ["By gradually removing noise from random static", "By copying existing images", "By generating pixels left to right"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch05-generative",
      "title": "Text-to-Image: Prompting Visual Creativity",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Type 'a corgi astronaut on Mars, oil painting.' The model draws it in seconds.", "visual": "ğŸ•" },
        "buildup": { "text": "Text-to-image models use CLIP to align text embeddings with image generation.", "visual": "ğŸ”—" },
        "discovery": { "text": "The text encoder converts your prompt into a vector that guides the diffusion process.", "visual": "ğŸ§­" },
        "twist": { "text": "Prompt engineering matters: word order, adjectives, and style keywords change output dramatically.", "visual": "âœï¸" },
        "climax": { "text": "Negative prompts tell the model what to avoid: 'no blurry, no extra fingers.'", "visual": "ğŸš«" },
        "punchline": { "text": "Words become worlds.", "visual": "ğŸŒ" }
      },
      "quiz": {
        "question": "What role does CLIP play in text-to-image models?",
        "options": ["Aligns text embeddings with image generation", "Stores images in a database", "Compresses the final image"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "Self-Driving Perception Pipeline",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "A car sees a stop sign, a pedestrian, and a lane lineâ€”all in 50 milliseconds.", "visual": "ğŸš—" },
        "buildup": { "text": "Self-driving uses cameras, lidar, and radar fused into a single perception stack.", "visual": "ğŸ“¡" },
        "discovery": { "text": "Tasks run in parallel: detection, segmentation, depth, lane tracking, and traffic sign reading.", "visual": "ğŸ”€" },
        "twist": { "text": "Edge cases kill: unusual lighting, obscured signs, and construction zones break assumptions.", "visual": "ğŸš§" },
        "climax": { "text": "Redundancy is safety: if one sensor fails, others must compensate.", "visual": "ğŸ›¡ï¸" },
        "punchline": { "text": "See everything. Miss nothing. Decide instantly.", "visual": "âš¡" }
      },
      "quiz": {
        "question": "Why do self-driving cars use multiple sensor types?",
        "options": ["For redundancy if one sensor fails", "To reduce cost", "Because cameras don't work at night"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "Medical Image Analysis",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "An AI spots a tumor in an X-ray that two radiologists missed.", "visual": "ğŸ¥" },
        "buildup": { "text": "CV models analyze X-rays, CT scans, MRIs, and pathology slides for disease detection.", "visual": "ğŸ”¬" },
        "discovery": { "text": "Models trained on millions of scans can match or exceed specialist-level accuracy.", "visual": "ğŸ“Š" },
        "twist": { "text": "Medical data is scarce, private, and expensive to labelâ€”requiring specialized techniques.", "visual": "ğŸ”’" },
        "climax": { "text": "FDA approval is required before clinical deployment. Accuracy alone isn't enough.", "visual": "ğŸ“‹" },
        "punchline": { "text": "AI assists the doctor. The doctor makes the call.", "visual": "ğŸ‘¨â€âš•ï¸" }
      },
      "quiz": {
        "question": "What is needed beyond accuracy for medical AI deployment?",
        "options": ["Regulatory approval (e.g., FDA)", "More training data only", "Faster GPUs"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "OCR and Document Understanding",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "Point your phone at a sign in Japanese and it translates to English instantly.", "visual": "ğŸ“±" },
        "buildup": { "text": "OCR (optical character recognition) converts images of text into machine-readable strings.", "visual": "ğŸ“" },
        "discovery": { "text": "Modern OCR handles handwriting, rotated text, curved surfaces, and 100+ languages.", "visual": "ğŸŒ" },
        "twist": { "text": "Document AI goes beyond OCRâ€”it understands tables, forms, and document structure.", "visual": "ğŸ“‹" },
        "climax": { "text": "Invoice processing, receipt scanning, and contract extraction all rely on document vision.", "visual": "ğŸ’¼" },
        "punchline": { "text": "Read any document. Understand the structure.", "visual": "ğŸ‘ï¸" }
      },
      "quiz": {
        "question": "What does OCR convert?",
        "options": ["Images of text into machine-readable strings", "Audio into text", "Code into images"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "Edge Deployment: Vision on Tiny Devices",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Your security camera runs object detection locallyâ€”no cloud, no latency, no privacy risk.", "visual": "ğŸ“¹" },
        "buildup": { "text": "Edge deployment runs CV models on phones, cameras, drones, and microcontrollers.", "visual": "ğŸ“±" },
        "discovery": { "text": "Model compression (pruning, quantization, distillation) fits big models into small chips.", "visual": "ğŸ—œï¸" },
        "twist": { "text": "Edge hardware varies wildly: NVIDIA Jetson, Apple Neural Engine, Coral TPUâ€”each needs optimization.", "visual": "ğŸ”§" },
        "climax": { "text": "ONNX and TFLite provide portable formats for deploying across different edge hardware.", "visual": "ğŸ“¦" },
        "punchline": { "text": "Smart vision, no cloud required.", "visual": "â˜ï¸" }
      },
      "quiz": {
        "question": "Why deploy vision models at the edge?",
        "options": ["Low latency, privacy, and no cloud dependency", "Cloud is always faster", "Edge devices have unlimited compute"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch02-cnns",
      "title": "Batch Normalization in Vision Models",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "Without batch norm, deep CNNs train painfully slowly. With it, they converge in half the time.", "visual": "âš¡" },
        "buildup": { "text": "Batch normalization normalizes layer inputs to zero mean and unit variance during training.", "visual": "ğŸ“Š" },
        "discovery": { "text": "This reduces internal covariate shiftâ€”layers don't need to constantly adapt to changing inputs.", "visual": "ğŸ”„" },
        "twist": { "text": "BatchNorm behaves differently at train vs inference time, which causes subtle bugs in deployment.", "visual": "ğŸ›" },
        "climax": { "text": "LayerNorm and GroupNorm are alternatives that avoid batch-size dependency issues.", "visual": "ğŸ”§" },
        "punchline": { "text": "Normalize the chaos. Train faster.", "visual": "ğŸƒ" }
      },
      "quiz": {
        "question": "What does batch normalization help with?",
        "options": ["Faster training convergence by normalizing layer inputs", "Making images brighter", "Increasing model size"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch03-detection",
      "title": "Anchor Boxes: How Detectors Propose Regions",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "How does a detector know where to look? It starts with thousands of pre-defined anchor boxes.", "visual": "ğŸ“¦" },
        "buildup": { "text": "Anchors are preset bounding boxes of various sizes and aspect ratios scattered across the image.", "visual": "ğŸ“" },
        "discovery": { "text": "The model predicts offsets from anchors rather than raw coordinatesâ€”much easier to learn.", "visual": "ğŸ¯" },
        "twist": { "text": "Too few anchors miss objects. Too many slow inference. Finding the right balance is an art.", "visual": "âš–ï¸" },
        "climax": { "text": "Anchor-free detectors (CenterNet, FCOS) skip anchors entirely by predicting object centers.", "visual": "ğŸ”®" },
        "punchline": { "text": "Start with anchors. Graduate to anchor-free.", "visual": "ğŸ“ˆ" }
      },
      "quiz": {
        "question": "What are anchor boxes in object detection?",
        "options": ["Pre-defined bounding boxes the model refines with offsets", "The final detection outputs", "Image borders"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch04-segmentation",
      "title": "Stereo Vision: Depth from Two Cameras",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "Close one eye: flat world. Open both: depth perception. Stereo vision gives cameras the same trick.", "visual": "ğŸ‘€" },
        "buildup": { "text": "Two cameras capture the same scene from slightly different angles, like human eyes.", "visual": "ğŸ“¸" },
        "discovery": { "text": "Matching pixels between views reveals disparityâ€”the shift encodes distance to each point.", "visual": "ğŸ“" },
        "twist": { "text": "Textureless surfaces (white walls, glass) have no features to match, creating depth holes.", "visual": "ğŸ•³ï¸" },
        "climax": { "text": "Modern stereo combines classical matching with deep learning to fill these gaps.", "visual": "ğŸ§ " },
        "punchline": { "text": "Two views beat one. Depth emerges from difference.", "visual": "ğŸ”" }
      },
      "quiz": {
        "question": "How does stereo vision estimate depth?",
        "options": ["By comparing pixel shifts between two camera views", "Using a single high-res image", "Measuring light intensity"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch05-generative",
      "title": "Image Super-Resolution: Enhancing Detail",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "text": "'Enhance!' the detective says on TV. In 2024, AI actually does itâ€”and it's eerily good.", "visual": "ğŸ”" },
        "buildup": { "text": "Super-resolution upscales low-resolution images by predicting missing pixel details.", "visual": "ğŸ“ˆ" },
        "discovery": { "text": "Models learn from pairs of low-res and high-res images to fill in realistic textures.", "visual": "ğŸ–¼ï¸" },
        "twist": { "text": "The model invents plausible details that may not match reality. It hallucinates textures.", "visual": "ğŸ­" },
        "climax": { "text": "Applications: satellite imagery, medical scans, old photo restoration, and video upscaling.", "visual": "ğŸ›°ï¸" },
        "punchline": { "text": "More pixels, more detailâ€”but some of it is imagined.", "visual": "âœ¨" }
      },
      "quiz": {
        "question": "What is a risk of image super-resolution?",
        "options": ["The model may hallucinate details that weren't in the original", "It always produces perfect results", "It reduces image quality"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "Video Understanding: Beyond Single Frames",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "A photo of a hand near a ball tells you nothing. A video shows the throw, catch, or drop.", "visual": "ğŸ¥" },
        "buildup": { "text": "Video understanding analyzes temporal sequences: actions, events, and scene transitions.", "visual": "ğŸ“¹" },
        "discovery": { "text": "Architectures: 3D CNNs, video transformers, and two-stream networks (spatial + temporal).", "visual": "ğŸ§ " },
        "twist": { "text": "Video models are 10-100Ã— more expensive than image models due to the extra time dimension.", "visual": "ğŸ’°" },
        "climax": { "text": "Applications: surveillance, sports analysis, autonomous driving, and content moderation.", "visual": "ğŸš—" },
        "punchline": { "text": "Time adds context. Video tells the whole story.", "visual": "ğŸ“–" }
      },
      "quiz": {
        "question": "Why are video models more expensive than image models?",
        "options": ["The extra temporal dimension multiplies computation", "Videos have fewer pixels", "Video data is smaller"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch01-pixels",
      "title": "Image Augmentation: More Data for Free",
      "difficulty": "Beginner",
      "story": {
        "hook": { "text": "You have 1,000 cat photos. Flip, rotate, and crop them and now you have 10,000.", "visual": "ğŸ±" },
        "buildup": { "text": "Data augmentation applies random transforms to training images to create synthetic variety.", "visual": "ğŸ”„" },
        "discovery": { "text": "Common augmentations: flips, rotations, color jitter, random crops, and cutout/mixup.", "visual": "ğŸ¨" },
        "twist": { "text": "Bad augmentations hurt: flipping a '6' makes a '9.' Always match augmentations to your domain.", "visual": "âš ï¸" },
        "climax": { "text": "Augmentation is the cheapest way to reduce overfitting and improve generalization.", "visual": "ğŸ“ˆ" },
        "punchline": { "text": "Transform your data. Multiply your model's experience.", "visual": "âœ¨" }
      },
      "quiz": {
        "question": "What is the purpose of image augmentation?",
        "options": ["To increase training data variety and reduce overfitting", "To compress images", "To delete bad images"],
        "correct": 0
      }
    },
    {
      "chapter_id": "ai--computer-vision-basics--ch06-practical",
      "title": "Medical Imaging: AI Assisting Doctors",
      "difficulty": "Advanced",
      "story": {
        "hook": { "text": "An AI spots a tumor a radiologist missed. Another AI flags a healthy scan as cancer. Stakes are high.", "visual": "ğŸ¥" },
        "buildup": { "text": "Medical imaging AI detects anomalies in X-rays, CT scans, MRIs, and pathology slides.", "visual": "ğŸ”¬" },
        "discovery": { "text": "Models are trained on labeled scans from expert radiologistsâ€”expensive, scarce, and sometimes disagreeing.", "visual": "ğŸ“Š" },
        "twist": { "text": "FDA approval, liability, and patient trust add layers far beyond model accuracy.", "visual": "âš–ï¸" },
        "climax": { "text": "The best approach: AI as assistant, not replacement. Flag findings for human review.", "visual": "ğŸ‘¨â€âš•ï¸" },
        "punchline": { "text": "AI sees patterns. Doctors decide treatment.", "visual": "ğŸ¤" }
      },
      "quiz": {
        "question": "How should AI be used in medical imaging?",
        "options": ["As an assistant that flags findings for human review", "As the sole decision maker", "Only for non-critical scans"],
        "correct": 0
      }
    }
  ]
}
