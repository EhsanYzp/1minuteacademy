{
  "categoryId": "ai",
  "subject": "AI",
  "courseId": "ai--ai-product-design",
  "courseTitle": "AI Product Design",
  "emoji": "ğŸ¨",
  "color": "#EF4444",
  "requireAuthoredStory": true,
  "chapters": [
    {
      "id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "Thinking in AI Products",
      "position": 1
    },
    {
      "id": "ai--ai-product-design--ch02-user-experience",
      "title": "User Experience",
      "position": 2
    },
    {
      "id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Trust & Transparency",
      "position": 3
    },
    {
      "id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "Feedback & Iteration",
      "position": 4
    },
    {
      "id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "Deployment Patterns",
      "position": 5
    },
    {
      "id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Measuring Success",
      "position": 6
    }
  ],
  "topics": [
    {
      "id": "ai--ai-product-design--t01-when-to-use-ai",
      "chapter_id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "When to Use AI",
      "description": "Not every product needs AI â€” here's how to tell.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ¤”", "text": "Your team wants to add AI to the product because competitors have it. But the feature they want to build is a dropdown menu. You don't need AI for a dropdown." },
        "buildup": { "visual": "ğŸ“‹", "text": "AI fits when: the task involves judgment, ambiguity, or language understanding. It doesn't fit when: the logic is deterministic, the rules are clear, or a simple algorithm solves the problem." },
        "discovery": { "visual": "ğŸ’¡", "text": "Good AI use cases: content generation, search over unstructured data, classification at scale, summarization, and personalization. Bad use cases: anything that can be solved with if/else or a database query." },
        "twist": { "visual": "âš¡", "text": "Adding AI to a product that doesn't need it increases cost, latency, and unpredictability â€” all for zero user value. The best AI features are the ones users actually need." },
        "climax": { "visual": "ğŸ", "text": "Start with the user problem, not the technology. If AI solves the problem better than simpler alternatives, use it. If not, don't." },
        "punchline": { "visual": "ğŸ¬", "text": "The best AI product decision is sometimes: 'We don't need AI for this.'" }
      },
      "quiz": {
        "question": "When is AI the wrong choice for a product feature?",
        "options": [
          "When the task involves ambiguity",
          "When the logic is deterministic and a simple algorithm solves the problem",
          "When users want natural language interaction",
          "When the dataset is large"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t02-user-problems-first",
      "chapter_id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "User Problems First",
      "description": "Design around user needs, not AI capabilities.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ‘¤", "text": "The team built a stunning AI feature that generates detailed product comparisons. Users wanted a simple 'yes or no' answer: 'Should I buy this?' The feature was impressive and unused." },
        "buildup": { "visual": "ğŸ”", "text": "Start with research: what are users trying to accomplish? What's slow, painful, or confusing? Talk to real users before writing a single prompt." },
        "discovery": { "visual": "ğŸ’¡", "text": "Frame AI features as solutions: 'Users spend 20 minutes searching for answers â†’ an AI assistant answers in 10 seconds.' The value is clear and measurable." },
        "twist": { "visual": "âš¡", "text": "Users don't care that it's AI. They care that it works. If the AI summary is wrong 20% of the time, users would rather have a well-organized FAQ." },
        "climax": { "visual": "ğŸ", "text": "The question isn't 'What can AI do?' It's 'What problem needs solving and does AI solve it better than the alternatives?'" },
        "punchline": { "visual": "ğŸ¬", "text": "Build for the problem, not the technology. Users adopt solutions, not demos." }
      },
      "quiz": {
        "question": "What's the biggest risk of designing AI features without user research?",
        "options": [
          "The AI won't be powerful enough",
          "You build impressive features that don't solve real user problems",
          "Users always love AI features",
          "User research is too expensive"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t03-prototyping-ai-features",
      "chapter_id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "Prototyping AI Features",
      "description": "Testing AI ideas cheaply before building the full system.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ§ª", "text": "Building a full RAG pipeline takes months. Before committing, test the concept: paste documents into ChatGPT, ask the same questions users would, and see if the answers are good enough." },
        "buildup": { "visual": "ğŸ”§", "text": "AI prototyping levels: (1) Manual test with ChatGPT, (2) Script with API calls, (3) Simple app with hardcoded data, (4) Full system. Each level answers a different question." },
        "discovery": { "visual": "ğŸ’¡", "text": "Level 1 answers: 'Can an LLM do this task at all?' Level 2 answers: 'Does it work programmatically?' Level 3 answers: 'Do users find it useful?' Level 4 is production." },
        "twist": { "visual": "âš¡", "text": "Most AI product ideas fail at Level 1. The model can't do the task reliably enough. Better to discover this in an afternoon than after three months of engineering." },
        "climax": { "visual": "ğŸ", "text": "Gate your investment: don't move to Level 2 until Level 1 works. Don't build production until Level 3 shows user demand. Each level is cheaper than the next." },
        "punchline": { "visual": "ğŸ¬", "text": "Prototype before you build. An afternoon of testing saves months of wasted engineering." }
      },
      "quiz": {
        "question": "What should a Level 1 AI prototype answer?",
        "options": [
          "How to deploy the system to production",
          "Whether the LLM can do the task at all with acceptable quality",
          "How many users will pay for the feature",
          "Which vector database to use"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t04-designing-for-uncertainty",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "Designing for Uncertainty",
      "description": "UX patterns for outputs that might be wrong.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "â“", "text": "Traditional software: click 'Calculate Total' â†’ get the exact total. AI software: ask 'What's my total?' â†’ get an answer that's probably right. Designing for 'probably' is different." },
        "buildup": { "visual": "ğŸ¨", "text": "UX patterns for uncertainty: show confidence indicators, present multiple options instead of one answer, make it easy to verify and correct, always provide a non-AI fallback." },
        "discovery": { "visual": "ğŸ’¡", "text": "Email autocomplete is a great model: suggest text, let the user accept or reject with one keystroke. The AI proposes, the human disposes. Low friction, high control." },
        "twist": { "visual": "âš¡", "text": "Users lose trust faster than they gain it. One confident wrong answer damages trust more than ten 'I'm not sure' responses. Design for appropriate confidence levels." },
        "climax": { "visual": "ğŸ", "text": "The best AI UX makes mistakes recoverable: undo buttons, edit options, 'try again' prompts. Users forgive mistakes they can easily fix." },
        "punchline": { "visual": "ğŸ¬", "text": "Design for 'probably right.' Make it easy to verify, easy to correct, easy to try again." }
      },
      "quiz": {
        "question": "What's a key UX principle for AI features that might produce wrong outputs?",
        "options": [
          "Never show the AI output to users",
          "Make mistakes easy to verify, correct, and recover from",
          "Always hide confidence levels",
          "Assume the AI is always correct"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t05-progressive-disclosure",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "Progressive Disclosure",
      "description": "Show simple results first, details on demand.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "The AI returns a 500-word analysis. Users wanted a yes or no. Most close the window without reading. You overwhelmed them." },
        "buildup": { "visual": "ğŸ¯", "text": "Progressive disclosure: show the summary first ('Yes, this product fits your needs'). Let users click to expand ('Here's the detailed comparison'). Let power users access the raw data." },
        "discovery": { "visual": "ğŸ’¡", "text": "Three layers work well: (1) One-line answer, (2) Paragraph explanation, (3) Full details with sources. 80% of users stop at layer 1. The rest drill down as needed." },
        "twist": { "visual": "âš¡", "text": "This also reduces perceived errors: if the one-line answer is wrong, the user sees a short mistake. If they read a 500-word wrong answer, the failure feels much bigger." },
        "climax": { "visual": "ğŸ", "text": "Design AI outputs in layers. Lead with the shortest useful answer. Let curiosity drive users deeper." },
        "punchline": { "visual": "ğŸ¬", "text": "Lead with the answer. Follow with the explanation. Save the details for those who ask." }
      },
      "quiz": {
        "question": "Why does progressive disclosure work well for AI outputs?",
        "options": [
          "It hides all AI output from users",
          "It shows the most useful information first and lets users explore deeper if needed",
          "It requires more API calls",
          "Users always want the maximum detail"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t06-handling-edge-cases",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "Handling Edge Cases",
      "description": "What happens when the AI doesn't have an answer.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸš§", "text": "The user asks about a product that launched yesterday. The AI's knowledge base doesn't have it yet. The AI confidently describes a similar but different product. The user buys the wrong thing." },
        "buildup": { "visual": "ğŸ›¡ï¸", "text": "Edge case design: (1) Out-of-scope queries â†’ 'I can't help with that, try [alternative].' (2) Low confidence â†’ 'I'm not sure, here's what I found.' (3) No data â†’ 'I don't have information about that yet.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Design explicit paths for failure: a 'no answer' response, a human handoff, a search fallback. Every dead end should have a way forward." },
        "twist": { "visual": "âš¡", "text": "The hardest edge case is the one you didn't anticipate. Log all queries and review the ones with no matches or low confidence weekly. New edge cases reveal themselves over time." },
        "climax": { "visual": "ğŸ", "text": "Good AI products handle failure as gracefully as they handle success. The 'I don't know' path should be as polished as the 'here's your answer' path." },
        "punchline": { "visual": "ğŸ¬", "text": "Plan for 'I don't know' as carefully as you plan for 'here's the answer.'" }
      },
      "quiz": {
        "question": "What should an AI product do when it doesn't have an answer?",
        "options": [
          "Generate a plausible-sounding guess",
          "Explicitly admit uncertainty and provide alternative paths (human handoff, search, etc.)",
          "Show a generic error page",
          "Stay silent"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t07-building-user-trust",
      "chapter_id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Building User Trust",
      "description": "Why trust is the currency of AI products.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ¤", "text": "Users try your AI feature once. It gives a wrong answer. They never use it again. Trust in AI products is fragile â€” it's lost fast and rebuilt slowly." },
        "buildup": { "visual": "ğŸ“Š", "text": "Trust builders: consistent accuracy, transparent limitations ('I'm best at X, not great at Y'), sources for claims, and easy correction. Trust breakers: confident wrong answers, hidden limitations, and no way to verify." },
        "discovery": { "visual": "ğŸ’¡", "text": "Be honest about what the AI can't do. A chatbot that says 'I'm not sure about that â€” let me connect you with a human' is more trusted than one that always answers confidently." },
        "twist": { "visual": "âš¡", "text": "Overtrust is also dangerous: users blindly accept AI recommendations for medical, financial, or legal decisions. Design friction for high-stakes outputs: 'Please verify this with a professional.'" },
        "climax": { "visual": "ğŸ", "text": "Trust is earned through consistent accuracy and honest uncertainty. Design for calibrated confidence â€” sure when it should be sure, uncertain when it should be uncertain." },
        "punchline": { "visual": "ğŸ¬", "text": "The most trusted AI isn't the most confident. It's the most honestly calibrated." }
      },
      "quiz": {
        "question": "What builds user trust in AI products?",
        "options": [
          "Always giving confident answers, even when uncertain",
          "Consistent accuracy combined with transparent limitations and easy verification",
          "Removing all uncertainty indicators",
          "Making the AI seem human"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t08-explainability",
      "chapter_id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Explainability",
      "description": "Helping users understand why the AI made a decision.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”", "text": "The AI rejects a loan application. The applicant asks why. 'The model determined you're high risk' isn't an explanation â€” it's a restatement." },
        "buildup": { "visual": "ğŸ’¬", "text": "Explainability means showing the reasoning: 'Your application was flagged because your debt-to-income ratio exceeds 40% and you have less than 2 years of credit history.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "For LLM-based systems, explanations can be generated alongside the answer: 'I recommended this product because it matches your stated budget ($500) and preferred brand (Sony).'" },
        "twist": { "visual": "âš¡", "text": "Explanations can be wrong too â€” the model might generate a plausible-sounding reason that doesn't match its actual decision process. Verify that explanations are consistent with the output." },
        "climax": { "visual": "ğŸ", "text": "Not every feature needs deep explainability. For low-stakes suggestions (movie recommendations), a brief reason suffices. For high-stakes decisions (loans, medical), detailed explanations are essential." },
        "punchline": { "visual": "ğŸ¬", "text": "If the AI says 'no,' users deserve to know why. Match explanation depth to decision stakes." }
      },
      "quiz": {
        "question": "When is detailed explainability most important?",
        "options": [
          "For movie recommendations",
          "For high-stakes decisions like loans, medical, or legal recommendations",
          "Explainability is never important",
          "Only for entertainment products"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t09-showing-ai-limitations",
      "chapter_id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Showing AI Limitations",
      "description": "Be upfront about what the AI can't do.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âš ï¸", "text": "'AI-powered financial advisor' sounds impressive until a user asks about tax law in Brazil and gets US tax advice applied to their situation. The AI didn't know its own boundaries." },
        "buildup": { "visual": "ğŸ“‹", "text": "Every AI feature has a scope: what it's trained on, what data it has access to, what it can and can't do. Communicate this clearly in the UI." },
        "discovery": { "visual": "ğŸ’¡", "text": "Simple patterns: a disclaimer bar ('This AI assistant knows about our product catalog. For medical questions, consult a doctor.'), or a capabilities page users can reference." },
        "twist": { "visual": "âš¡", "text": "Users skip disclaimers. Put limitations where they're relevant: if the user asks about something out of scope, respond with the limitation in context, not in a help page they'll never read." },
        "climax": { "visual": "ğŸ", "text": "Contextual limitations > blanket disclaimers. Tell users what the AI can't do right when they try to do it." },
        "punchline": { "visual": "ğŸ¬", "text": "An AI that knows what it doesn't know is more useful than one that pretends to know everything." }
      },
      "quiz": {
        "question": "What's more effective than a blanket AI disclaimer?",
        "options": [
          "No disclaimer at all",
          "Contextual limitations shown when the user asks something out of scope",
          "A longer disclaimer",
          "Hiding all limitations from users"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t10-collecting-implicit-feedback",
      "chapter_id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "Collecting Implicit Feedback",
      "description": "Learning from user behavior, not just explicit ratings.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ‘€", "text": "Only 5% of users click the thumbs up/down button. But 100% of users show you what they think through their behavior â€” if you track it." },
        "buildup": { "visual": "ğŸ“Š", "text": "Implicit feedback signals: the user regenerated the response (probably bad), copied the text (probably good), asked a follow-up (might need clarification), or left immediately (probably useless)." },
        "discovery": { "visual": "ğŸ’¡", "text": "Track: time spent reading responses, copy-paste events, regenerate clicks, follow-up questions, and session abandonment. Each signal tells you something about quality." },
        "twist": { "visual": "âš¡", "text": "Implicit signals are noisy: a user might copy-paste a wrong answer to share it as a complaint. No single signal is definitive â€” look for patterns across thousands of interactions." },
        "climax": { "visual": "ğŸ", "text": "Combine implicit and explicit feedback: implicit signals give you volume and coverage, explicit ratings give you ground truth. Both together give you the full picture." },
        "punchline": { "visual": "ğŸ¬", "text": "Users vote with their behavior, not just their thumbs. Learn to read both signals." }
      },
      "quiz": {
        "question": "What is an example of positive implicit feedback in an AI product?",
        "options": [
          "The user clicks thumbs down",
          "The user copies the AI's response text",
          "The user immediately closes the page",
          "The user clicks 'regenerate'"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t11-iterating-on-prompts",
      "chapter_id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "Iterating on Prompts",
      "description": "Systematic prompt improvement based on user data.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "You've tweaked the system prompt 47 times based on individual complaints. Each fix breaks something else. You need a system, not a whack-a-mole approach." },
        "buildup": { "visual": "ğŸ“‹", "text": "Systematic iteration: (1) Collect failing queries, (2) Categorize failure types, (3) Fix the most common category, (4) Run full eval to check for regressions, (5) Deploy if improved overall." },
        "discovery": { "visual": "ğŸ’¡", "text": "Keep a prompt changelog: what you changed, why, and what the eval scores were before and after. This prevents circular changes where you revert improvements from last month." },
        "twist": { "visual": "âš¡", "text": "Prompt changes have diminishing returns. After 5â€“10 iterations, you hit a ceiling. Further improvement usually requires better retrieval, better data, or a better model â€” not a better prompt." },
        "climax": { "visual": "ğŸ", "text": "Version your prompts like code. Test changes against a full eval set. Don't fix individual complaints â€” fix categories of failures." },
        "punchline": { "visual": "ğŸ¬", "text": "Fix failure categories, not individual queries. And know when the prompt isn't the bottleneck anymore." }
      },
      "quiz": {
        "question": "What indicates you've hit the ceiling on prompt improvement?",
        "options": [
          "The first prompt change didn't work",
          "After 5-10 iterations, eval scores stop improving significantly",
          "Users stop giving feedback",
          "The model refuses to follow instructions"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t12-graceful-degradation",
      "chapter_id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "Graceful Degradation",
      "description": "What happens when the AI service goes down.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”Œ", "text": "Your AI search is down. Users see a blank page. No search results, no error message, nothing. They think the whole app is broken." },
        "buildup": { "visual": "ğŸ›¡ï¸", "text": "Graceful degradation: when the AI service fails, fall back to a simpler experience. AI search â†’ keyword search. AI summary â†’ show the raw document. AI chatbot â†’ link to FAQ." },
        "discovery": { "visual": "ğŸ’¡", "text": "Design the non-AI fallback first. If your product doesn't work at all without AI, you have a single point of failure. The AI should enhance the experience, not be the experience." },
        "twist": { "visual": "âš¡", "text": "Cost spikes count as outages too. If your AI costs suddenly 10x (API rate change, model update), you need the ability to gracefully switch to a cheaper model or turn off AI features." },
        "climax": { "visual": "ğŸ", "text": "Every AI feature should have a fallback. Test the fallback regularly. Users should notice degradation, not complete breakage." },
        "punchline": { "visual": "ğŸ¬", "text": "Your app should still work when the AI doesn't. Design the fallback before you design the feature." }
      },
      "quiz": {
        "question": "What is a good practice for AI product resilience?",
        "options": [
          "Make AI the only way to use the product",
          "Design non-AI fallbacks so the product still works when the AI service fails",
          "Never turn off AI features",
          "Ignore AI outages â€” they're rare"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t13-feature-flags-for-ai",
      "chapter_id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "Feature Flags for AI",
      "description": "Roll out AI features gradually with instant kill switches.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸš¦", "text": "You launch the AI feature to all users. It works for 80% but generates embarrassing results for 20%. You can't roll back without a full deployment. Feature flags would have saved you." },
        "buildup": { "visual": "ğŸ”§", "text": "Feature flags let you: enable AI for 10% of users first, roll out to more as quality is confirmed, instantly disable if problems appear, and run A/B tests between AI and non-AI versions." },
        "discovery": { "visual": "ğŸ’¡", "text": "AI features are higher risk than traditional features because they're non-deterministic. Gradual rollout is more important for AI than for a button color change." },
        "twist": { "visual": "âš¡", "text": "Feature flags for AI aren't just on/off. You might want: model version flags (GPT-4 vs GPT-4o), prompt version flags, and context length flags. Each is a knob you can tune." },
        "climax": { "visual": "ğŸ", "text": "Ship AI features behind flags: start at 5%, monitor for a week, expand to 25%, monitor again, then go to 100%. The kill switch is always one click away." },
        "punchline": { "visual": "ğŸ¬", "text": "Ship slowly, monitor closely, kill instantly if needed. Feature flags make this possible." }
      },
      "quiz": {
        "question": "Why are feature flags especially important for AI features?",
        "options": [
          "AI features don't need feature flags",
          "AI outputs are non-deterministic, making gradual rollout and instant rollback critical",
          "Feature flags slow down AI features",
          "Only non-AI features need gradual rollout"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t14-defining-ai-product-kpis",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Defining AI Product KPIs",
      "description": "Metrics that tell you if your AI feature is working.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "The AI feature has 10,000 daily queries. Is that good? Without KPIs, you can't tell if users love it or just don't have an alternative." },
        "buildup": { "visual": "ğŸ“‹", "text": "AI product KPIs: adoption rate (what % of eligible users try it?), retention (do they come back?), task completion (do they finish what they started?), and satisfaction (thumbs up ratio)." },
        "discovery": { "visual": "ğŸ’¡", "text": "The most important KPI is the one that measures your product's core promise. If the AI helps users find answers, track 'time to answer.' If it generates content, track 'content published.'" },
        "twist": { "visual": "âš¡", "text": "Usage â‰  value. Users might use the AI chatbot a lot because it gives confusing answers that require multiple attempts. High usage + low satisfaction = a bad sign." },
        "climax": { "visual": "ğŸ", "text": "Pick 2â€“3 KPIs that reflect actual value delivered. Track them weekly. If they trend down, the feature needs work â€” regardless of how cool the technology is." },
        "punchline": { "visual": "ğŸ¬", "text": "Usage measures curiosity. Retention measures value. Track what matters." }
      },
      "quiz": {
        "question": "Why is high usage alone not a good KPI for an AI feature?",
        "options": [
          "High usage always means high value",
          "Users might interact frequently because the AI gives confusing answers requiring retries",
          "Usage is impossible to measure",
          "KPIs aren't needed for AI features"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t15-cost-per-interaction",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Cost per Interaction",
      "description": "Understanding and controlling AI operational costs.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ’°", "text": "Each AI chatbot interaction costs $0.03. With 100K daily users, that's $3,000/day â€” $90K/month. Your subscription revenue is $50K/month. The feature loses money." },
        "buildup": { "visual": "ğŸ“Š", "text": "Track cost per interaction: LLM API cost + embedding cost + infrastructure. Break it down by feature, user tier, and query type to find the most expensive patterns." },
        "discovery": { "visual": "ğŸ’¡", "text": "Cost optimization levers: use smaller models for simple queries, cache frequent responses, limit context length, set per-user rate limits, and move to open-source models for high-volume tasks." },
        "twist": { "visual": "âš¡", "text": "The cheapest interaction is the one that doesn't happen. If users ask the same question 500 times a day, cache the answer. If an AI feature is rarely used, maybe it doesn't need real-time inference." },
        "climax": { "visual": "ğŸ", "text": "Know your unit economics: cost per interaction Ã— interactions per user Ã— users = your AI operating cost. If it doesn't support the business model, redesign the feature or the pricing." },
        "punchline": { "visual": "ğŸ¬", "text": "Free AI features for users aren't free for you. Know your cost per interaction before you scale." }
      },
      "quiz": {
        "question": "What is the most impactful way to reduce AI cost per interaction?",
        "options": [
          "Disable the AI feature entirely",
          "Cache frequent responses and use smaller models for simple queries",
          "Use the most expensive model for everything",
          "Remove all rate limits"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t16-ai-product-lifecycle",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "AI Product Lifecycle",
      "description": "From prototype to mature product â€” what changes at each stage.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "Your AI feature launched 6 months ago. The easy improvements are done. Users are more demanding. Competitors caught up. What does the next phase look like?" },
        "buildup": { "visual": "ğŸ“‹", "text": "Stage 1 (0â€“3 months): Ship the MVP, measure baseline quality, fix the worst failures. Stage 2 (3â€“9 months): Optimize prompts and retrieval, build eval pipelines, reduce costs. Stage 3 (9+ months): Fine-tune models, personalize experiences, build moats." },
        "discovery": { "visual": "ğŸ’¡", "text": "Each stage has different bottlenecks. Early: 'Does it work?' Middle: 'Does it work well enough at scale?' Late: 'How do we differentiate and retain users?'" },
        "twist": { "visual": "âš¡", "text": "AI products never reach 'done.' Models improve, user expectations rise, and competitors iterate. Plan for continuous investment, not a one-time build." },
        "climax": { "visual": "ğŸ", "text": "Budget for ongoing AI costs in perpetuity: model updates, data freshness, eval maintenance, and incremental improvement. An unmaintained AI feature degrades fast." },
        "punchline": { "visual": "ğŸ¬", "text": "AI products aren't 'build and forget.' They're 'build and maintain forever.' Plan accordingly." }
      },
      "quiz": {
        "question": "What makes AI products different from traditional features in terms of maintenance?",
        "options": [
          "AI products require no maintenance after launch",
          "AI products need continuous investment â€” model updates, data freshness, and ongoing improvement",
          "Traditional features require more maintenance",
          "AI products are cheaper to maintain"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t17-mapping-ai-use-cases",
      "chapter_id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "Mapping AI Use Cases",
      "description": "Systematically identifying where AI adds the most value in your product.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ—ºï¸", "text": "A PM says 'Let's add AI to everything!' The team adds chatbots to 12 pages. Users engage with 1. The other 11 are ignored." },
        "buildup": { "visual": "ğŸ“Š", "text": "Not every feature benefits from AI. Map your product's workflows and identify where users are slow, frustrated, or producing inconsistent results. Those are your AI opportunities." },
        "discovery": { "visual": "ğŸ’¡", "text": "High-value AI use cases share traits: repetitive work, natural language input/output, pattern recognition, or personalisation. Low-value: well-defined logic, simple CRUD, or deterministic calculations." },
        "twist": { "visual": "âš¡", "text": "The best AI use case isn't always the flashiest. Auto-categorising support tickets might save more time than a chatbot â€” even though it's invisible to users." },
        "climax": { "visual": "ğŸ", "text": "Score each use case on: user pain severity, technical feasibility, error tolerance, and expected ROI. Prioritise high pain + high tolerance + clear ROI." },
        "punchline": { "visual": "ğŸ¬", "text": "AI everywhere is AI nowhere. Pick the spots where it matters most." }
      },
      "quiz": {
        "question": "Which trait makes a workflow a good candidate for AI?",
        "options": [
          "It's already perfectly automated",
          "It involves repetitive work with natural language or pattern recognition",
          "It requires deterministic, exact calculations",
          "It has zero tolerance for errors"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t18-ai-first-vs-ai-assisted",
      "chapter_id": "ai--ai-product-design--ch01-thinking-in-ai-products",
      "title": "AI-First vs AI-Assisted",
      "description": "Choosing between AI-driven and AI-augmented product experiences.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ”„", "text": "GitHub Copilot assists â€” you drive, AI suggests. ChatGPT is AI-first â€” AI drives, you guide. Same technology, radically different product designs." },
        "buildup": { "visual": "âš–ï¸", "text": "AI-first: the AI generates the output, the user reviews and edits. AI-assisted: the user does the work, the AI helps along the way. Each has different UX implications." },
        "discovery": { "visual": "ğŸ’¡", "text": "AI-first works when: output quality is good enough for most cases, users expect speed, and editing is easy. AI-assisted works when: precision matters, users are experts, and AI suggestions add value without disrupting flow." },
        "twist": { "visual": "âš¡", "text": "Many products start AI-assisted and evolve to AI-first as model quality improves. Email draft suggestions â†’ full email generation â†’ email agent that sends on your behalf." },
        "climax": { "visual": "ğŸ", "text": "Choose based on your error tolerance and user expertise. High-stakes + expert users = AI-assisted. Low-stakes + casual users = AI-first." },
        "punchline": { "visual": "ğŸ¬", "text": "AI-first isn't always better. Sometimes the best AI experience is a quiet suggestion in the sidebar." }
      },
      "quiz": {
        "question": "When is an AI-first design appropriate?",
        "options": [
          "When output must be 100% accurate",
          "When users are domain experts who prefer full control",
          "When output quality is good enough and users value speed",
          "When the task requires no editing"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-product-design--t19-managing-user-expectations",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "Managing User Expectations",
      "description": "Setting the right expectations so users aren't disappointed by AI limitations.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ­", "text": "Users see 'AI-powered' and expect perfection. When the AI makes a mistake, trust drops to zero. Not because the AI was bad, but because expectations were wrong." },
        "buildup": { "visual": "ğŸ“‹", "text": "Expectation management starts with how you frame the feature. 'AI-generated draft (please review)' sets different expectations than 'AI answer' or 'Smart recommendation.'" },
        "discovery": { "visual": "ğŸ’¡", "text": "Use language that implies assistance, not authority: 'suggested,' 'draft,' 'may include errors,' 'based on available data.' Make it clear the AI is helping, not deciding." },
        "twist": { "visual": "âš¡", "text": "Under-promise, over-deliver. If you say 'this might not be perfect' and it IS perfect, users are delighted. If you imply perfection and it's 95% right, users focus on the 5% failure." },
        "climax": { "visual": "ğŸ", "text": "The best AI products train users to expect AI-level quality (very good but not perfect) and make it easy to correct mistakes when they happen." },
        "punchline": { "visual": "ğŸ¬", "text": "Frame AI as a helpful assistant, not an oracle. Users forgive assistants. They don't forgive liars." }
      },
      "quiz": {
        "question": "How should AI features be framed to manage expectations?",
        "options": [
          "As perfect, 100% accurate systems",
          "As replacements for human judgment",
          "As helpful suggestions that may need review",
          "Without any indication that AI is involved"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-product-design--t20-loading-states-for-ai",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "Loading States for AI",
      "description": "Designing engaging waiting experiences for slow AI responses.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "â³", "text": "A spinner for 8 seconds feels like an eternity. A streaming text response with a 'thinking...' animation feels fast â€” even if both take the same time." },
        "buildup": { "visual": "ğŸ¨", "text": "AI responses are slower than traditional APIs (2-15 seconds vs <500ms). Users need to feel progress, not stare at a blank screen." },
        "discovery": { "visual": "ğŸ’¡", "text": "Design patterns: streaming text (show tokens as they arrive), skeleton loaders with context ('Analyzing your document...'), progress stages ('Step 1: Reading... Step 2: Summarizing...'), and intermediate results." },
        "twist": { "visual": "âš¡", "text": "Never show a generic spinner for AI features. Contextual loading states ('Searching 1,247 documents...') educate users about what's happening and feel 2x faster than generic spinners." },
        "climax": { "visual": "ğŸ", "text": "The best AI loading states are: informative (what's happening), progressive (show partial results), and interruptible (let users cancel or modify mid-generation)." },
        "punchline": { "visual": "ğŸ¬", "text": "Perceived speed is designed, not just engineered. A good loading state is a product decision." }
      },
      "quiz": {
        "question": "What is the best loading state for AI features?",
        "options": [
          "A generic spinner",
          "No loading state â€” just wait",
          "Contextual progress with streaming and intermediate results",
          "A blank screen"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-product-design--t21-ai-error-messages",
      "chapter_id": "ai--ai-product-design--ch02-user-experience",
      "title": "AI Error Messages",
      "description": "Designing helpful error experiences when AI features fail.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âš ï¸", "text": "'Something went wrong. Please try again.' The user tries again. Same error. And again. They leave forever." },
        "buildup": { "visual": "ğŸ“‹", "text": "AI features fail in unique ways: rate limits, context too long, content filtered, model unavailable, poor quality output. Each needs a different user-facing message." },
        "discovery": { "visual": "ğŸ’¡", "text": "Good AI error messages are: specific ('Your text is too long â€” try shortening to under 4,000 words'), actionable ('Try rephrasing your question'), and honest ('AI couldn't generate a reliable answer for this query')." },
        "twist": { "visual": "âš¡", "text": "Some 'errors' aren't errors â€” they're quality issues. If the AI generates a low-confidence response, it's better to say 'I'm not sure about this' than to show bad output with no warning." },
        "climax": { "visual": "ğŸ", "text": "Map every failure mode to a user-friendly message and a recovery action. Rate limit â†’ 'Try again in a moment.' Too long â†’ 'Shorten your input.' Content filtered â†’ 'Please rephrase.'" },
        "punchline": { "visual": "ğŸ¬", "text": "A good error message turns frustration into action. A bad one turns users away." }
      },
      "quiz": {
        "question": "What makes a good AI error message?",
        "options": [
          "Generic text like 'Something went wrong'",
          "Technical error codes only",
          "Specific, actionable messages with recovery suggestions",
          "No error message â€” just retry silently"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--ai-product-design--t22-confidence-indicators",
      "chapter_id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Confidence Indicators",
      "description": "Showing users how confident the AI is in its output.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“Š", "text": "The AI classifies an email as 'spam' with 51% confidence. It shows no indicator. The user trusts it and misses an important email from a client." },
        "buildup": { "visual": "ğŸšï¸", "text": "Confidence indicators tell users how sure the AI is: high confidence (act on it), medium (review it), low (don't trust it without checking). This transparency builds trust." },
        "discovery": { "visual": "ğŸ’¡", "text": "Design patterns: colour-coded badges (green/yellow/red), percentage scores, verbal qualifiers ('Very likely,' 'Possibly,' 'Uncertain'), or highlighting which parts of the output are most/least reliable." },
        "twist": { "visual": "âš¡", "text": "LLM confidence is notoriously poorly calibrated â€” a model can be 99% 'confident' and still be wrong. Use downstream signals (retrieval scores, validator checks) rather than raw model probabilities." },
        "climax": { "visual": "ğŸ", "text": "Confidence indicators shift responsibility to the user appropriately. 'I'm very sure' = proceed. 'I'm not sure' = check this yourself. Users learn when to trust and when to verify." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't just show the answer. Show how much the AI trusts its own answer." }
      },
      "quiz": {
        "question": "Why are raw LLM probability scores unreliable as confidence indicators?",
        "options": [
          "They are always perfectly calibrated",
          "They are poorly calibrated â€” the model can be highly confident and still wrong",
          "They are too hard to compute",
          "Users can't understand percentages"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t23-source-attribution",
      "chapter_id": "ai--ai-product-design--ch03-trust-and-transparency",
      "title": "Source Attribution",
      "description": "Showing users where the AI's information came from.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸ“", "text": "'According to your company's HR policy...' says the AI. The user clicks the citation â€” it links to the actual policy document, page 14. Trust skyrockets." },
        "buildup": { "visual": "ğŸ”—", "text": "Source attribution means showing which documents, data points, or knowledge the AI used to generate its response. It makes outputs verifiable." },
        "discovery": { "visual": "ğŸ’¡", "text": "In RAG systems, display the retrieved chunks alongside the answer. Link to source documents. Highlight which parts of the response correspond to which source." },
        "twist": { "visual": "âš¡", "text": "Attribution can be wrong â€” the AI might cite a document but actually hallucinate the specific claim. Verify that cited sources actually support the stated claims, don't just trust the link." },
        "climax": { "visual": "ğŸ", "text": "Good attribution: clickable source links, relevant quotes from source documents, and clear labelling when no source was found for a claim." },
        "punchline": { "visual": "ğŸ¬", "text": "Show your work. Citations turn an AI claim into a verifiable statement." }
      },
      "quiz": {
        "question": "What is a risk of AI source attribution?",
        "options": [
          "Users might check the sources",
          "The AI might cite a source that doesn't actually support the claim",
          "Sources are always perfectly matched",
          "Attribution makes responses slower"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t24-user-corrections",
      "chapter_id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "User Corrections",
      "description": "Letting users correct AI outputs and learning from those corrections.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âœï¸", "text": "The AI classifies a ticket as 'billing.' The support agent changes it to 'technical.' That correction is gold â€” if you capture it." },
        "buildup": { "visual": "ğŸ”„", "text": "User corrections are the highest-quality feedback signal. They tell you exactly where the AI was wrong and what the right answer should have been." },
        "discovery": { "visual": "ğŸ’¡", "text": "Make corrections easy: editable outputs, dropdown overrides, 'this is wrong' buttons, and inline editing. Every correction should be logged with the original AI output for comparison." },
        "twist": { "visual": "âš¡", "text": "Corrections are a free eval dataset. Aggregate them monthly: which types of errors are most common? Which inputs trigger the most corrections? This drives targeted improvement." },
        "climax": { "visual": "ğŸ", "text": "The correction loop: AI generates â†’ user corrects â†’ correction logged â†’ patterns analysed â†’ prompts/models improved â†’ fewer corrections needed. This is how AI products get better." },
        "punchline": { "visual": "ğŸ¬", "text": "Every correction is a lesson. Make it easy to teach your AI." }
      },
      "quiz": {
        "question": "Why are user corrections valuable for AI improvement?",
        "options": [
          "They tell you exactly where the AI was wrong and what the right answer is",
          "They reduce server costs",
          "They make the UI look better",
          "They are not useful for improvement"
        ],
        "correct": 0
      }
    },
    {
      "id": "ai--ai-product-design--t25-thumbs-up-thumbs-down",
      "chapter_id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "Thumbs Up / Thumbs Down",
      "description": "Designing simple feedback mechanisms for AI outputs.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ğŸ‘", "text": "ChatGPT's thumbs up/down buttons are the most important UI element in AI history. They generate millions of quality signals per day at zero cost." },
        "buildup": { "visual": "ğŸ¯", "text": "Binary feedback (good/bad) is the simplest feedback mechanism. Low friction means high response rates. Even 5% of users rating gives you a massive quality signal." },
        "discovery": { "visual": "ğŸ’¡", "text": "Place feedback buttons directly below every AI output. Optionally add 'Why?' with pre-set categories: 'Inaccurate,' 'Not relevant,' 'Too long,' 'Offensive.' Structured reasons are more actionable." },
        "twist": { "visual": "âš¡", "text": "Positive feedback bias: users mostly rate when they're happy or very unhappy. The middle (mediocre responses) goes unrated. Account for this bias in your analysis." },
        "climax": { "visual": "ğŸ", "text": "Track thumbs-down rate by feature, by time, and by query type. A rising thumbs-down rate is an early warning of quality degradation." },
        "punchline": { "visual": "ğŸ¬", "text": "Two buttons. Infinite signal. Add them to every AI output." }
      },
      "quiz": {
        "question": "What bias exists in thumbs up/down feedback?",
        "options": [
          "Users rate everything equally",
          "Users mostly rate when very happy or very unhappy, leaving mediocre responses unrated",
          "Thumbs down is always more common",
          "There is no bias in binary feedback"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t26-prompt-iteration-workflow",
      "chapter_id": "ai--ai-product-design--ch04-feedback-and-iteration",
      "title": "Prompt Iteration Workflow",
      "description": "A structured process for improving prompts based on real user feedback.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”¬", "text": "A PM changes the system prompt 15 times in a week. Quality goes up, then down, then sideways. No one knows which version was best because nothing was tracked." },
        "buildup": { "visual": "ğŸ“‹", "text": "Prompt iteration needs a process: (1) Collect failure examples, (2) Hypothesize a prompt fix, (3) Test against eval set, (4) Deploy to canary, (5) Measure, (6) Ship or revert." },
        "discovery": { "visual": "ğŸ’¡", "text": "Keep a prompt changelog. For each version: what changed, why, what the eval scores were before and after, and whether it was shipped. This history is invaluable." },
        "twist": { "visual": "âš¡", "text": "Prompt changes can have unexpected side effects. Fixing one category of errors might introduce new ones. Always run the FULL eval set, not just the cases you're fixing." },
        "climax": { "visual": "ğŸ", "text": "The best teams iterate weekly: review this week's failures, adjust prompts, run evals, and ship improvements. It's a continuous improvement cycle, not a one-time task." },
        "punchline": { "visual": "ğŸ¬", "text": "Prompts are living documents. Iterate with data, not intuition." }
      },
      "quiz": {
        "question": "Why should you run the full eval set after a prompt change?",
        "options": [
          "To increase API costs",
          "Because fixing one category of errors might introduce new ones elsewhere",
          "Full eval sets are faster to run",
          "It's not necessary â€” just test the fixed cases"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t27-fallback-experiences",
      "chapter_id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "Fallback Experiences",
      "description": "What users see when the AI feature is unavailable or low quality.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ğŸª‚", "text": "The AI API goes down at 2 PM. Your entire search feature shows blank results. 10,000 users see an empty page because there's no fallback." },
        "buildup": { "visual": "ğŸ›¡ï¸", "text": "Every AI feature needs a fallback: what happens when the API is down, when the response is too slow, or when the output quality is unacceptable?" },
        "discovery": { "visual": "ğŸ’¡", "text": "Fallback hierarchy: (1) Cache â€” serve the last known good response. (2) Simpler model â€” try a faster, cheaper model. (3) Rule-based â€” use traditional logic. (4) Human â€” route to a human agent." },
        "twist": { "visual": "âš¡", "text": "Users who experience a graceful fallback don't even notice the AI was degraded. Users who see an error page lose trust permanently. The fallback IS the product quality." },
        "climax": { "visual": "ğŸ", "text": "Test fallbacks regularly. Force-disable the AI feature in staging and verify the fallback experience. If the fallback is broken, your feature has a single point of failure." },
        "punchline": { "visual": "ğŸ¬", "text": "Hope for the best, design for the worst. Every AI feature needs a Plan B." }
      },
      "quiz": {
        "question": "What is the correct order of a fallback hierarchy?",
        "options": [
          "Show an error page immediately",
          "Cache â†’ simpler model â†’ rule-based â†’ human agent",
          "Always route to a human agent first",
          "Retry the same request indefinitely"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t28-ab-testing-ai-features",
      "chapter_id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "A/B Testing AI Features",
      "description": "Running controlled experiments to measure AI feature impact.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ”¬", "text": "Your team debates for weeks: should the AI summarise or show bullet points? An A/B test answers the question in 3 days with real user data." },
        "buildup": { "visual": "âš–ï¸", "text": "A/B testing AI features works like traditional A/B: split users into groups, show each group a different variant, measure key metrics, and pick the winner." },
        "discovery": { "visual": "ğŸ’¡", "text": "AI A/B tests should measure: user engagement (clicks, time spent), task completion rate, user satisfaction (feedback signals), and downstream business metrics (conversion, retention)." },
        "twist": { "visual": "âš¡", "text": "AI responses are non-deterministic, adding noise to experiments. You need larger sample sizes and longer test durations than traditional A/B tests to achieve statistical significance." },
        "climax": { "visual": "ğŸ", "text": "A/B test everything: models, prompts, UI treatments, response lengths, and even whether to show AI at all. Data beats opinions." },
        "punchline": { "visual": "ğŸ¬", "text": "Don't debate. Experiment. Let users decide which AI experience wins." }
      },
      "quiz": {
        "question": "Why do AI A/B tests need larger sample sizes?",
        "options": [
          "AI features have fewer users",
          "Non-deterministic AI responses add noise, requiring more data for statistical significance",
          "A/B testing tools don't support AI",
          "AI features don't need A/B testing"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t29-multi-modal-ai-experiences",
      "chapter_id": "ai--ai-product-design--ch05-deployment-patterns",
      "title": "Multi-Modal AI Experiences",
      "description": "Designing products that combine text, image, audio, and video AI.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ­", "text": "A user takes a photo of their broken appliance. The AI identifies the model, diagnoses the issue from the image, and generates a repair guide with illustrations. Text alone couldn't do this." },
        "buildup": { "visual": "ğŸ”€", "text": "Multi-modal AI handles multiple input/output types: text + images (GPT-4V), text + audio (Whisper + TTS), text + video, and combinations. This enables richer, more natural interactions." },
        "discovery": { "visual": "ğŸ’¡", "text": "Design for the strongest modality: use images when visual context matters, audio for hands-free scenarios, and text for precision. Don't force multi-modal when single-modal works fine." },
        "twist": { "visual": "âš¡", "text": "Multi-modal adds complexity: larger payloads, slower responses, higher costs, and more failure modes. Each modality can fail independently. Design fallbacks for each." },
        "climax": { "visual": "ğŸ", "text": "The best multi-modal experiences feel seamless: take a photo, get a text answer. Record a voice note, get a summary. The modality switch is invisible to the user." },
        "punchline": { "visual": "ğŸ¬", "text": "The future of AI products is multi-modal. But only use it when it genuinely helps the user." }
      },
      "quiz": {
        "question": "When should you design a multi-modal AI experience?",
        "options": [
          "Always â€” it's always better",
          "When multiple input types genuinely help the user complete their task",
          "Only when competitors offer it",
          "Never â€” text is always sufficient"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t30-personalisation-with-ai",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Personalisation with AI",
      "description": "Using AI to tailor product experiences to individual users.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "ğŸ¯", "text": "Two users search 'Python.' A beginner gets tutorials. An expert gets API documentation. Same query, different AI-powered results. Engagement doubles." },
        "buildup": { "visual": "ğŸ‘¤", "text": "AI personalisation adapts content, recommendations, and responses based on user history, preferences, skill level, and behavior patterns." },
        "discovery": { "visual": "ğŸ’¡", "text": "Implement personalisation through: system prompt injection (add user context to every prompt), personalised retrieval (weight results by user history), and adaptive UI (show different features based on expertise)." },
        "twist": { "visual": "âš¡", "text": "Personalisation creates filter bubbles â€” users only see what the AI thinks they want. Build in discovery mechanisms so users can explore beyond their profile." },
        "climax": { "visual": "ğŸ", "text": "Measure personalisation quality: does it increase engagement, reduce support tickets, and improve task completion? If not, the personalisation might be adding complexity without value." },
        "punchline": { "visual": "ğŸ¬", "text": "The best product knows its user. AI makes that knowledge actionable at scale." }
      },
      "quiz": {
        "question": "What is a risk of AI personalisation?",
        "options": [
          "It always improves user satisfaction",
          "It can create filter bubbles where users only see familiar content",
          "It reduces system complexity",
          "It has no impact on engagement"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t31-task-completion-rates",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Task Completion Rates",
      "description": "Measuring whether AI actually helps users accomplish their goals.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "âœ…", "text": "Your AI feature has 10,000 daily users. Engagement is high. But only 12% of users actually complete their task. The AI is entertaining, not useful." },
        "buildup": { "visual": "ğŸ“Š", "text": "Task completion rate (TCR) measures the percentage of users who achieve their goal. For a search feature: did they find what they needed? For a drafting tool: did they use the generated content?" },
        "discovery": { "visual": "ğŸ’¡", "text": "Define completion signals for each AI feature: clicks on generated content, time saved vs manual process, downstream actions (e.g., sending the drafted email), or explicit 'done' signals." },
        "twist": { "visual": "âš¡", "text": "High engagement + low completion = the AI is interesting but not effective. Low engagement + high completion = the AI is useful but hard to find. You need both." },
        "climax": { "visual": "ğŸ", "text": "TCR is the ultimate AI product metric. It answers: 'Does this AI feature actually help?' Everything else â€” accuracy, latency, cost â€” is in service of task completion." },
        "punchline": { "visual": "ğŸ¬", "text": "The goal isn't AI usage. It's user success. Measure what matters." }
      },
      "quiz": {
        "question": "What does a high engagement but low task completion rate indicate?",
        "options": [
          "The AI feature is perfect",
          "The AI is interesting but not effectively helping users accomplish their goals",
          "Users are very satisfied",
          "The feature should be removed immediately"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--ai-product-design--t32-competitive-ai-moats",
      "chapter_id": "ai--ai-product-design--ch06-measuring-success",
      "title": "Competitive AI Moats",
      "description": "Building defensible advantages in AI-powered products.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ğŸ°", "text": "You build a great AI feature. Six months later, three competitors copy it. They use the same model, the same prompts. What makes your version better?" },
        "buildup": { "visual": "ğŸ§±", "text": "AI models are commoditised â€” anyone can call GPT-4. The moats are in: proprietary data, user feedback loops, domain-specific fine-tuning, and integrated workflows." },
        "discovery": { "visual": "ğŸ’¡", "text": "The strongest moat is a flywheel: more users â†’ more feedback â†’ better AI â†’ more users. Every interaction makes your product smarter. Competitors starting fresh can't replicate years of user data." },
        "twist": { "visual": "âš¡", "text": "Data moats erode if you don't invest in them. Models improve so fast that a 6-month data advantage can be wiped out by a new model. Combine data moats with speed of iteration." },
        "climax": { "visual": "ğŸ", "text": "Durable AI moats: proprietary training data, embedded workflows users depend on, network effects, and continuous improvement velocity. Build at least two." },
        "punchline": { "visual": "ğŸ¬", "text": "The model isn't the moat. The data, the workflow, and the feedback loop are." }
      },
      "quiz": {
        "question": "What creates a durable competitive advantage in AI products?",
        "options": [
          "Using the most expensive model",
          "Proprietary data, user feedback loops, and integrated workflows",
          "Having the best marketing",
          "Being the first to launch"
        ],
        "correct": 1
      }
    }
  ]
}
