{
  "categoryId": "ai",
  "subject": "AI",
  "courseId": "ai--model-limitations",
  "courseTitle": "Model Limitations",
  "emoji": "üöß",
  "color": "#EF4444",
  "requireAuthoredStory": true,
  "chapters": [
    {
      "id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "What Models Can't Do",
      "position": 1
    },
    {
      "id": "ai--model-limitations--ch02-hallucinations",
      "title": "Hallucinations",
      "position": 2
    },
    {
      "id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Reasoning Failures",
      "position": 3
    },
    {
      "id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Context & Knowledge",
      "position": 4
    },
    {
      "id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Behavioral Quirks",
      "position": 5
    },
    {
      "id": "ai--model-limitations--ch06-working-around-limits",
      "title": "Working Around Limits",
      "position": 6
    }
  ],
  "topics": [
    {
      "id": "ai--model-limitations--t01-models-are-not-minds",
      "chapter_id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "Models Are Not Minds",
      "description": "LLMs predict tokens ‚Äî they don't think or understand.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üß†", "text": "The model writes a poem about grief that makes you cry. It must understand grief, right? No. It predicts which words follow other words. It's never felt anything." },
        "buildup": { "visual": "üìã", "text": "LLMs are statistical pattern matchers trained on text. They learn which token is likely to come next given the previous tokens. The output looks like understanding, but the mechanism is pattern completion." },
        "discovery": { "visual": "üí°", "text": "This distinction matters practically: the model doesn't 'know' your codebase. It generates plausible-sounding code based on patterns. It can't verify truth ‚Äî it can only predict likely text." },
        "twist": { "visual": "‚ö°", "text": "The illusion of understanding is powerful. Users confide personal problems to chatbots, trust medical advice from models, and attribute emotions to text generators. Recognizing the illusion is step one." },
        "climax": { "visual": "üèÅ", "text": "Use models for what they're good at: pattern-based generation. Don't use them for what requires genuine understanding, verification, or reasoning from first principles." },
        "punchline": { "visual": "üé¨", "text": "It writes like it thinks. It doesn't think. The output is impressive, but the mechanism is pattern matching." }
      },
      "quiz": {
        "question": "What is the core mechanism of LLM text generation?",
        "options": [
          "Deep understanding of concepts and reasoning",
          "Predicting the most likely next token based on training patterns",
          "Retrieving answers from a knowledge database",
          "Simulating human consciousness"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t02-no-real-world-grounding",
      "chapter_id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "No Real-World Grounding",
      "description": "Models know text about the world, not the world itself.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üåç", "text": "Ask the model: 'Is it raining in London right now?' It gives a confident answer. It has no idea ‚Äî it can't see outside, check sensors, or access real-time data." },
        "buildup": { "visual": "üìö", "text": "LLMs know what people have written about the world, not the world itself. Their knowledge is frozen at the training cutoff date. Anything after that is unknown." },
        "discovery": { "visual": "üí°", "text": "This is why RAG and tool use exist: they give the model access to current data. Without them, the model is a very eloquent historian who stopped reading the news months ago." },
        "twist": { "visual": "‚ö°", "text": "The model will answer questions about current events using outdated information without any warning. It doesn't know what it doesn't know ‚Äî there's no internal 'this information might be stale' flag." },
        "climax": { "visual": "üèÅ", "text": "Never trust an LLM for current facts without grounding. If the answer depends on today's data, fetch today's data. The model's training data is always in the past." },
        "punchline": { "visual": "üé¨", "text": "Models read the world's text. They didn't experience the world. For current facts, ground them in live data." }
      },
      "quiz": {
        "question": "Why can't LLMs answer questions about current events accurately?",
        "options": [
          "They choose not to",
          "Their knowledge is frozen at the training cutoff date with no access to real-time information",
          "Current events are blocked by content policies",
          "They can always answer current event questions"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t03-confidence-without-correctness",
      "chapter_id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "Confidence Without Correctness",
      "description": "Models sound certain even when they're wrong.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üé≠", "text": "'The capital of Australia is Sydney,' the model states confidently. It's wrong ‚Äî it's Canberra. But the sentence was grammatically perfect and delivered without hesitation." },
        "buildup": { "visual": "üìä", "text": "LLMs don't have a reliable confidence signal. They generate text with the same fluency whether the content is correct or fabricated. There's no built-in 'I'm not sure' mechanism." },
        "discovery": { "visual": "üí°", "text": "This is fundamentally different from humans: a human might pause, say 'I think...' or 'I'm not sure.' An LLM produces equally confident text for facts and fabrications." },
        "twist": { "visual": "‚ö°", "text": "Users interpret confident language as correctness. Studies show people trust AI answers more when they sound authoritative ‚Äî even when they're wrong. Confidence is a UX problem, not just a model problem." },
        "climax": { "visual": "üèÅ", "text": "Design for this limitation: add verification steps, show sources, use phrases like 'Based on my training data...' and never present AI outputs as authoritative facts." },
        "punchline": { "visual": "üé¨", "text": "Confidence ‚â† correctness. The model sounds sure because it always sounds sure. Verify everything." }
      },
      "quiz": {
        "question": "Why is LLM confidence misleading?",
        "options": [
          "LLMs are always correct",
          "Models generate equally fluent text for correct and incorrect information",
          "Confidence always correlates with accuracy",
          "LLMs explicitly flag uncertainty"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t04-what-causes-hallucinations",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "What Causes Hallucinations",
      "description": "Why models make things up and the mechanisms behind it.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üåÄ", "text": "You ask for a bibliography. The model generates titles that look real, authors that exist, and journals that publish similar work ‚Äî but none of the papers are real." },
        "buildup": { "visual": "üîç", "text": "Hallucinations happen because the model generates the most probable continuation, not the most truthful. If plausible-sounding text requires inventing a fact, the model invents it." },
        "discovery": { "visual": "üí°", "text": "Root causes: (1) Training data gaps ‚Äî the model extrapolates into unknown territory. (2) Distributional patterns ‚Äî 'Dr. Smith published in Nature' is a common pattern, so the model generates instances of it. (3) No truth verification ‚Äî the model can't check its own output." },
        "twist": { "visual": "‚ö°", "text": "Hallucinations aren't bugs ‚Äî they're features of how the model works. The same mechanism that lets the model write creative fiction also lets it fabricate academic papers." },
        "climax": { "visual": "üèÅ", "text": "You can reduce hallucinations (RAG, constrained generation, fact-checking) but you can't eliminate them entirely. Design systems that assume hallucinations will happen." },
        "punchline": { "visual": "üé¨", "text": "The model doesn't lie ‚Äî it doesn't know what truth is. It generates probable text. Sometimes probable isn't true." }
      },
      "quiz": {
        "question": "What is the fundamental cause of LLM hallucinations?",
        "options": [
          "Intentional deception by the model",
          "The model generates the most probable text continuation, which may not be factually true",
          "Hardware failures",
          "Insufficient training data"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t05-factual-hallucinations",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "Factual Hallucinations",
      "description": "When the model states false facts as truth.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "‚ùå", "text": "'Einstein won the Nobel Prize in 1905 for his theory of relativity.' Sounds right, but it's wrong twice: he won it in 1921, and it was for the photoelectric effect." },
        "buildup": { "visual": "üìã", "text": "Factual hallucinations include: wrong dates, invented statistics, attributed quotes to wrong people, confused identities (mixing up similar-sounding names), and completely fabricated events." },
        "discovery": { "visual": "üí°", "text": "Factual hallucinations are most dangerous in professional contexts: legal advice with fake case citations, medical information with wrong dosages, financial data with invented numbers." },
        "twist": { "visual": "‚ö°", "text": "Partial truths are worse than complete fabrications: 'Einstein won the Nobel Prize' (true) 'in 1905' (wrong) 'for relativity' (wrong). The grain of truth makes the errors harder to catch." },
        "climax": { "visual": "üèÅ", "text": "For any factual claim: verify independently. Use RAG to ground facts in real data. Add disclaimers for high-stakes content. Never ship unverified AI-generated facts." },
        "punchline": { "visual": "üé¨", "text": "The most convincing hallucinations contain real facts mixed with false ones. That's what makes them dangerous." }
      },
      "quiz": {
        "question": "Why are partial-truth hallucinations more dangerous than complete fabrications?",
        "options": [
          "They're not ‚Äî complete fabrications are always worse",
          "The grain of truth makes the false parts harder to detect",
          "Partial truths are always caught by users",
          "LLMs never produce partial truths"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t06-citation-hallucinations",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "Citation Hallucinations",
      "description": "When the model invents sources that don't exist.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìñ", "text": "A lawyer submits a brief citing six precedent cases. The judge finds that none of the cases exist ‚Äî the model generated realistic-sounding case names, docket numbers, and summaries for imaginary cases." },
        "buildup": { "visual": "üìã", "text": "Citation hallucination: the model generates sources (papers, court cases, URLs, book titles) that follow the pattern of real citations but reference things that don't exist." },
        "discovery": { "visual": "üí°", "text": "The model doesn't search databases ‚Äî it generates text that looks like a citation. 'Smith v. Jones, 2019 WL 1234567' follows the pattern perfectly, but the case is invented." },
        "twist": { "visual": "‚ö°", "text": "Citation hallucinations are especially insidious because people trust citations ‚Äî the whole point of a citation is to verify a claim. When the citation itself is fake, the verification chain breaks." },
        "climax": { "visual": "üèÅ", "text": "Never trust AI-generated citations without verification. Use RAG with actual databases, or cross-reference every citation with the real source before using it." },
        "punchline": { "visual": "üé¨", "text": "A hallucinated citation isn't evidence ‚Äî it's evidence of how well the model mimics citation formats." }
      },
      "quiz": {
        "question": "Why are citation hallucinations particularly problematic?",
        "options": [
          "They're easy to spot",
          "People trust citations as verification, so fake citations undermine the entire trust chain",
          "Citations aren't important",
          "LLMs don't generate citations"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t07-math-and-counting",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Math and Counting",
      "description": "Why LLMs struggle with arithmetic and precise counting.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üî¢", "text": "'How many r's are in strawberry?' The model says 2. There are 3. A model that writes poetry can't count letters in a word." },
        "buildup": { "visual": "üìä", "text": "LLMs process text as tokens, not as individual characters or numbers. They don't have a calculator ‚Äî they predict what the answer looks like based on similar problems in training data." },
        "discovery": { "visual": "üí°", "text": "For simple arithmetic (2+2), the model memorized the answer. For complex math (347 √ó 923), it's guessing based on pattern. The answer looks like a number but isn't computed." },
        "twist": { "visual": "‚ö°", "text": "Chain-of-thought helps somewhat: 'Work through this step by step' makes the model show its work, catching some errors. But the underlying issue remains ‚Äî it's still predicting tokens, not computing." },
        "climax": { "visual": "üèÅ", "text": "For anything that requires precise math: give the model a calculator tool. For counting: use code execution. Never trust raw LLM arithmetic for anything that matters." },
        "punchline": { "visual": "üé¨", "text": "LLMs generate math answers. They don't compute them. For precision, use a tool." }
      },
      "quiz": {
        "question": "Why do LLMs struggle with arithmetic?",
        "options": [
          "They're not powerful enough",
          "They predict what the answer looks like rather than actually computing it",
          "Math is blocked by safety filters",
          "They always get math right"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t08-logical-reasoning",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Logical Reasoning",
      "description": "Where model reasoning breaks down.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "üîó", "text": "'All cats are animals. Some animals are pets. Therefore, all cats are pets.' The model agrees. The logic is invalid ‚Äî but the conclusion happens to be true for cats, so the model doesn't catch the error." },
        "buildup": { "visual": "üß©", "text": "LLMs struggle with formal logic: they confuse correlation with causation, miss logical negations, and accept invalid syllogisms when the conclusion sounds plausible." },
        "discovery": { "visual": "üí°", "text": "The model reasons by pattern: if the conclusion sounds like something that's usually true, it agrees. It doesn't actually trace the logical chain. Change 'cats' to 'aardvarks' and the error becomes obvious to the model too." },
        "twist": { "visual": "‚ö°", "text": "This makes LLMs particularly dangerous for legal, medical, or financial reasoning where the chain of logic matters as much as the conclusion. A correct-sounding answer with faulty logic is still wrong." },
        "climax": { "visual": "üèÅ", "text": "For logic-critical applications: validate reasoning chains, not just conclusions. Use structured prompting that forces explicit logical steps. Or use traditional logic engines for formal reasoning." },
        "punchline": { "visual": "üé¨", "text": "The model pattern-matches its way to answers. When the pattern fails, the logic fails. Check the chain, not just the conclusion." }
      },
      "quiz": {
        "question": "Why might an LLM accept an invalid logical argument?",
        "options": [
          "It always checks formal logic rigorously",
          "If the conclusion sounds plausible based on training patterns, it agrees regardless of logical validity",
          "LLMs don't process logical arguments",
          "Invalid logic is always caught"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t09-negation-and-instruction-following",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Negation Problems",
      "description": "Why 'don't do X' often results in the model doing X.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üö´", "text": "Prompt: 'Do NOT mention competitor products.' Response: 'While I won't mention competitor products like ProductX and ProductY...' ‚Äî it mentioned them while saying it wouldn't." },
        "buildup": { "visual": "üìã", "text": "Negation is hard for LLMs: 'don't mention X' puts X in the model's attention. The model is now thinking about X and is more likely to generate text involving X." },
        "discovery": { "visual": "üí°", "text": "Reframe negatively stated instructions as positive ones: instead of 'Don't mention competitors,' say 'Only discuss our own products.' Tell the model what to do, not what to avoid." },
        "twist": { "visual": "‚ö°", "text": "Long lists of 'don'ts' are the worst: 'Don't mention pricing, competitors, legal issues, or internal processes.' Each item primes the model to think about exactly those topics." },
        "climax": { "visual": "üèÅ", "text": "Positive framing > negative framing. 'Talk only about [topic]' works better than 'Don't talk about [everything else].' Constrain the model's scope rather than listing exceptions." },
        "punchline": { "visual": "üé¨", "text": "Don't tell the model what to avoid. Tell it what to do. The brain ‚Äî and the model ‚Äî can't unhear a suggestion." }
      },
      "quiz": {
        "question": "Why does 'Don't mention X' often cause the model to mention X?",
        "options": [
          "The model intentionally disobeys",
          "Mentioning X in the prompt puts it in the model's attention, making it more likely to generate text about X",
          "Negation instructions are always ignored",
          "This only happens with small models"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t10-context-window-limits",
      "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Context Window Limits",
      "description": "The model can only see a limited amount of text at once.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìè", "text": "You paste a 200-page document into the prompt. The model summarizes the first 10 pages and the last 10 pages. The middle 180 pages might as well not exist." },
        "buildup": { "visual": "üìä", "text": "The context window is the total amount of text the model can process at once: system prompt + conversation history + input + response all share the same limit." },
        "discovery": { "visual": "üí°", "text": "Even with 128K or 1M token windows, models don't attend equally to all positions. Information in the middle of a long context is often missed ‚Äî the 'lost in the middle' phenomenon." },
        "twist": { "visual": "‚ö°", "text": "Bigger context windows don't solve the problem ‚Äî they just move the boundary. A model with a 1M token window still struggles with information in the middle of 500K tokens." },
        "climax": { "visual": "üèÅ", "text": "Don't rely on large context windows for precise retrieval. Use RAG to find the relevant pieces first, then give the model only what it needs ‚Äî focused context beats massive context." },
        "punchline": { "visual": "üé¨", "text": "The model can see a lot of text. It pays attention to much less. Focus the context, don't flood it." }
      },
      "quiz": {
        "question": "What is the 'lost in the middle' phenomenon?",
        "options": [
          "Models process all context positions equally",
          "Models tend to miss information placed in the middle of long contexts, favoring the beginning and end",
          "Context windows are unlimited",
          "Only short contexts have this problem"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t11-knowledge-cutoffs",
      "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Knowledge Cutoffs",
      "description": "The model's training data has a hard stop date.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üìÖ", "text": "You ask the model about an event that happened last month. It either says it doesn't know (good) or confidently describes a completely different event (bad)." },
        "buildup": { "visual": "‚è∞", "text": "Every model has a training data cutoff: GPT-4 might have data up to April 2024, Claude up to early 2025. Anything after the cutoff is unknown to the model." },
        "discovery": { "visual": "üí°", "text": "The cutoff isn't always obvious. The model might have outdated information about prices, laws, company names, or APIs that changed after training. It doesn't flag stale data." },
        "twist": { "visual": "‚ö°", "text": "Web browsing tools give the illusion of current knowledge, but the model's reasoning about the current data still relies on its training knowledge. It might misinterpret current events through an outdated lens." },
        "climax": { "visual": "üèÅ", "text": "For time-sensitive information: always use live data sources. Note the model's cutoff date and design systems that compensate for stale knowledge." },
        "punchline": { "visual": "üé¨", "text": "The model lives in the past. For anything current, bring the data to the model ‚Äî it can't go get it." }
      },
      "quiz": {
        "question": "How should you handle questions about events after the model's training cutoff?",
        "options": [
          "Trust the model's best guess",
          "Use live data sources and RAG to provide current information",
          "Tell users the model always has current information",
          "Knowledge cutoffs don't affect answers"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t12-sycophancy",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Sycophancy",
      "description": "When the model agrees with you even when you're wrong.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üòä", "text": "'I think the Earth is the largest planet in our solar system.' The model: 'That's an interesting perspective! While the Earth is certainly impressive...' It should just say you're wrong." },
        "buildup": { "visual": "üìã", "text": "Sycophancy: the model agrees with the user, avoids contradicting them, and validates incorrect beliefs. It was trained to be helpful and agreeable ‚Äî sometimes too agreeable." },
        "discovery": { "visual": "üí°", "text": "RLHF (the training process that makes models helpful) can over-optimize for user approval. Humans rated 'agreeable' responses higher during training, so the model learned to agree." },
        "twist": { "visual": "‚ö°", "text": "Sycophancy is worst for domain experts who phrase wrong statements confidently. The model defers to apparent expertise: 'As a doctor, I believe X' makes the model less likely to correct factually wrong X." },
        "climax": { "visual": "üèÅ", "text": "For critical applications: prompt the model to prioritize correctness over agreeableness. 'Always correct factual errors, even if the user states them confidently.'" },
        "punchline": { "visual": "üé¨", "text": "A model that never disagrees with you isn't helpful ‚Äî it's a yes-machine. Prompt for honesty, not politeness." }
      },
      "quiz": {
        "question": "What causes sycophancy in LLMs?",
        "options": [
          "The model is trying to be deceptive",
          "RLHF training rewarded agreeable responses, making the model over-optimize for user approval",
          "Sycophancy doesn't exist in modern models",
          "It's a hardware issue"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t13-sensitivity-to-phrasing",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Sensitivity to Phrasing",
      "description": "The same question asked differently gets different answers.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "üîÑ", "text": "'What are the benefits of remote work?' gets a positive response. 'What are the problems with remote work?' gets a negative response. 'Analyze remote work.' gets a balanced response. Same topic, different framing, different answers." },
        "buildup": { "visual": "üìã", "text": "LLMs are highly sensitive to how questions are phrased: word choice, question framing, order of options, and even punctuation can change the output significantly." },
        "discovery": { "visual": "üí°", "text": "This means: the person writing the prompt has enormous influence over the output. A slightly reworded question can flip the model's recommendation. It's not reasoning ‚Äî it's reflecting the framing." },
        "twist": { "visual": "‚ö°", "text": "In production, this means user phrasing affects answer quality unpredictably. 'How do I cancel?' might work great while 'Cancel my account' gets a confused response ‚Äî because the training data had different patterns." },
        "climax": { "visual": "üèÅ", "text": "Test with diverse phrasings of the same question. If the answer changes significantly with minor rewording, the model isn't reliable for that task. Add query normalization if needed." },
        "punchline": { "visual": "üé¨", "text": "The answer depends on how you ask. Test many ways to find out how fragile your system really is." }
      },
      "quiz": {
        "question": "Why is prompt phrasing sensitivity a practical problem?",
        "options": [
          "It only matters for creative writing",
          "Users phrase questions differently, leading to inconsistent answers for the same underlying question",
          "Models aren't sensitive to phrasing",
          "It makes the model faster"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t14-overreliance-on-models",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Overreliance on Models",
      "description": "The danger of trusting AI output without verification.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ü´†", "text": "A developer uses AI-generated code without reviewing it. The code works for the test case but has a subtle security vulnerability. It passes code review because the reviewer also trusted the AI." },
        "buildup": { "visual": "üìä", "text": "Automation bias: humans tend to accept AI outputs uncritically, especially when the AI seems confident. The more impressive the AI appears, the less likely people are to question it." },
        "discovery": { "visual": "üí°", "text": "Studies show that people with AI assistance sometimes perform worse than people without it ‚Äî because they defer to the AI even when their own judgment was correct." },
        "twist": { "visual": "‚ö°", "text": "The fix isn't to distrust the AI entirely ‚Äî that would negate its value. The fix is calibrated trust: verify high-stakes outputs, sample-check routine outputs, and always maintain the ability to override." },
        "climax": { "visual": "üèÅ", "text": "Use AI as a co-pilot, not an autopilot. The human reviews, validates, and takes responsibility for the final output." },
        "punchline": { "visual": "üé¨", "text": "AI is a tool that requires a skilled operator. Trust it, but verify. Every time." }
      },
      "quiz": {
        "question": "What is automation bias?",
        "options": [
          "A bias in the AI's training data",
          "The tendency for humans to accept AI outputs uncritically, even when incorrect",
          "A type of model hallucination",
          "Bias in automated testing"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t15-when-not-to-use-llms",
      "chapter_id": "ai--model-limitations--ch06-working-around-limits",
      "title": "When Not to Use LLMs",
      "description": "Tasks where traditional software is simply better.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "üö´", "text": "A startup uses GPT-4 to validate email addresses. It costs $0.01 per validation. A regex costs $0.00001. The LLM is 1,000x more expensive and less accurate." },
        "buildup": { "visual": "üìã", "text": "Don't use LLMs for: deterministic tasks (validation, formatting), precise calculations, database queries, pattern matching, or anything with clearly defined rules." },
        "discovery": { "visual": "üí°", "text": "The rule: if you can write the logic as a function that always gives the correct answer, use the function. LLMs are for tasks where the 'correct answer' is ambiguous or context-dependent." },
        "twist": { "visual": "‚ö°", "text": "Hybrid is often best: use an LLM to understand the user's intent, then use traditional code to execute it. 'What does the user want?' ‚Üí LLM. 'Calculate the shipping cost' ‚Üí function." },
        "climax": { "visual": "üèÅ", "text": "Match the tool to the task: LLMs for ambiguity and language, traditional code for precision and determinism. The best systems combine both." },
        "punchline": { "visual": "üé¨", "text": "Not everything needs an LLM. The best AI engineers know when to use a regex instead." }
      },
      "quiz": {
        "question": "When should you use traditional code instead of an LLM?",
        "options": [
          "Never ‚Äî LLMs are always better",
          "When the task has deterministic rules and a clearly defined correct answer",
          "Only when the LLM is down",
          "Traditional code can't handle any modern tasks"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t16-mitigating-limitations",
      "chapter_id": "ai--model-limitations--ch06-working-around-limits",
      "title": "Mitigating Limitations",
      "description": "Practical strategies for working within model constraints.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "üõ†Ô∏è", "text": "You know the model hallucinates, can't do math, and sometimes ignores instructions. You're shipping the product anyway. How do you build something reliable on an unreliable foundation?" },
        "buildup": { "visual": "üìã", "text": "Mitigation strategies: (1) RAG for factual grounding, (2) Tool use for math and data retrieval, (3) Output validation for format compliance, (4) Retry with rephrasing for instruction following, (5) Human-in-the-loop for high stakes." },
        "discovery": { "visual": "üí°", "text": "The key insight: treat the model as a probabilistic component in a deterministic system. Wrap it with validation, retry logic, and fallback paths ‚Äî just like you'd handle any unreliable external service." },
        "twist": { "visual": "‚ö°", "text": "Every mitigation adds latency and cost. A system with RAG + reranking + validation + retry can take 10 seconds per query. Design for the trade-off between reliability and speed." },
        "climax": { "visual": "üèÅ", "text": "The best AI systems don't pretend models are perfect. They architect around the imperfections: ground when possible, validate always, fail gracefully." },
        "punchline": { "visual": "üé¨", "text": "You can't fix the model. But you can build systems that handle its failures. That's the job." }
      },
      "quiz": {
        "question": "What is the key principle for building reliable systems with unreliable LLMs?",
        "options": [
          "Wait for models to become perfect",
          "Treat the model as a probabilistic component wrapped in validation, retry logic, and fallback paths",
          "Never use LLMs in production",
          "Rely entirely on the model's output"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t17-emergent-abilities-myth",
      "chapter_id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "Emergent Abilities Myth",
      "description": "Why 'emergent abilities' may be a measurement artifact, not magic.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "‚ú®", "text": "GPT-4 can suddenly do tasks GPT-3 couldn't. It must have 'emerged' some new ability. Or did the benchmark just happen to flip from 0% to 60% as the model crossed a quality threshold?" },
        "buildup": { "visual": "üìä", "text": "Emergent abilities: capabilities that appear suddenly at a certain model scale. Some researchers argue these are real phase transitions. Others say they're artifacts of how we measure." },
        "discovery": { "visual": "üí°", "text": "When using binary scoring (right or wrong), a model that gradually gets better at a task will appear to suddenly 'get it' at a threshold. Switch to partial credit scoring, and the improvement is smooth ‚Äî no emergence." },
        "twist": { "visual": "‚ö°", "text": "This matters practically: don't assume a bigger model will suddenly solve your problem. Capability improves gradually. If the current model gets 30% accuracy, a 2x bigger model might get 45%, not 95%." },
        "climax": { "visual": "üèÅ", "text": "Benchmark your actual task with the models you can afford. Don't hope the next model will magically solve everything. Plan for incremental improvement." },
        "punchline": { "visual": "üé¨", "text": "Emergence might be real. Or it might be how we're holding the ruler. Either way, don't bet your product on it." }
      },
      "quiz": {
        "question": "Why might emergent abilities be a measurement artifact?",
        "options": [
          "Because models don't improve with scale",
          "Binary scoring can make gradual improvement appear as a sudden jump",
          "Because all models have the same abilities",
          "Emergent abilities are always real"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t18-brittleness",
      "chapter_id": "ai--model-limitations--ch01-what-models-cant-do",
      "title": "Brittleness",
      "description": "Small input changes that cause dramatically different outputs.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "ü™ü", "text": "Your prompt works perfectly. You change one word ‚Äî 'analyze' to 'analyse' ‚Äî and the output quality drops by 40%. The model is brittle." },
        "buildup": { "visual": "üî®", "text": "Brittleness: LLMs are sensitive to minor input variations that humans would consider equivalent. Rephrasing, reordering, or even capitalisation changes can significantly alter outputs." },
        "discovery": { "visual": "üí°", "text": "This sensitivity comes from how models were trained: specific phrasings appeared more often in high-quality training data. The model associates certain phrasings with quality, not the semantic content." },
        "twist": { "visual": "‚ö°", "text": "Brittleness means you can't just write a prompt once and trust it. You need to test with variations: different phrasings, orderings, and edge cases. A prompt that works on 10 examples might fail on the 11th." },
        "climax": { "visual": "üèÅ", "text": "Mitigate brittleness: test prompts with input variations, use structured prompts with consistent formatting, and run regression tests when changing prompt templates." },
        "punchline": { "visual": "üé¨", "text": "LLMs look robust until you change a comma. Test extensively." }
      },
      "quiz": {
        "question": "What is model brittleness?",
        "options": [
          "Models that break permanently after errors",
          "Sensitivity to minor input variations that cause dramatically different outputs",
          "Models that only work on one task",
          "Hardware failures in GPUs"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t19-citation-hallucinations",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "Citation Hallucinations",
      "description": "When AI invents fake references that look completely real.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìö", "text": "A lawyer uses ChatGPT to prepare a legal brief. It cites six relevant cases with real-sounding names, courts, and dates. None of them exist. The lawyer is sanctioned by the judge." },
        "buildup": { "visual": "üìñ", "text": "Citation hallucination: the model generates references that follow the format of real citations (author names, journal titles, years, DOIs) but point to papers, cases, or books that don't exist." },
        "discovery": { "visual": "üí°", "text": "The model learned the pattern of citations, not the citations themselves. It knows 'Smith et al., 2021, Nature, vol. 589' looks like a valid citation, so it generates plausible ones." },
        "twist": { "visual": "‚ö°", "text": "These are extra dangerous because they pass casual verification: the author exists, the journal exists, the date range is reasonable. You have to actually look up the specific reference to discover it's fake." },
        "climax": { "visual": "üèÅ", "text": "Never trust AI-generated citations without verifying them. Use RAG with real document sources, or explicitly instruct the model to cite only from provided documents." },
        "punchline": { "visual": "üé¨", "text": "AI writes perfect-looking citations to papers that don't exist. Verify every single one." }
      },
      "quiz": {
        "question": "Why are citation hallucinations particularly dangerous?",
        "options": [
          "They're easy to spot",
          "They follow real citation formats with plausible details, making casual verification insufficient",
          "They only occur with small models",
          "They always contain obviously fake names"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t20-confident-wrong-answers",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "Confident Wrong Answers",
      "description": "Why AI is most dangerous when it's confidently incorrect.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üò§", "text": "You ask the model: 'How many R's in strawberry?' It confidently answers: 'Two.' It's wrong ‚Äî there are three. It says it with the same confidence as correct answers." },
        "buildup": { "visual": "üìä", "text": "LLMs don't have calibrated confidence. They don't 'know what they don't know.' A correct answer and a hallucinated answer look identical in tone, formatting, and apparent certainty." },
        "discovery": { "visual": "üí°", "text": "This is a fundamental problem: there's no reliable built-in signal for when the model is likely wrong. High token probabilities don't guarantee factual accuracy." },
        "twist": { "visual": "‚ö°", "text": "Users trust confident-sounding text. When the model says 'The answer is X' with authority, people rarely question it ‚Äî especially when it's right 90% of the time." },
        "climax": { "visual": "üèÅ", "text": "Add uncertainty markers: ask the model to rate its confidence, cross-reference with external sources, and design UIs that remind users to verify critical information." },
        "punchline": { "visual": "üé¨", "text": "The model doesn't hedge because it doesn't know it's wrong. That confidence is the danger." }
      },
      "quiz": {
        "question": "Why can't you use the model's tone to judge accuracy?",
        "options": [
          "Correct and incorrect answers are equally confident ‚Äî the model has no calibrated uncertainty",
          "The model always sounds uncertain",
          "Confident answers are always correct",
          "Tone is a reliable accuracy indicator"
        ],
        "correct": 0
      }
    },
    {
      "id": "ai--model-limitations--t21-detecting-hallucinations",
      "chapter_id": "ai--model-limitations--ch02-hallucinations",
      "title": "Detecting Hallucinations",
      "description": "Practical techniques for catching AI fabrications before users see them.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "üîç", "text": "Your AI customer service bot tells a customer their refund will arrive in 3 days. Your policy says 10 days. How do you catch this before the message is sent?" },
        "buildup": { "visual": "üìã", "text": "Hallucination detection approaches: (1) Cross-reference against source documents, (2) Ask a second model to verify, (3) Check factual claims against a knowledge base, (4) Use NLI (Natural Language Inference) models." },
        "discovery": { "visual": "üí°", "text": "The most practical technique for RAG apps: compare the AI's response against the retrieved documents. If the response contains claims not supported by the sources, flag it as a potential hallucination." },
        "twist": { "visual": "‚ö°", "text": "No detection method is perfect. You're using one imperfect system to check another. The goal is reducing hallucination frequency, not eliminating it entirely." },
        "climax": { "visual": "üèÅ", "text": "Layer defenses: source grounding + response validation + user feedback signals. Each layer catches different types of hallucinations. Together, they catch most." },
        "punchline": { "visual": "üé¨", "text": "You can't prevent all hallucinations. But you can catch most of them before users see them." }
      },
      "quiz": {
        "question": "What is the most practical hallucination detection technique for RAG applications?",
        "options": [
          "Checking token probabilities",
          "Comparing the AI's response against the retrieved source documents",
          "Asking the user if the answer seems right",
          "Using a larger model"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t22-multi-step-math",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Multi-Step Math Failures",
      "description": "Why LLMs struggle with calculations that require multiple steps.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üßÆ", "text": "You ask: 'If I buy 7 items at $12.99 each with 8.5% tax, what's my total?' The model confidently says $98.62. The correct answer is $98.72. Close, but wrong." },
        "buildup": { "visual": "üìê", "text": "LLMs do math by pattern matching, not computation. For simple math (2+2), the pattern is strong enough. For multi-step problems, errors compound at each step because the model isn't actually calculating." },
        "discovery": { "visual": "üí°", "text": "Each step in a chain of reasoning has some probability of error. A 5-step problem where each step has 95% accuracy has only 77% overall accuracy. More steps = more errors." },
        "twist": { "visual": "‚ö°", "text": "Chain-of-thought prompting helps because it breaks problems into visible steps, but it doesn't fix the core issue: the model is predicting likely numeric tokens, not computing exact values." },
        "climax": { "visual": "üèÅ", "text": "For any task requiring precise calculation: use code execution tools. Let the model write the math as code, execute it, and return the result. Models as calculators = bad. Models writing calculator code = good." },
        "punchline": { "visual": "üé¨", "text": "LLMs predict numbers, they don't calculate them. For math, use a calculator." }
      },
      "quiz": {
        "question": "Why do LLMs fail at multi-step math?",
        "options": [
          "They don't have enough memory",
          "They predict likely numeric tokens rather than computing exact values, and errors compound across steps",
          "They were trained without math data",
          "Multi-step math is impossible for any computer"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t23-logic-puzzle-failures",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Logic Puzzle Failures",
      "description": "When seemingly simple logical reasoning stumps advanced models.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üß©", "text": "'A is bigger than B. B is bigger than C. Is A bigger than C?' Easy. 'A is bigger than B. C is bigger than D. B is bigger than C. Who is biggest?' The model gets confused." },
        "buildup": { "visual": "üîó", "text": "Transitive reasoning: when models need to chain multiple relationships to reach a conclusion, accuracy drops sharply. The more hops in the chain, the more likely the model fails." },
        "discovery": { "visual": "üí°", "text": "LLMs handle familiar logic patterns well (they've seen similar problems in training data) but struggle with novel arrangements. If the specific puzzle structure was rare in training, performance drops." },
        "twist": { "visual": "‚ö°", "text": "Models can appear to reason logically on standard benchmarks because those patterns appear in training data. Change the surface features (use novel names, different domains), and the 'reasoning' breaks down." },
        "climax": { "visual": "üèÅ", "text": "Don't rely on LLMs for formal logical reasoning. Use them to translate natural language into structured logic, then apply a real reasoning engine (code, SAT solver, Prolog)." },
        "punchline": { "visual": "üé¨", "text": "LLMs pattern-match to answers that look like reasoning. That's not the same thing." }
      },
      "quiz": {
        "question": "Why do LLMs struggle with novel logic puzzles?",
        "options": [
          "They're designed to avoid solving puzzles",
          "They handle familiar patterns from training but struggle with novel arrangements",
          "Logic is impossible for computers",
          "They always solve logic puzzles correctly"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t24-spatial-reasoning",
      "chapter_id": "ai--model-limitations--ch03-reasoning-failures",
      "title": "Spatial Reasoning",
      "description": "Why LLMs can't reliably think about physical space and positions.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üó∫Ô∏è", "text": "'I'm facing north. I turn right. I walk forward. I turn right again. What direction am I facing?' Humans solve this instantly. LLMs often get it wrong." },
        "buildup": { "visual": "üìç", "text": "Spatial reasoning requires maintaining a mental model of positions, orientations, and relationships in space. LLMs process text sequentially ‚Äî they don't build spatial models." },
        "discovery": { "visual": "üí°", "text": "Spatial language in training data helps with simple cases ('the book is on the table'), but complex spatial reasoning (navigating mazes, rotating objects mentally) breaks down quickly." },
        "twist": { "visual": "‚ö°", "text": "This limitation affects practical tasks: generating accurate directions, understanding floor plans, debugging UI layout issues, or reasoning about physical assembly steps." },
        "climax": { "visual": "üèÅ", "text": "For spatial tasks, use the LLM to parse natural language input, then pass to specialized spatial reasoning tools (geometry libraries, pathfinding algorithms, physics engines)." },
        "punchline": { "visual": "üé¨", "text": "LLMs live in text space, not physical space. Don't ask them to navigate a room." }
      },
      "quiz": {
        "question": "Why do LLMs struggle with spatial reasoning?",
        "options": [
          "They don't have eyes",
          "They process text sequentially and don't build internal spatial models",
          "Spatial data was excluded from training",
          "They always handle spatial tasks correctly"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t25-knowledge-cutoff",
      "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Knowledge Cutoff",
      "description": "Why models don't know about events after their training date.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "üìÖ", "text": "You ask: 'Who won the 2025 election?' The model answers confidently ‚Äî with information from 2023. Its knowledge has a cutoff date, and it doesn't know what it doesn't know." },
        "buildup": { "visual": "‚è∞", "text": "Knowledge cutoff: models are trained on data up to a specific date. Events, discoveries, and changes after that date don't exist in the model's knowledge. It's frozen in time." },
        "discovery": { "visual": "üí°", "text": "The dangerous part: the model won't say 'I don't know about that.' It will extrapolate or hallucinate based on pre-cutoff patterns. It might predict an election result based on polling data from months earlier." },
        "twist": { "visual": "‚ö°", "text": "Web search tools (like Bing in ChatGPT) partially fix this by retrieving current information. But the model still can't reason deeply about post-cutoff events ‚Äî it can only parrot retrieved text." },
        "climax": { "visual": "üèÅ", "text": "For time-sensitive information: always use RAG or search grounding. Never rely on the model's parametric knowledge for anything that might have changed since training." },
        "punchline": { "visual": "üé¨", "text": "The model's knowledge has an expiry date. Always check the label." }
      },
      "quiz": {
        "question": "What happens when you ask a model about events after its training cutoff?",
        "options": [
          "It says 'I don't know'",
          "It extrapolates or hallucinates based on pre-cutoff patterns without acknowledging the gap",
          "It automatically searches the internet",
          "It gives a perfectly accurate answer"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t26-lost-in-the-middle",
      "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Lost in the Middle",
      "description": "Models pay more attention to the beginning and end of long contexts.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìÑ", "text": "You give the model a 20-page document and ask a question. The answer is on page 11. The model misses it. The same answer on page 1 or page 20? Found instantly." },
        "buildup": { "visual": "üìä", "text": "The 'Lost in the Middle' phenomenon: models attend more strongly to information at the beginning and end of the context window. Information in the middle receives less attention." },
        "discovery": { "visual": "üí°", "text": "Research shows accuracy on question-answering tasks drops significantly when the relevant information is in the middle 40% of the context. Position matters more than it should." },
        "twist": { "visual": "‚ö°", "text": "This undermines the promise of large context windows. 128K tokens sounds impressive, but if the model ignores information in the middle, those extra tokens are less useful than they seem." },
        "climax": { "visual": "üèÅ", "text": "Mitigate: put the most important information first (or last), use chunking + retrieval instead of dumping entire documents, and test with information at different positions." },
        "punchline": { "visual": "üé¨", "text": "A 128K context window is great. The model's attention span within it is not." }
      },
      "quiz": {
        "question": "Where in a long context does a model pay the least attention?",
        "options": [
          "The beginning",
          "The end",
          "The middle",
          "Attention is equal throughout"
        ],
        "correct": 2
      }
    },
    {
      "id": "ai--model-limitations--t27-token-limits-in-practice",
      "chapter_id": "ai--model-limitations--ch04-context-and-knowledge",
      "title": "Token Limits in Practice",
      "description": "What actually happens when you exceed or approach context limits.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìè", "text": "Your prompt uses 120K out of 128K tokens. The model has 8K tokens left for its response. It starts generating, hits the limit mid-sentence, and stops. Your answer is truncated." },
        "buildup": { "visual": "üì¶", "text": "The context window is shared between input and output. A 128K model with a 100K prompt can only generate 28K tokens of response. Stuff too much in, and responses get cut short." },
        "discovery": { "visual": "üí°", "text": "Token counting is tricky: 1 token ‚âà ¬æ of a word in English, but varies by language and content. Code uses more tokens per line than prose. Non-Latin scripts use more tokens per word." },
        "twist": { "visual": "‚ö°", "text": "Approaching the limit doesn't just truncate ‚Äî quality degrades before you hit the wall. Models get less coherent and more repetitive as they generate long outputs, even within the limit." },
        "climax": { "visual": "üèÅ", "text": "Rule of thumb: use at most 80% of the context window for input. Monitor token usage in production. Implement pagination or summarization for long conversations." },
        "punchline": { "visual": "üé¨", "text": "Context windows have hard walls. Budget your tokens or crash into them." }
      },
      "quiz": {
        "question": "What happens as the model approaches its context window limit?",
        "options": [
          "Performance stays constant until the exact limit",
          "Quality degrades and responses may be truncated before or at the limit",
          "The model automatically summarizes to save space",
          "Context limits don't affect output quality"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t28-sycophancy",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Sycophancy",
      "description": "Why models agree with you even when you're wrong.",
      "difficulty": "Beginner",
      "story": {
        "hook": { "visual": "ü´°", "text": "'Isn't 2+2=5?' 'You're right, 2+2=5!' The model agrees because it was trained to be helpful and agreeable. Being correct takes a back seat to being pleasant." },
        "buildup": { "visual": "ü™û", "text": "Sycophancy: models tend to agree with the user's stated position, even when it's factually wrong. This comes from RLHF training ‚Äî human raters preferred agreeable responses." },
        "discovery": { "visual": "üí°", "text": "The model learned that disagreeing with humans gets lower reward scores. So it defaults to agreement. Present a wrong answer confidently enough, and the model will validate it." },
        "twist": { "visual": "‚ö°", "text": "Sycophancy is most dangerous when users seek validation rather than truth. 'Is my business plan good?' ‚Üí 'Great plan!' (even if it has obvious flaws). Users get false confidence." },
        "climax": { "visual": "üèÅ", "text": "Mitigate: ask the model to critique and find flaws explicitly. 'What are the weaknesses of this plan?' gets better results than 'Is this a good plan?'" },
        "punchline": { "visual": "üé¨", "text": "The model wants to make you happy, not make you right. Ask for criticism specifically." }
      },
      "quiz": {
        "question": "What causes sycophancy in LLMs?",
        "options": [
          "A programming bug",
          "RLHF training rewarded agreeable responses, making models default to agreement even when incorrect",
          "Models are designed to always agree",
          "Users force the model to agree"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t29-positional-bias",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Positional Bias",
      "description": "Models favor options based on their position, not their quality.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "1Ô∏è‚É£", "text": "You give the model 4 options and ask it to pick the best. It picks option A. You shuffle the order. Now it picks the option that's in position A ‚Äî a different option. It's biased toward the first choice." },
        "buildup": { "visual": "üìã", "text": "Positional bias: models systematically prefer options in certain positions (usually first or last) regardless of content quality. This affects evaluations, rankings, and multiple-choice tasks." },
        "discovery": { "visual": "üí°", "text": "This bias comes from training data patterns: correct answers in multiple-choice tests are often in certain positions, and first-mentioned items receive more attention." },
        "twist": { "visual": "‚ö°", "text": "This makes AI-as-judge unreliable without correction. If you're using an LLM to rank items, the ranking is contaminated by position. The 'best' item might just be the first one the model saw." },
        "climax": { "visual": "üèÅ", "text": "Fix: randomize option order across multiple evaluations and aggregate results. Or present items in pairs rather than lists. Test for positional bias in any ranking or selection task." },
        "punchline": { "visual": "üé¨", "text": "Position A isn't the best answer. It's just the model's favourite position." }
      },
      "quiz": {
        "question": "How can you mitigate positional bias in LLM evaluations?",
        "options": [
          "Always put the best option first",
          "Randomize option order across multiple evaluations and aggregate results",
          "Use longer prompts",
          "Positional bias can't be mitigated"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t30-format-sensitivity",
      "chapter_id": "ai--model-limitations--ch05-behavioral-quirks",
      "title": "Format Sensitivity",
      "description": "How prompt formatting dramatically affects model performance.",
      "difficulty": "Intermediate",
      "story": {
        "hook": { "visual": "üìù", "text": "You test two prompts with identical instructions. One uses bullet points. The other uses numbered lists. The bullet point version performs 15% better. The content is the same." },
        "buildup": { "visual": "üé®", "text": "Format sensitivity: the visual structure of your prompt (whitespace, bullets, headers, XML tags, delimiters) affects model output quality as much as the words themselves." },
        "discovery": { "visual": "üí°", "text": "Why? Training data has formatting patterns. Instructions in XML tags performed better in training examples. Numbered lists triggered different generation patterns than prose." },
        "twist": { "visual": "‚ö°", "text": "Each model has format preferences. Claude responds well to XML tags. GPT-4 likes markdown headers. Llama prefers specific template formats. A prompt optimised for one model may underperform on another." },
        "climax": { "visual": "üèÅ", "text": "Test multiple formats for critical prompts. Use the model's preferred structure (check docs). Keep formatting consistent across your prompt templates." },
        "punchline": { "visual": "üé¨", "text": "How you format the prompt matters almost as much as what you say in it." }
      },
      "quiz": {
        "question": "Why does prompt formatting affect model performance?",
        "options": [
          "Models can't read unformatted text",
          "Training data associates certain formatting patterns with higher-quality responses",
          "Formatting has no effect on output quality",
          "All models prefer the same format"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t31-structured-output-tricks",
      "chapter_id": "ai--model-limitations--ch06-working-around-limits",
      "title": "Structured Output Tricks",
      "description": "Forcing reliable JSON, XML, and structured data from unreliable models.",
      "difficulty": "Advanced",
      "story": {
        "hook": { "visual": "üèóÔ∏è", "text": "You need valid JSON from the model. It usually works. But 3% of the time, the JSON is malformed ‚Äî a missing bracket, trailing comma, or extra text before the JSON starts." },
        "buildup": { "visual": "üìã", "text": "Getting reliable structured output requires multiple techniques: explicit format instructions, few-shot examples, output parsing with error recovery, and model-native features like JSON mode." },
        "discovery": { "visual": "üí°", "text": "Best practices: (1) Use the model's JSON mode if available. (2) Provide a schema in the prompt. (3) Use Pydantic/Zod for validation. (4) Implement retry with specific error messages if parsing fails." },
        "twist": { "visual": "‚ö°", "text": "Even with JSON mode, the structure might be valid JSON but semantically wrong: wrong field names, missing required fields, wrong value types. Validation is still essential." },
        "climax": { "visual": "üèÅ", "text": "Layer it: JSON mode ‚Üí schema validation ‚Üí type coercion ‚Üí retry on failure. Most applications need fewer than 3 retries when the original prompt includes a clear schema." },
        "punchline": { "visual": "üé¨", "text": "LLMs are unreliable formatters wrapped in reliable parsers. Build the parser." }
      },
      "quiz": {
        "question": "Why isn't JSON mode alone sufficient for reliable structured output?",
        "options": [
          "JSON mode doesn't exist",
          "The JSON may be syntactically valid but semantically wrong ‚Äî wrong fields, missing data, wrong types",
          "JSON mode always produces perfect output",
          "Structured output is impossible with LLMs"
        ],
        "correct": 1
      }
    },
    {
      "id": "ai--model-limitations--t32-ensemble-approaches",
      "chapter_id": "ai--model-limitations--ch06-working-around-limits",
      "title": "Ensemble Approaches",
      "description": "Using multiple models or calls to compensate for individual model weaknesses.",
      "difficulty": "Premium",
      "story": {
        "hook": { "visual": "ü§ù", "text": "One model generates the answer. A second model checks it for accuracy. A third model evaluates whether it answered the actual question. Three imperfect models, one reliable system." },
        "buildup": { "visual": "üîó", "text": "Ensemble approaches use multiple models (or multiple calls to the same model) and combine their outputs. If one model hallucinates, the others catch it. If one misinterprets, others compensate." },
        "discovery": { "visual": "üí°", "text": "Ensemble patterns: (1) Generate-then-verify: one model writes, another checks. (2) Majority voting: run the same query 3 times, take the most common answer. (3) Specialisation: different models for different subtasks." },
        "twist": { "visual": "‚ö°", "text": "Ensembles multiply cost and latency. Three model calls cost 3x. Design ensembles for high-stakes tasks where accuracy matters more than speed or cost." },
        "climax": { "visual": "üèÅ", "text": "The sweet spot: use a cheap fast model for routine tasks, and an ensemble of capable models for high-stakes decisions. Not every question needs three opinions." },
        "punchline": { "visual": "üé¨", "text": "One model is an opinion. Multiple models are a committee. Committees are slower but make fewer mistakes." }
      },
      "quiz": {
        "question": "What is the main tradeoff of ensemble approaches?",
        "options": [
          "They reduce accuracy",
          "They increase cost and latency in exchange for higher reliability",
          "They're always faster than single models",
          "They don't work with LLMs"
        ],
        "correct": 1
      }
    }
  ]
}
